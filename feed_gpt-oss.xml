<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: gpt-oss</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'gpt-oss' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Wed, 14 Jan 2026 02:06:48 +0000</lastBuildDate>
  <item>
    <title>[Refactor] [8/N] to simplify the vLLM openai responsesapi_serving architecture</title>
    <link>https://github.com/vllm-project/vllm/pull/32260</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32260</guid>
    <pubDate>Tue, 13 Jan 2026 13:24:01 +0000</pubDate>
    <description>
## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Frontend] Normalize Responses API input for multi-turn conversations</title>
    <link>https://github.com/vllm-project/vllm/pull/32253</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32253</guid>
    <pubDate>Tue, 13 Jan 2026 10:14:38 +0000</pubDate>
    <description>## Purpose

Normalizes input types in the Responses API for **Codex CLI** multi-turn conversation support.

When Codex echoes back previous responses in multi-turn conversations, it includes the exact types that were returned by the server. However, the server expects different types on input vs output:
- Server returns: `output_text` ‚Üí Client should send: `input_text`
- Server returns: `output_text` in reasoning ‚Üí Client should send: `reasoning_text`

This PR adds preprocessing to automatically normalize these types before validation, enabling seamless multi-turn conversations with Codex without requiring manual type conversion.

**Changes:**
- Rename `function_call_parsing` ‚Üí `preprocess_input` (more descriptive)
- Add `output_text` ‚Üí `input_text` conversion for message content
- Add `ou

...</description>
  </item>
  <item>
    <title>[Frontend] Add encrypted_content to reasoning items for round-tripping</title>
    <link>https://github.com/vllm-project/vllm/pull/32247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32247</guid>
    <pubDate>Tue, 13 Jan 2026 09:24:44 +0000</pubDate>
    <description>## Purpose

Populates the `encrypted_content` field in `ResponseReasoningItem` to enable clients to round-trip reasoning items in multi-turn conversations.

Before this change, `encrypted_content` was always `null` in reasoning item responses, which broke clients that rely on this field to echo back reasoning items from previous turns.

**Changes:**
- Add `_create_reasoning_item_with_encrypted_content()` helper in `harmony_utils.py`
- Update all `ResponseReasoningItem` usages in `serving_responses.py` to use the helper

## Test Plan

```bash
curl -s http://localhost:8000/v1/responses   -H &quot;Content-Type: application/json&quot;   -H &quot;Authorization: Bearer test&quot;   -d &#x27;{
    &quot;model&quot;: &quot;gpt-oss&quot;,
    &quot;input&quot;: &quot;What is 2+2?&quot;
  }&#x27; | jq &#x27;.output[] | select(.type == &quot;reasoning&quot;) | {id, encrypted_content}

...</description>
  </item>
  <item>
    <title>Add descriptive error message for missing tools.</title>
    <link>https://github.com/vllm-project/vllm/pull/32147</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32147</guid>
    <pubDate>Mon, 12 Jan 2026 05:28:45 +0000</pubDate>
    <description>Summary: Add descriptive error message for missing tools.

Test Plan: Unit test

Differential Revision: D90477073


---

&gt; [!NOTE]
&gt; &lt;sup&gt;[Cursor Bugbot](https://cursor.com/dashboard?tab=bugbot) is generating a summary for commit 07adcb40f7abf76cf0a056735d621117a4a31949. Configure [here](https://cursor.com/dashboard?tab=bugbot).&lt;/sup&gt;

---

&gt; [!NOTE]
&gt; Improves robustness and debuggability of tool invocation.
&gt; 
&gt; - Add `_get_tool_session` in `ParsableContext` and `HarmonyContext` to raise a descriptive `KeyError` with available sessions and a `--tool-server` hint
&gt; - Update `call_tool` paths to use `_get_tool_session` instead of direct `_tool_sessions[...]` access for `python`, `browser`, and `container`
&gt; - Add unit tests validating error messaging, listing of available sessions, and suc

...</description>
  </item>
  <item>
    <title>Oracle improvements</title>
    <link>https://github.com/vllm-project/vllm/pull/32122</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32122</guid>
    <pubDate>Sun, 11 Jan 2026 14:33:31 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
* add support for running naive and ag/rs dispatch/combine via Modular Kernels
* update oracle to query kernels for support for various features
* update fp8.py to own the modular kernel internally for dp/ep case and tp case

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes upd

...</description>
  </item>
  <item>
    <title>[Bugfix] Fix Harmony preamble visibility in Responses API</title>
    <link>https://github.com/vllm-project/vllm/pull/32114</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32114</guid>
    <pubDate>Sun, 11 Jan 2026 12:40:51 +0000</pubDate>
    <description>##  Purpose

Fix incorrect handling of Harmony format preambles in the Responses API.

Per the https://cookbook.openai.com/articles/openai-harmony, preambles (commentary channel messages with recipient=None) are explanatory text intended to be shown to end-users (e.g., &quot;I&#x27;ll search for that information now...&quot;). The current implementation incorrectly treats them as hidden reasoning content.

This PR changes how commentary-channel preambles are parsed:
  - Before: ResponseReasoningItem (hidden in reasoning_content)
  - After: ResponseOutputMessage (visible in content)

Related: This is a follow-up fix to #29972, which added GPT-OSS models with Harmony chat format.

## Migration Note

Breaking Change: Preambles now appear in content instead of reasoning_content. Clients that relied on the pr

...</description>
  </item>
  <item>
    <title>[RFC] Improve environment variable declaration and handling (#31249)</title>
    <link>https://github.com/vllm-project/vllm/pull/32070</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32070</guid>
    <pubDate>Sat, 10 Jan 2026 01:06:48 +0000</pubDate>
    <description>This PR implements the refactoring proposed in issue #31249 to improve environment variable handling in vLLM by consolidating declarations into a single source of truth with automatic type conversion.

## Summary

Replaces the duplicated type definitions and getter dictionary pattern with a unified system that:
- Declares each variable once with type annotations and defaults
- Automatically converts types based on type hints
- Supports custom parsing, lazy defaults, and validated choices
- Provides better IDE support through proper docstrings
- Includes pre-commit validation to prevent errors

## Changes

### New Package Structure
- `vllm/envs/__init__.py`: Module interface with `__getattr__` for dynamic variable access and automatic type conversion
- `vllm/envs/_variables.py`: Single sour

...</description>
  </item>
  <item>
    <title>[5/N][Attention] Finish eliminating `vllm/attention` folder</title>
    <link>https://github.com/vllm-project/vllm/pull/32064</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32064</guid>
    <pubDate>Fri, 09 Jan 2026 23:03:42 +0000</pubDate>
    <description>Merge https://github.com/vllm-project/vllm/pull/32060 before this.

## Purpose
Step 5 of #31919: This PR finishes eliminating the `vllm/attention` folder by doing the following:
* Split `vllm/attention/layer.py` into `vllm/model_executor/layers/attention/mla_attention.py` (`MLAAttention`, `unified_mla_attention`) and `vllm/model_executor/layers/attention/attention.py` (`Attention`, `unified_attention`)
* Move `vllm/attention/utils/kv_sharing_utils.py` content into `vllm/model_executor/layers/attention/attention.py`
* Move `vllm/attention/utils/kv_transfer_utils.py` to `vllm/model_executor/layers/attention/kv_transfer_utils.py`
* Eliminate `vllm/attention` folder
* Add imports to `vllm/model_executor/layers/attention/__init__.py` to enable module-level imports

## Test Plan
CI (should run a

...</description>
  </item>
  <item>
    <title>[Frontend] Add `reasoning_effort` and `parallel_tool_calls` to `extra_args` of `SamplingParams`.</title>
    <link>https://github.com/vllm-project/vllm/pull/31952</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31952</guid>
    <pubDate>Thu, 08 Jan 2026 07:25:45 +0000</pubDate>
    <description>## Purpose

~~I suggest adding two fields, `reasoning_effort` and `parallel_tool_calls`, to `SamplingParams` to enable logits processors to support the OpenAI-compatible parameters by controlling token generation.~~

I suggest adding two fields, `reasoning_effort` and `parallel_tool_calls`, to `extra_args` of `SamplingParams` to enable logits processors to support the OpenAI-compatible parameters by controlling token generation.

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e 

...</description>
  </item>
  <item>
    <title>[Misc] Add VLLM_USE_FLASHINFER_ROPE to control the RoPE kernel for cuda</title>
    <link>https://github.com/vllm-project/vllm/pull/31893</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31893</guid>
    <pubDate>Wed, 07 Jan 2026 10:49:29 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Related PRs:
#21126 integrated the Flashinfer RoPE kernel.
#25299 disabled it by default due to CI failures.
#30729 enabled it by default for Deepseek RoPE.

This PR adds `VLLM_USE_FLASHINFER_ROPE` env so that we can easily enable Flashinfer RoPE kernel.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes up

...</description>
  </item>
  <item>
    <title>[responsesAPI] get reasoning token metrics for simpleContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31839</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31839</guid>
    <pubDate>Tue, 06 Jan 2026 22:18:30 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Bugfix] Sanitize malformed tool call recipients in Harmony parser</title>
    <link>https://github.com/vllm-project/vllm/pull/31677</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31677</guid>
    <pubDate>Sun, 04 Jan 2026 09:30:09 +0000</pubDate>
    <description>Some GPT-OSS base models occasionally generate malformed Harmony format sequences like `to=functions.bash&lt;|channel|&gt;commentary` instead of the correct `to=functions.bash &lt;|constrain|&gt;json`. This causes the function name to be parsed incorrectly as `bash&lt;|channel|&gt;commentary` instead of `bash`.

This fix sanitizes the recipient string by stripping `&lt;|channel|&gt;` and everything after it before extracting the function name. The fix is applied in three locations to cover all API endpoints:

- `harmony_utils.py`: /v1/responses (non-streaming)
- `openai_tool_parser.py`: /v1/chat/completions (non-streaming)
- `serving_chat_stream_harmony.py`: /v1/chat/completions (streaming)

The /v1/responses streaming endpoint already worked correctly because it captures the recipient before malformed tokens can

...</description>
  </item>
  <item>
    <title>[Kernel] Add Sonic MoE integration for Hopper GPUs</title>
    <link>https://github.com/vllm-project/vllm/pull/31548</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31548</guid>
    <pubDate>Tue, 30 Dec 2025 22:01:32 +0000</pubDate>
    <description>## Purpose

Integrate [Sonic MoE](https://github.com/Dao-AILab/sonic-moe) for Hopper GPUs. ([paper](https://arxiv.org/abs/2512.14080)) üé∏ üöÄ 

Addresses #31039

### IMPORTANT

**blocked by `sonicmoe` in CI to run tests (Need contributor/maintainer to trigger?)**

## Key changes

  - `SonicMoeExperts` class following `DeepGemmExperts` pattern
  - Weight permutation for swiglu format compatibility (per [sonic-moe#12](https://github.com/Dao-AILab/sonic-moe/issues/12))
  - Routing conversion from vLLM&#x27;s `topk_ids/topk_weights` to Sonic&#x27;s scheduling tensors
  - Wiring into `UnquantizedFusedMoeMethod` behind `VLLM_USE_SONIC_MOE` env flag
  - H100 CI config with `pip install sonicmoe`

### Gating logic

  Sonic MoE auto-disables when:
  - Expert parallelism enabled
  - MoE biases present
  - FlashI

...</description>
  </item>
  <item>
    <title>[LoRA] Simplify gpt-oss w13_lora_b weight loading</title>
    <link>https://github.com/vllm-project/vllm/pull/31255</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31255</guid>
    <pubDate>Wed, 24 Dec 2025 02:55:40 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

This PR simplifies gpt-oss `w13_lora_b` weight loading code.

gpt-oss `lora_b` weights is in interleaved layout `[w1_0, w3_0, w1_1, w3_1, ...]`. The current `_slice_w13_b` code do de-interleaving, slicing, and re-interleaving. But the code can be simplified by directly returning a contiguous slice of the original interleaved dimension.

Below is a short example of the original code:

```
&gt;&gt;&gt; start_idx = 0
&gt;&gt;&gt; end_idx = 2
&gt;&gt;&gt; w13_lora_b = torch.tensor([[[11], [21], [12], [22], [13], [23], [14], [24]]]) # shape [num_experts, output_size, rank]
&gt;&gt;&gt; print(w13_lora_b)
tensor([[[11],
         [21],
         [12],
         [22],
         [13],
         [23],
         [14],
         [24]]])
&gt;&gt;&gt; 
&gt;&gt;&gt; w1_lora_b = w13_lora_b[:, ::2, :] # even positions
&gt;&gt;&gt; p

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31250</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31250</guid>
    <pubDate>Wed, 24 Dec 2025 00:18:46 +0000</pubDate>
    <description>This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and allows us to reuse the weights wh

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31247</guid>
    <pubDate>Tue, 23 Dec 2025 23:27:39 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;
This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and all

...</description>
  </item>
  <item>
    <title>Cleanup basic and entrypoint test organisation</title>
    <link>https://github.com/vllm-project/vllm/pull/31228</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31228</guid>
    <pubDate>Tue, 23 Dec 2025 17:33:22 +0000</pubDate>
    <description>Supersedes https://github.com/vllm-project/vllm/pull/27747:

- Don&#x27;t call basic test files individually
- Move the unit tests into a `unit` directory so we don&#x27;t have to manually ignore everything else
- Move `openai/tool_parsers` into the `unit` tests because that&#x27;s what they are</description>
  </item>
  <item>
    <title>[Feature] add StreamableResponsesParser and token usage counting for ParsableContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31094</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31094</guid>
    <pubDate>Sun, 21 Dec 2025 11:11:46 +0000</pubDate>
    <description># Purpose

#30759 

- Count the token usage for ParsableContext properly, including `reasoning_tokens` and `output_tokens` and `tool_output_tokens`.
- A StreamableResposesParser is created to parse output token one by one, which can also be used to create an StreamParsableContext

# Design

## Overview

This PR introduces a new `StreamableResponsesParser` that processes output tokens one by one, enabling accurate token counting for reasoning tokens in `ParsableContext`.

## Key Changes

### 1. StreamableResponsesParser (`vllm/entrypoints/openai/parser/responses_parser.py`)

A new streaming parser that processes tokens incrementally:

- **`current_channel`** property: Identifies the current token type
  - `&#x27;analysis&#x27;`: reasoning token
  - `&#x27;commentary&#x27;`: tool call token
  - `&#x27;final&#x27;`: regul

...</description>
  </item>
  <item>
    <title>Add positional embedding and kv_cache fusion for llama and gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30978</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30978</guid>
    <pubDate>Thu, 18 Dec 2025 17:57:04 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>Use aiter triton fused_add_rmsnorm_pad for gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30976</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30976</guid>
    <pubDate>Thu, 18 Dec 2025 17:39:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Adds fused padding op before router GEMM on ROCm, eliminating this unfused pad after the GEMM before the fused_moe: https://github.com/ROCm/vllm/blob/main/vllm/model_executor/layers/fused_moe/layer.py#1603

Before:
&lt;img width=&quot;1370&quot; height=&quot;30&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/11db1bf1-e8b0-49c5-b180-3d1c730f07ca&quot; /&gt;
After:
&lt;img width=&quot;1462&quot; height=&quot;34&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/e5979f88-03c0-4173-adbd-8227786e929f&quot; /&gt;

Follow-up is to replace this with a single `F.pad` and try fusing CK rmsnorm and pad instead of manually calling this kernel.

See also #30357

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- 

...</description>
  </item>
  <item>
    <title>[Perf] Eliminate padding and slicing op for GPT-OSS with Flashinfer MXFP4 MXFP8 MoE</title>
    <link>https://github.com/vllm-project/vllm/pull/30647</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30647</guid>
    <pubDate>Sun, 14 Dec 2025 13:40:05 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

- Depends on Flashinfer update #30993
- Eliminated padding op before the MoE: by setting the alignment in flashinfer mxfp8 quant, the output quantized tensor will be padded.
- Eliminated slicing op after the MoE: by passing the output tensor with unpadded hidden size to MoE kernel, this depends on a Flashinfer PR:
  - https://github.com/flashinfer-ai/flashinfer/pull/2217
  - This will also resolve the previous AR+Norm fusion broken by slice op issue: #28841
- Cleaned up the padding logic: for mxfp4 quant, the padded hidden size is calculated in `create_weights()`, the `maybe_roundup_hidden_size()` in `vllm/model_executor/layers/fused_moe/layer.py` seems like a dup.


## Test Plan &amp;&amp; Test Result(GPT-OSS-120b TP8)

### Accuracy

PR:
```
[{&#x27;eval_name

...</description>
  </item>
  <item>
    <title>[GPT OSS] Fix tool_choice required</title>
    <link>https://github.com/vllm-project/vllm/pull/30557</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30557</guid>
    <pubDate>Fri, 12 Dec 2025 14:30:24 +0000</pubDate>
    <description></description>
  </item>
  <item>
    <title>[Frontend] Honor chat template for gpt-oss harmony (#23015)</title>
    <link>https://github.com/vllm-project/vllm/pull/30482</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30482</guid>
    <pubDate>Thu, 11 Dec 2025 10:23:37 +0000</pubDate>
    <description>## Issue
- For gpt-oss models, --chat-template (and tokenizer/chat_template_kwargs) are ignored; prompts are rendered via Harmony instead of the normal chat-template pipeline.
## Reason
- When hf_config.model_type == &quot;gpt_oss&quot;, vLLM sets use_harmony and routes requests through _make_request_with_harmony(...), which directly constructs Harmony system/developer/user messages and calls render_for_completion. That path never calls _preprocess_chat / apply_hf_chat_template, so any chat template settings are bypassed.
## Summary
- allow server-level chat_template to be applied even in Harmony (gpt-oss) paths for chat and responses
- pass tokenizer into Harmony preprocessing and render via apply_hf_chat_template when provided, with safe fallback to Harmony default on errors
- tighten typing/forma

...</description>
  </item>
  <item>
    <title>[Optimization]: Add fused router for GPTOSS</title>
    <link>https://github.com/vllm-project/vllm/pull/30471</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30471</guid>
    <pubDate>Thu, 11 Dec 2025 07:55:47 +0000</pubDate>
    <description>&lt;details&gt;
&lt;summary&gt; output collect_env &lt;/summary&gt;

```
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 24.04.3 LTS (x86_64)
GCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version                : Could not collect
CMake version                : version 3.28.3
Libc version                 : glibc-2.39

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version               : 3.12.11

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] GPT OSS, DS FP4 Upstream fp8 with static scales</title>
    <link>https://github.com/vllm-project/vllm/pull/30357</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30357</guid>
    <pubDate>Tue, 09 Dec 2025 19:49:32 +0000</pubDate>
    <description>## gpt-oss120b-w-mxfp4-a-fp8

server:
&gt; VLLM_ROCM_USE_AITER=1 HIP_VISIBLE_DEVICES=1 VLLM_USE_AITER_UNIFIED_ATTENTION=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_ROCM_USE_AITER_FUSED_MOE_A16W4=0 vllm serve /data/models/gpt-oss120b-w-mxfp4-a-fp8     --port 8000     --swap-space 64     --max-model-len 10368     --tensor-parallel-size 1     --max-num-seqs 1024     --gpu-memory-utilization 0.95     --no-enable-prefix-caching     --enforce-eager

client:
&gt; curl http://localhost:8000/v1/completions -H &quot;Content-Type: application/json&quot; -d &#x27;{
    &quot;model&quot;: &quot;/data/models/gpt-oss120b-w-mxfp4-a-fp8&quot;,
    &quot;prompt&quot;: &quot;San Francisco is a&quot;,
    &quot;max_tokens&quot;: 16,
    &quot;temperature&quot;: 0
}&#x27;

output:
&gt; {&quot;id&quot;:&quot;cmpl-9e80e71642c56933&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1765949222,&quot;model&quot;:&quot;/data/models/gpt-oss120b-w-mxfp4-a-fp

...</description>
  </item>
  <item>
    <title>[gpt-oss] Add model_identity to system message retrieval for harmony chat template</title>
    <link>https://github.com/vllm-project/vllm/pull/30247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30247</guid>
    <pubDate>Mon, 08 Dec 2025 08:43:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
When using GPT-OSS, `model_identity` passed in from `chat_template_kwargs` is not being used to adjust model&#x27;s identity. This PR is to pass the `model_identity` from chat/completion to the system message formatter.
## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If you

...</description>
  </item>
  <item>
    <title>Feat/support nemotron h mtp wip</title>
    <link>https://github.com/vllm-project/vllm/pull/30208</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30208</guid>
    <pubDate>Sun, 07 Dec 2025 13:46:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[DO NOT MERGE] Introduce Renderer for processing chat messages (using `ModelConfig`)</title>
    <link>https://github.com/vllm-project/vllm/pull/30200</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30200</guid>
    <pubDate>Sun, 07 Dec 2025 08:30:34 +0000</pubDate>
    <description>Ready for review.

## Purpose

- Prototype an interface, `vllm.renderers.RendererLike`, to process chat messages into engine inputs.
- Introduce `RENDERER_REGISTRY` which lazily registers renderers to avoid circular import problem.
- Move implementation-specific chat utils to the corresponding renderer in `vllm.renderers`.
- Initialize the renderer in `InputPreprocessor`, replacing the tokenizer initialization inside `LLMEngine` and `AsyncLLM`.
- Replace `EngineClient.get_tokenizer()` with `EngineClient.renderer.get_tokenizer()` to avoid unnecessary async.
- Update tests accordingly, and move some tests into a new directory `tests/renderers` that is run under `Async Engine, Inputs, Utils, Worker, Config Test (CPU)`.

Towards #22880 and #23873

Future work:

- Merge `CompletionRenderer` wit

...</description>
  </item>
  <item>
    <title>[Perf] Add benchmark script for triton unified attention kernel performance</title>
    <link>https://github.com/vllm-project/vllm/pull/30191</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30191</guid>
    <pubDate>Sun, 07 Dec 2025 03:13:29 +0000</pubDate>
    <description>
## Purpose
Add a script for benchmarking triton unified attention kernel performance and compare against flash attention. This could be useful for testing new hardware performance etc 

## Test Plan
```

(vllm) ubuntu@209-20-157-255:~/vllm$ python benchmarks/kernels/benchmark_triton_unified_attention.py
flash_attn not available. Only Triton Unified Attention will be benchmarked.
================================================================================
Triton Unified Attention Benchmark
================================================================================
Configuration:
  dtype: torch.bfloat16
  num_query_heads: 32
  num_kv_heads: 8
  head_size: 128
  block_size: 16
  use_alibi: False
  sliding_window: -1
  softcap: 0.0
  FlashAttention available: False
==================

...</description>
  </item>
  <item>
    <title>[Feature] Add track_token_ids for Efficient Selective Token Logprobs Tracking</title>
    <link>https://github.com/vllm-project/vllm/pull/30030</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30030</guid>
    <pubDate>Thu, 04 Dec 2025 07:04:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
This PR adds a new `track_token_ids` parameter to `SamplingParams` that enables efficient tracking of log probabilities (logprobs) for specific tokens without requiring full vocabulary logprobs retrieval. This feature addresses [Issue#29280](https://github.com/vllm-project/vllm/issues/29280)

## Implementation Overview

The feature is implemented across the vLLM V1 engine pipeline, from parameter validation to output processing.

### 1. Parameter Extension (`vllm/sampling_params.py`)

Added `track_token_ids: list[int] | None` parameter with validation ensuring all IDs are non-negative integers:

```python
track_token_ids: list[int] | None = None
&quot;&quot;&quot;List of token IDs to track logprobs for at every generation step.&quot;&quot;&quot;
```

### 2. New Data Structures 

...</description>
  </item>
  <item>
    <title>Added regression test for openai/harmony/issues/78</title>
    <link>https://github.com/vllm-project/vllm/pull/29830</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29830</guid>
    <pubDate>Tue, 02 Dec 2025 01:06:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Add regression tests for [openai/harmony#78](https://github.com/openai/harmony/issues/78), particularly the FastAPI `vllm serve` path which wasn&#x27;t fully addressed in #26185.

## Test Plan

Modify the existing `test_output_messages_enabled` test in test_response_api_with_harmony.py to add the following validations:

```python
    for _message in [*response.input_messages, *response.output_messages]:
        for _item in _message.get(&quot;content&quot;):
            assert isinstance(_item, dict), _message
            assert len(_item) &gt; 0, _message
```

&lt;details&gt;&lt;summary&gt;full code details&lt;/summary&gt;

```python
@pytest.mark.asyncio
@pytest.mark.parametrize(&quot;model_name&quot;, [MODEL_NAME])
async def test_output_messages_enabled(client: OpenAI, model_name: str, serv

...</description>
  </item>
  <item>
    <title>[Frontend] Add streaming tool-call support to Responses API (non-Harmony)</title>
    <link>https://github.com/vllm-project/vllm/pull/29726</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29726</guid>
    <pubDate>Sat, 29 Nov 2025 12:18:12 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Fix for #29725, 

### Summary
This pull request fixes an issue where non harmony models using the Responses API with streaming and tools emit only ResponseTextDeltaEvent events, instead of ResponseFunctionCallArgumentsDeltaEvent when a tool call is selected. This prevents clients from reliably detecting and parsing tool call arguments from the stream.

### Fix
This change updates the streaming path for non harmony models so that:

When the model selects a tool call, the arguments are surfaced as ResponseFunctionCallArgumentsDeltaEvent instead of plain text deltas. The event structure is now consistent with harmony models and with the non streaming Responses API behavior. With this, clients can treat harmony and non harmony models uniformly when han

...</description>
  </item>
  <item>
    <title>Flashrl</title>
    <link>https://github.com/vllm-project/vllm/pull/29586</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29586</guid>
    <pubDate>Thu, 27 Nov 2025 05:56:16 +0000</pubDate>
    <description>## Updating vLLM version to 0.11 with flash-rl updates and patches</description>
  </item>
  <item>
    <title>fix: Add validation for tool requests that the tool is available</title>
    <link>https://github.com/vllm-project/vllm/pull/29498</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29498</guid>
    <pubDate>Wed, 26 Nov 2025 14:26:48 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

  - Add request-level validation so Harmony tool calls fail fast when the requested built-in tool is not available on the configured tool server (browser/
    code_interpreter/container). This prevents the silent ‚Äúbest-effort‚Äù behavior noted in the PR discussion and surfaces an invalid_request_error instead.

## Test Plan

  - .venv/bin/pytest tests/entrypoints/openai/test_serving_responses.py -q

## Test Result

  - 6 passed, 3 warnings (module-level tests including new coverage)

## Fix PR

https://github.com/vllm-project/vllm/issues/29432

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [x] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The tes

...</description>
  </item>
  <item>
    <title>Support compressed-tensors W4A8 MoE checkpoints in GptOssModel weight loader for CPU</title>
    <link>https://github.com/vllm-project/vllm/pull/29315</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29315</guid>
    <pubDate>Mon, 24 Nov 2025 14:43:11 +0000</pubDate>
    <description>1. Add GptOssModel.load_per_expert_unfused_w4a8 helper to handle per-expert unfused MoE weights (gate_proj, up_proj, down_proj) in W4A8 checkpoints and map them into the fused FusedMoE layout (w13_* and w2_* parameters).
	‚Ä¢	Handles .weight, .weight_scale, .bias, and .input_scale suffixes.
	‚Ä¢	For biases, manually slices and writes into the appropriate columns of w13_bias (gate vs up) and w2_bias, supporting both 1D and 2D parameter layouts and using expert_id to pick the correct expert slice when the source tensor has an extra expert dimension.
	‚Ä¢	For weights/scales, delegates to a custom weight_loader when present, falling back to default_weight_loader otherwise, and surfaces whether the mapping was successfully handled.

2. Extend _load_weights_other to:
	‚Ä¢	Detect W4A8 (int4 weights, int8

...</description>
  </item>
  <item>
    <title>Fix gpt oss tool parser v2</title>
    <link>https://github.com/vllm-project/vllm/pull/29236</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29236</guid>
    <pubDate>Sat, 22 Nov 2025 11:27:13 +0000</pubDate>
    <description>Purpose
This PR fixes critical issues with tool call extraction in the OpenAIToolParser (Harmony/gpt-oss), specifically when using Custom Tools and MCP (Model Context Protocol).

Previously, the parser failed to correctly capture tool calls that resided in the active stream buffer (current_content) but hadn&#x27;t yet been finalized into a message object. This led to two major issues:

Tool Call Failure: valid tool calls were dropped or ignored.

Channel Leakage/Hallucination: The model appeared to hallucinate or leak tool call structures into the final (user-facing) message channel instead of processing them as structured tool calls.

Changes Implemented:

Active Buffer Parsing: Added logic to inspect parser.current_content and parser.current_channel. If the active channel is commentary (used 

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations </title>
    <link>https://github.com/vllm-project/vllm/pull/29008</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29008</guid>
    <pubDate>Wed, 19 Nov 2025 11:21:15 +0000</pubDate>
    <description>## Purpose

This PR aims for:
- [x] quark model loading, combined with `mxfp4` loading [function ](https://github.com/vllm-project/vllm/blob/256a33ecb4923a4bfa6e1a9adbc69f5255a5aa6d/vllm/model_executor/models/gpt_oss.py#L309) for original openai/gpt-oss-20b &amp; openai/gpt-oss-120b
- [x] OCPMX_W4A16, OCPMX_W4AFP8 MoE scheme and emulation forward, unified into class `QuarkOCP_MX_MoEMethod`

## Test Plan

- Models:
  - [x] GPT_OSS_20B
  - [x] GPT_OSS_120B
- Quantization schemes:
  - [x] W: MXFP4, A: BF16, (optional) KV: FP8
  - [x] W: MXFP4, A: FP8, (optional) KV: FP8
  - [x] W: MXFP4, A: MXFP4, (optional) KV: FP8
- TP:
  - [x] TP1
  - [x] TP2
  - [x] TP4
  - [x] TP8

See results below.

## Test Result
&lt;img width=&quot;1418&quot; height=&quot;304&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/86

...</description>
  </item>
  <item>
    <title>[Feature] Generic Model Support via TrainableAttention and ModelRegistry parallelism constructor callback</title>
    <link>https://github.com/vllm-project/vllm/pull/28685</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28685</guid>
    <pubDate>Thu, 13 Nov 2025 21:25:45 +0000</pubDate>
    <description>## Purpose

Implement RFC #28326 to enable users to easily integrate custom models with vLLM for both training and inference, with support for external parallelism libraries (e.g., Megatron-LM, FSDP, DeepSpeed).

Enables use cases like:
  - RL training with vLLM acceleration
  - Integrating third-party models without rewriting parallelism logic
  - Mixing external tensor parallel layers (Megatron) with vLLM attention

## Test Plan

```
# example, should be a test
python examples/offline_inference/custom_model_with_megatron.py
```
  - Full transformer using Megatron-LM&#x27;s ColumnParallelLinear + RowParallelLinear for MLPs
  - TrainableFlashAttention for attention (with KV cache + backward pass)
  - Ground truth validation against independent PyTorch run
  - Tests TP=4, dynamic batching, train

...</description>
  </item>
  <item>
    <title>[Bug]: rocm crash AMD Ryzen AI 9 HX PRO 370 w/ Radeon 890M - docker/podman</title>
    <link>https://github.com/vllm-project/vllm/issues/28460</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28460</guid>
    <pubDate>Tue, 11 Nov 2025 10:16:32 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0
Clang version                : 20.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-7.0.0 25314 f4087f6b428f0e6f575ebac8a8a724dab123d06e)
CMake version                : version 3.31.6
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0a0+git1c57644
Is debug build               : False
CUDA used to build PyTorc

...</description>
  </item>
  <item>
    <title>Fix/responses api harmony channel metadata    #28262</title>
    <link>https://github.com/vllm-project/vllm/pull/28355</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28355</guid>
    <pubDate>Sun, 09 Nov 2025 00:48:27 +0000</pubDate>
    <description> 
 
## Purpose
Fix issue #28262: Restore missing channel metadata when converting Responses API output items back to Harmony Messages for multi-turn conversations.
Changes:
Set channel=&#x27;commentary&#x27; for function_call_output type inputs
Set channel=&#x27;analysis&#x27; or &#x27;commentary&#x27; for reasoning type based on the following message (commentary if followed by function_call, analysis otherwise)
Add test to verify channel metadata is correctly preserved across conversation turns
Update parse_response_input() to accept optional next_msg parameter for context-aware channel assignmen
## Test Plan
pytest tests/entrypoints/openai/test_response_api_with_harmony.py::test_function_call_with_previous_input_messages -v
## Test Result
pass 
---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description

...</description>
  </item>
  <item>
    <title>[Bugfix] Parse gpt-oss refusals w/ newer openai-harmony</title>
    <link>https://github.com/vllm-project/vllm/pull/28303</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28303</guid>
    <pubDate>Fri, 07 Nov 2025 15:35:05 +0000</pubDate>
    <description>## Purpose

The output generated by gpt-oss models does not always strictly follow its expected harmony chat template format. This commonly - but not exclusively - happens when gpt-oss-120b generates refusals for content that violates its built-in safety guidelines.

To fix this, a non-strict mode was added to the openai-harmony library to allow attempted recovery of malformed message headers in the model output, such as a missing `&lt;|message|&gt;` special token before the assistant text.

This will resolve some cases where the error
`openai_harmony.HarmonyError: unexpected tokens remaining in message header` was previously thrown. It will not resolve all of those, as not every malformed message output can be recovered. Other ongoing work around using structured output for the Harmony format c

...</description>
  </item>
  <item>
    <title>[Frontend] [gpt-oss] Chat format GD for tool calling with gptoss</title>
    <link>https://github.com/vllm-project/vllm/pull/28148</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28148</guid>
    <pubDate>Wed, 05 Nov 2025 18:17:40 +0000</pubDate>
    <description>## Purpose
Building on top of the initial gpt-oss reasoning parser in #25515, this PR fleshes out the full structural tags schema to guide the chat format for gpt-oss.

It is only added to the Responses API path for now, but it should be able to fix issues like the ones mentioned in #24954 without needing to make adjustments to Harmony.

Guided decoding for the gpt-oss chat format should only be enabled if `structured_outputs_config.enable_in_reasoning` is True, since the chat format is technically in the reasoning section of model output. It also lets us continue to use structured outputs for the content of the final message. `structured_outputs_config.reasoning_parser` is set by default for gpt-oss. 

TODO: test that structured output works alongside this after #28000 is merged, and adju

...</description>
  </item>
  <item>
    <title>[Quantization] quantized kv cache loading for models like qwen</title>
    <link>https://github.com/vllm-project/vllm/pull/27980</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27980</guid>
    <pubDate>Mon, 03 Nov 2025 12:03:40 +0000</pubDate>
    <description>## Purpose
This PR aims to fix quantized kv cache model loading issues using AMD-Quark. 
The fix was initially applied to the `gpt-oss` model and may be extended to other models in `vllm/model_executor/models`, as they share a nearly identical methodology.

Here is the original error traceback w/o this PR fixing.
&lt;img width=&quot;1219&quot; height=&quot;468&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/1ce1b92e-04a9-404a-91da-f06151cd54c9&quot; /&gt;

##
Here is a check list of some models defined in `vllm/model_executor/models/` for the supportness of quantized kv cache loading. Models with `get_cache_scale` called is marked as ‚úÖ, while models without `get_cache_scale` called is marked as ‚ö†Ô∏è.

The ‚ö†Ô∏è symbol is preferred over ‚ùå since some models can intrinsically have non-quantized KV cache, for i

...</description>
  </item>
  <item>
    <title>[Bugfix] Handle escaped characters in GLM tool parser to prevent double serialization</title>
    <link>https://github.com/vllm-project/vllm/pull/27970</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27970</guid>
    <pubDate>Mon, 03 Nov 2025 08:37:00 +0000</pubDate>
    <description>## Purpose

This PR fixes a bug where the tool call parser fails when the model&#x27;s output contains literal escaped characters, such as `\n`, `\&quot;`.

**The Problem:**
When GLM-4 outputs tool calls, the XML may contain &#x27;\n&#x27; literal characters and the JSON content within `&lt;arg_value&gt;` tags may contain escaped characters like `\&quot;`, causing regex matching and json parsing both failed. For example:

```
&lt;tool_call&gt;todo_write\n&lt;arg_key&gt;todos&lt;/arg_key&gt;\n&lt;arg_value&gt;[{\&quot;id\&quot;: \&quot;1\&quot;, \&quot;task\&quot;: \&quot;Ê£ÄÊü•ÂêéÁ´Ø‰ª£Á†Å‰∏≠ÁöÑÁ°¨ÁºñÁ†ÅÈóÆÈ¢ò\&quot;, \&quot;status\&quot;: \&quot;in_progress\&quot;}, {\&quot;id\&quot;: \&quot;2\&quot;, \&quot;task\&quot;: \&quot;Ê£ÄÊü•ÂâçÁ´Ø‰ª£Á†Å‰∏≠ÁöÑÁ°¨ÁºñÁ†ÅÈóÆÈ¢ò\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;3\&quot;, \&quot;task\&quot;: \&quot;Ê£ÄÊü•ËøùÂèçÂçï‰∏ÄËÅåË¥£ÁöÑ‰ª£Á†Å\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;4\&quot;, \&quot;task\&quot;: \&quot;ÂàõÂª∫Êï¥ÊîπÂª∫ËÆÆÊä•Âëä\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}]&lt;/arg_value&gt;
&lt;/tool_call&gt;
```

The current regex fails to ma

...</description>
  </item>
  <item>
    <title>[Metrics] [KVConnector] Add Offloading Connector metrics</title>
    <link>https://github.com/vllm-project/vllm/pull/27942</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27942</guid>
    <pubDate>Sun, 02 Nov 2025 15:36:12 +0000</pubDate>
    <description>Added queries and hits metrics for the Offloading Connector.
Also added timing metrics for store and load operations, which take the average time it takes to load/store, per-token.
The metrics are available from Prometheus and from the StatLogger.
## Purpose
Allows collection of timing metrics for the Offloading Connector, which is essential for future development.
@orozery please review.
## Test Plan

## Test Result

---

&gt; [!NOTE]
&gt; &lt;sup&gt;[Cursor Bugbot](https://cursor.com/dashboard?tab=bugbot) is generating a summary for commit c035d2f8f8b879dcd07eac2231070d023364b80a. Configure [here](https://cursor.com/dashboard?tab=bugbot).&lt;/sup&gt;

---

&gt; [!NOTE]
&gt; Introduces metrics and instrumentation for KV offloading transfers.
&gt; 
&gt; - Adds `OffloadingConnectorStats` and `OffloadPromMetrics` with hi

...</description>
  </item>
  <item>
    <title>[WIP] [GPT-OSS] customized symm_mem based EP comm kernel integration</title>
    <link>https://github.com/vllm-project/vllm/pull/27495</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27495</guid>
    <pubDate>Sat, 25 Oct 2025 01:04:50 +0000</pubDate>
    <description>Integrates Triton MoE communication kernels

achieved 60% throughput increase compared to #[24588](https://github.com/vllm-project/vllm/pull/24588)

&lt;img width=&quot;3918&quot; height=&quot;1996&quot; alt=&quot;36117&quot; src=&quot;https://github.com/user-attachments/assets/ad93165c-6869-4a01-8c5b-836f4421f656&quot; /&gt;
</description>
  </item>
  <item>
    <title>[CI/Build] Add aime25 eval to gpt-oss CI</title>
    <link>https://github.com/vllm-project/vllm/pull/27398</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27398</guid>
    <pubDate>Thu, 23 Oct 2025 07:38:49 +0000</pubDate>
    <description>## Purpose
This PR added eval config for gpt-oss-120b on H100. 

## Test Plan
pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-gpt-oss.txt --tp-size=1

## Test Result

1 passed, 6 warnings in 455.90s (0:07:35) 

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the releas

...</description>
  </item>
  <item>
    <title>[Bug]: gptoss calls built-in tool when no tools are given</title>
    <link>https://github.com/vllm-project/vllm/issues/27385</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27385</guid>
    <pubDate>Thu, 23 Oct 2025 04:01:36 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
==============================
        System Info
==============================
OS                           : Ubuntu 20.04.6 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-2ubuntu1~20.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 3.29.2
Libc version                 : glibc-2.31

==============================
       PyTorch Info
==============================
PyTorch version              : 2.8.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version

...</description>
  </item>
  <item>
    <title>[Bugfix] Actually enable serialize_messages for harmony Responses (related to #26185)</title>
    <link>https://github.com/vllm-project/vllm/pull/27377</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27377</guid>
    <pubDate>Thu, 23 Oct 2025 01:07:10 +0000</pubDate>
    <description>## Purpose

For the OpenAI-compatible `v1/responses` route, enable raw messages to be sent when `enable_response_messages` is set to True in `extra_body`.

Previously, the responses are empty because of an issue in openai/harmony. (https://github.com/openai/harmony/issues/78)

https://github.com/vllm-project/vllm/pull/26185 implements most of the fix, but these aren&#x27;t actually invoked, at least not when serving the model through the `vllm serve`. The reason is that the said PR specifies `when_used=&quot;json&quot;`. Thus, this serialization method is ignored because of the use of `model_dump()` in [vllm/entrypoints/openai/api_server.py#L527-L529](https://github.com/vllm-project/vllm/blob/a0003b56b0b822c52bb0f3035c164370a802e6f5/vllm/entrypoints/openai/api_server.py#L527-L529).

The fix is to trigger

...</description>
  </item>
  <item>
    <title>[Frontend][gpt-oss] Allow system message to overwrite model identity</title>
    <link>https://github.com/vllm-project/vllm/pull/27310</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27310</guid>
    <pubDate>Wed, 22 Oct 2025 00:57:44 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
The current way of implementation forces the system prompt to go into the developer message and it confuses the usage of different roles. We should have developer into developer message, and allow system message to override the system prompt as supported by signature of harmony_utils.py

## Test Result
Override
            &quot;openai_harmony_message&quot;: {
                &quot;author&quot;: {
                    &quot;role&quot;: &quot;system&quot;
                },
                &quot;content&quot;: [
                    {
                        &quot;system_content&quot;: {
                            &quot;type&quot;: &quot;system_content&quot;,
                            &quot;model_identity&quot;: &quot;You are a helpful agent reponds in Chinese.&quot;,
                            &quot;reasoning_effort&quot;: &quot;high&quot;,
                       

...</description>
  </item>
</channel>
</rss>
