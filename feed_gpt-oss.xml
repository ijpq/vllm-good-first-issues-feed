<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: gpt-oss</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'gpt-oss' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Sat, 07 Feb 2026 21:51:13 +0000</lastBuildDate>
  <item>
    <title>[ROCm][AITER] Add fused RoPE+KVCache pass with MultiOutputPattern fix</title>
    <link>https://github.com/vllm-project/vllm/pull/34037</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/34037</guid>
    <pubDate>Sat, 07 Feb 2026 03:48:21 +0000</pubDate>
    <description>## Summary

- Add `ROCmAiterTritonRopeReshapeKVCacheFusionPass` that fuses rotary embedding + reshape + KV cache update into a single AITER Triton kernel (`fused_qk_rope_reshape_and_cache`)
- **Fix for #33666:** PyTorch&#x27;s `MultiOutputPattern` only traces the first output&#x27;s subgraph for anchor node discovery. By returning `(dummy, q, k, v)` instead of `(q, k, v, dummy)`, the `unified_kv_cache_update` node (most connected subgraph) is traced first, ensuring all anchor nodes are discovered and pattern matching succeeds
- Refactor KV sharing / None guards from backend `do_kv_cache_update` methods to the caller in `attention.py` for cleaner separation of concerns
- Add `fuse_rope_kvcache` config option with `aiter_rope_kvcache_fusion_max_token_num` threshold (default 256) for small-batch decode

...</description>
  </item>
  <item>
    <title>[ROCm] update triton branch to support gpt-oss models for gfx11xx devices</title>
    <link>https://github.com/vllm-project/vllm/pull/34032</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/34032</guid>
    <pubDate>Sat, 07 Feb 2026 02:00:53 +0000</pubDate>
    <description>
Fix https://github.com/vllm-project/vllm/issues/33906
Fix https://github.com/vllm-project/vllm/issues/33143

To add support to gfx11xx devices to serve gpt-oss (20b for example) models , we need to update triton branch in Dockerfile.rocm_base.
The needed support was in upstream triton, but not in the branch used by vllm base docker.
The patch to rocm/triton was done using this associated PR  https://github.com/ROCm/triton/pull/923.


&lt;!-- markdownlint-disable --&gt;

## Purpose
To add support to gfx11xx devices for gpt-oss models 

(associated PR  https://github.com/ROCm/triton/pull/923 ).

## Test Plan
Build the base docker locally, and then build vllm docker.
Verified on gfx1100 devices. 
Do online serve verification

## Test Result
Verified on gfx1100 devices on `vllm serve` using `gpt-os

...</description>
  </item>
  <item>
    <title>[CI][MCP][Harmony] Heavy refactoring Harmony &amp; MCP response tests and stabilizing with deterministic test infrastructure</title>
    <link>https://github.com/vllm-project/vllm/pull/33949</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33949</guid>
    <pubDate>Thu, 05 Feb 2026 23:26:04 +0000</pubDate>
    <description>This PR eliminates systemic test flakiness in the Harmony and MCP Responses API integration tests by addressing root causes in both the test infrastructure and source code. The core problem was that tests asserted on non-deterministic LLM output using `@pytest.mark.flaky`, which corrupted server fixture lifecycles and masked real failures. This PR replaces that pattern with deterministic infrastructure: pinned system prompts, API-level retries, and a clear separation between server invariants (hard assertions) and model behaviour (soft/xfail).

## Motivation

The following tests were consistently flaky in CI:

- `test_mcp_tool_env_flag_enabled`
- `test_mcp_tool_with_allowed_tools_star`
- `test_mcp_tool_calling_streaming_types`
- `test_mcp_code_interpreter_streaming`
- `test_system_prompt_o

...</description>
  </item>
  <item>
    <title>[Refactor] Replace `activation: str` with `MoEActivation` enum</title>
    <link>https://github.com/vllm-project/vllm/pull/33843</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33843</guid>
    <pubDate>Wed, 04 Feb 2026 23:38:05 +0000</pubDate>
    <description>
## Purpose

We have had `activation` defined, validated, and passed around as a raw string forever. Now that we have popular models that don&#x27;t just use the `silu` default and also need to support non-gated MoEs and their activation functions, we need to have a single source of truth for all the activation functions that exist in vLLM and which MoE kernels support which functions. I want to start with MoE since that is where we have the most divergence in support due to fused kernels.

This PR introduces `MoEActivation` and aims to replace all usage of `activation: str` in internal interfaces for MoE. Note that we do keep `FusedMoE.__init__` as `activation: str` in order to avoid changes to the model definitions, however we do validate it with `MoEActivation.from_str(activation)` within th

...</description>
  </item>
  <item>
    <title>add video [Model] Video Frame Sparsification via Pixel-Level Similarity for Efficient Multimodal Long-Video Inference for Qwen2.5_vl and Qwen3_vlsparse</title>
    <link>https://github.com/vllm-project/vllm/pull/33780</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33780</guid>
    <pubDate>Wed, 04 Feb 2026 09:10:49 +0000</pubDate>
    <description>## Purpose
RFC: [#31803](https://github.com/vllm-project/vllm/issues/31803)

The exponential growth of video content and the rising demand for long-video intelligence (e.g., video summarization, multimodal QA, video generation) have made long-video inference a standard requirement for multimodal large models. Current models suffer from critical efficiency bottlenecks and face two key challenges: 

temporal and spatial redundancy of videos and input sequence overflow, thus necessitating a lightweight, adaptive, and scalable sparsification solution. 

This PR proposes a video sparsification scheme that adaptively selects key frames based on video inter-frame similarity and user-defined sparsity rates, and it has been compatible with Qwen2.5-VL and Qwen3-VL.

## Implementation details

This s

...</description>
  </item>
  <item>
    <title>[Draft] Move Harmony encoding to renderer layer and add gRPC render server</title>
    <link>https://github.com/vllm-project/vllm/pull/33693</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33693</guid>
    <pubDate>Tue, 03 Feb 2026 12:33:54 +0000</pubDate>
    <description>Background
I’ve concluded that integrating Harmony into the Renderer layer is essential for a complete and functional [render implementation](https://github.com/vllm-project/vllm/issues/32648#issuecomment-3774625001). I’ve opened this Draft PR to verify if this proposed final architecture aligns with the project&#x27;s direction.

If the direction is confirmed, I plan to break this down into smaller, manageable PRs.

Changes
1. Added uses_harmony property (9e6a89e)
  - Introduced the uses_harmony property to BaseRenderer.
  - Updated chat_completion/serving.py and responses/serving.py to use self.renderer.uses_harmony instead of self.use_harmony.
2. Migrated Harmony encoding logic to the Renderer layer ([80ce4a6](https://github.com/vllm-project/vllm/pull/33693/commits/80ce4a61a0fafef6df9da8b1cd

...</description>
  </item>
  <item>
    <title>Allow other models to use the Harmony format</title>
    <link>https://github.com/vllm-project/vllm/pull/33610</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33610</guid>
    <pubDate>Mon, 02 Feb 2026 22:35:07 +0000</pubDate>
    <description>This PR allows other, non-GPT-OSS models, to be used with the OpenAI Harmony format.

Prior to this PR, the `openai` tool call parser would crash with &quot;OpenAIToolParser requires token IDs and does not support text-based extraction.&quot; for any such model.</description>
  </item>
  <item>
    <title>[Model] Use layer_types config for sliding window selection in GPT-OSS</title>
    <link>https://github.com/vllm-project/vllm/pull/33594</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33594</guid>
    <pubDate>Mon, 02 Feb 2026 21:20:09 +0000</pubDate>
    <description>## Summary
- Read layer_types from model config instead of hardcoded even/odd pattern
- sliding_attention layers use sliding window, full_attention uses full context
- Fallback to legacy behavior if layer_types not specified

## Test plan
- [ ] Test with GPT-OSS model that has layer_types in config
- [ ] Verify fallback behavior with model without layer_types</description>
  </item>
  <item>
    <title>[Bugfix] Fix tool call streaming for gpt-oss/Harmony models</title>
    <link>https://github.com/vllm-project/vllm/pull/33520</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33520</guid>
    <pubDate>Sun, 01 Feb 2026 20:26:20 +0000</pubDate>
    <description>## Summary

This PR fixes several issues with tool call handling for gpt-oss models using the Harmony streaming parser:

1. **IndexError in streaming generator**: Added `auto_tools_called` check before accessing `prev_tool_call_arr` to prevent IndexError when the array is empty.

2. **Missing tool call IDs in non-streaming responses**: Added proper ID generation for named tool choice and auto tool choice cases that were missing the required `id` field.

3. **Split tool calls in streaming**: Fixed an issue where a single tool call was being split into multiple entries because:
   - Tool call IDs are now stored by recipient name (e.g., `functions.glob`) instead of index number, since `base_index` changes between streaming calls as messages complete.
   - Continuation chunks now include the s

...</description>
  </item>
  <item>
    <title>[Bugfix] Fix gpt-oss chat format mismatch with HuggingFace</title>
    <link>https://github.com/vllm-project/vllm/pull/33514</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33514</guid>
    <pubDate>Sun, 01 Feb 2026 18:41:59 +0000</pubDate>
    <description>## Purpose


Fix gpt-oss chat format mismatch with HuggingFace `apply_chat_template` output.                                                                                                                                                  
                                                                                                                                                                                                                                   
Fixes [#33210](https://github.com/vllm-project/vllm/issues/33210)

                                                                                                              
**Problem:**                                                                                                                                               

...</description>
  </item>
  <item>
    <title>[ROCm] AITER fused RoPE+KVCache</title>
    <link>https://github.com/vllm-project/vllm/pull/33443</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33443</guid>
    <pubDate>Fri, 30 Jan 2026 19:27:35 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Follow-up to #25954; adda a `fuse_rope_kvcache` Inductor pass to fuse QK RoPE and KVCache ops into the AITER fused kernel.

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://

...</description>
  </item>
  <item>
    <title>PR Title: [Feature] EWSJF: Adaptive Scheduler for Mixed-Workloads (30-50% Throughput Gain)</title>
    <link>https://github.com/vllm-project/vllm/pull/33392</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33392</guid>
    <pubDate>Fri, 30 Jan 2026 06:10:50 +0000</pubDate>
    <description>**Summary**  
This PR introduces EWSJF (Effective Workload‑based Shortest Job First), an adaptive request‑level scheduler designed to eliminate Head‑of‑Line (HoL) blocking in mixed workloads that combine short interactive queries with long batch requests.

The design aligns with RFC #24484 on mitigating HoL blocking and RFC #24484 on pluggable scheduler interfaces.

By grouping requests into performance‑homogeneous queues and applying a density‑weighted scoring function, EWSJF improves both throughput and tail latency compared to the default FCFS scheduler.

**Key Results**  
Benchmarks (Llama‑2‑13B, NVIDIA A100, Poisson arrivals):

- Token Throughput: +30% to +54%  
- TTFT (Short Requests): Up to 4× reduction  
- GPU Utilization: ~65% → ~80%

Full technical report: https://arxiv.org/abs/2

...</description>
  </item>
  <item>
    <title>Drafter Supports Multiple KVCache Groups</title>
    <link>https://github.com/vllm-project/vllm/pull/33318</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33318</guid>
    <pubDate>Thu, 29 Jan 2026 07:33:54 +0000</pubDate>
    <description>## Summary

This PR enables models with multiple KV-cache groups to be used as drafters in speculative decoding. Previously, the speculative decoding infrastructure assumed a single KV-cache group, which prevented the use of architectures like Gemma3 and GPT-OSS MoE models as drafters.

**Key changes:**
- Refactored `CommonAttentionMetadata` handling to support a dictionary of metadata per KV-cache group ID (`CommonAttnMetadataByGid`)
- Added per-group slot-mapping buffers for draft model inference
- Introduced `layer_names_to_kv_cache_gid` mapping to correctly route attention layers to their corresponding KV-cache groups

Fixes https://github.com/vllm-project/vllm/issues/33133

&lt;details&gt;
&lt;summary&gt;Try it: GPT-OSS 120b + 20b&lt;/summary&gt;

```bash
vllm serve openai/gpt-oss-120b \
  --no-enable-

...</description>
  </item>
  <item>
    <title>[Frontend] Add tool_choice=required support for GPT-OSS Harmony models</title>
    <link>https://github.com/vllm-project/vllm/pull/33306</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33306</guid>
    <pubDate>Thu, 29 Jan 2026 05:22:47 +0000</pubDate>
    <description>## Summary

This PR adds `tool_choice=&quot;required&quot;` support for GPT-OSS Harmony models.  
When `tool_choice=&quot;required&quot;` is set, the model is forced to generate tool calls instead of plain text responses.

---

## Problem

GPT-OSS models use the Harmony chat format, which differs from standard models in its response generation behavior.  
Even when `tool_choice=&quot;required&quot;` is set, these models tend to generate direct text responses instead of tool calls, resulting in only **91% success rate** for tool call generation.

---

## Solution

Harmony format models select one of three channels (`final`, `analysis`, `commentary`) after completing their internal processing.  
Tool calls are only generated through the `commentary` channel with a specified recipient.

This PR uses a **LogitsProcessor-ba

...</description>
  </item>
  <item>
    <title>[Quantization] - Consolidate experts_int8 with FP8 Modular Kernels</title>
    <link>https://github.com/vllm-project/vllm/pull/33178</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/33178</guid>
    <pubDate>Tue, 27 Jan 2026 15:39:06 +0000</pubDate>
    <description>## Purpose
Consolidates `experts_int8` and `Fp8OnlineMoEMethod` to share common weight loading and quantization logic through a new `MoeOnlineWeightLoader` class.

  Key changes:
  - Introduced `MoeOnlineWeightLoader` in `moe_weight_loader.py` that handles weight loading for MoE layers
  - Created `MoeQuantizationCallbacks` protocol that both `int8` and `fp8` methods implement
  - Both quantization methods now only need to provide:
    - get_quantized_dtype() - target dtype (int8 or fp8)
    - quantize_expert(weight) - single expert quantization logic
    - create_scale_tensors() - scale tensor creation (per-channel for int8, per-tensor for fp8)
    - setup_kernel() - kernel setup after quantization

Diagram of the consolidation and inheritance between fp8 and experts_int8 
&lt;img width=&quot;494

...</description>
  </item>
  <item>
    <title>[WIP] Fix GPT-OSS prefix caching not working with EAGLE</title>
    <link>https://github.com/vllm-project/vllm/pull/32801</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32801</guid>
    <pubDate>Wed, 21 Jan 2026 19:22:12 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Misc] Add JSON format logging support</title>
    <link>https://github.com/vllm-project/vllm/pull/32761</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32761</guid>
    <pubDate>Wed, 21 Jan 2026 07:34:06 +0000</pubDate>
    <description>Add support of  structured logging.

Inspired by https://github.com/vllm-project/vllm/pull/2432 and https://github.com/vllm-project/vllm/pull/13920, but takes a slightly different approach.

Example of `standard` output:
```
Traceback (most recent call last):
  File &quot;vllm/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py&quot;, line 402, in hf_raise_for_status
    response.raise_for_status()
  File &quot;vllm/.venv/lib/python3.10/site-packages/requests/models.py&quot;, line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent 

...</description>
  </item>
  <item>
    <title>[Core] Support `min_tokens` with speculative decoding</title>
    <link>https://github.com/vllm-project/vllm/pull/32642</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32642</guid>
    <pubDate>Tue, 20 Jan 2026 06:34:03 +0000</pubDate>
    <description>
## Purpose

Allow `min_tokens` in speculative decoding by masking stop tokens during rejection sampling.

## Test Plan

Run the server with speculative decoding enabled (example):

1. 
```sh
vllm serve deepseek-ai/DeepSeek-V3.2 \
		--tensor-parallel-size 8 \
		--tokenizer-mode deepseek_v32 \
		--tool-call-parser deepseek_v32 \
		--enable-auto-tool-choice \
		--reasoning-parser deepseek_v3 \
		--speculative_config &#x27;{&quot;num_speculative_tokens&quot;:1, &quot;method&quot;:&quot;deepseek_mtp&quot;}&#x27;
```

2.
```sh
CUDA_VISIBLE_DEVICES=0 vllm serve meta-llama/Llama-3.1-8B-Instruct     --tensor-parallel-size 1 --speculative_config &#x27;{&quot;num_speculative_tokens&quot;:1, &quot;method&quot;:&quot;eagle3&quot;,&quot;model&quot;:&quot;lmsys/SGLang-EAGLE3-Llama-3.1-8B-Instruct-SpecForge&quot;}&#x27; --max-model-len 16384
```

Then exercise `min_tokens` in an API call:

1.
```sh
cur

...</description>
  </item>
  <item>
    <title>Fix gpt‑oss Harmony token leaks in tool names and streaming content #32587</title>
    <link>https://github.com/vllm-project/vllm/pull/32633</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32633</guid>
    <pubDate>Tue, 20 Jan 2026 04:19:50 +0000</pubDate>
    <description>
## Purpose
 #32587

Defined sanitize_harmony_tool_name and strip_harmony_control_tokens in harmony_utils.py.

Used the sanitizer when reconstructing Harmony messages from chat/response history and when converting Harmony outputs back into API responses in harmony_utils.py.

Applied the sanitizer in tool extraction for chat completions (non‑stream) and stripped control tokens from final/commentary content in openai_tool_parser.py.

Applied the sanitizer in streaming deltas for chat completions and stripped control tokens from streamed content/reasoning deltas in stream_harmony.py.

Applied the sanitizer for function call events and stripped control tokens from response streaming deltas and done events in serving.py.

## Test Plan
```
python -m vllm.entrypoints.openai.api_server \
  --model

...</description>
  </item>
  <item>
    <title>[MoE Refactor] Create MK for TRTLLM Kernels</title>
    <link>https://github.com/vllm-project/vllm/pull/32564</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32564</guid>
    <pubDate>Mon, 19 Jan 2026 02:12:32 +0000</pubDate>
    <description>## Purpose
* convert TRTLLM Kernels into the modular kernel framework
* introduce the concept of a monolithic kernel to the mk framework
* remove HACK for the nvfp4 quant pre-AG

MoE refactor CI

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://

...</description>
  </item>
  <item>
    <title>[Refactor] Extract KV-cache update logic for FlashAttentionDiffKV backend</title>
    <link>https://github.com/vllm-project/vllm/pull/32509</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32509</guid>
    <pubDate>Sat, 17 Jan 2026 08:28:16 +0000</pubDate>
    <description>This PR extracts KV-cache update logic from the forward method for the FlashAttentionDiffKV backend, following the pattern established in #25954.

Changes:
- Added forward_includes_kv_cache_update: bool = False flag to FlashAttentionDiffKVBackend
- Extracted KV-cache update logic into do_kv_cache_update method in FlashAttentionDiffKVImpl
- Removed KV-cache update logic from the forward method

Related to issue #32335</description>
  </item>
  <item>
    <title>[Frontend] Add encrypted_content to reasoning items for round-tripping</title>
    <link>https://github.com/vllm-project/vllm/pull/32247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32247</guid>
    <pubDate>Tue, 13 Jan 2026 09:24:44 +0000</pubDate>
    <description>  ## Purpose

  Add optional encryption for the `encrypted_content` field in reasoning items. This closes a gap in vLLM&#x27;s implementation of OpenAI&#x27;s Responses API, enabling compatibility with clients like **Codex CLI** that rely on `encrypted_content` for multi-turn conversation continuity.

  **Key features:**
  - Opt-in via environment variable (disabled by default)
  - Uses Fernet symmetric encryption from the `cryptography` package
  - Supports custom key via `VLLM_REASONING_ENCRYPTION_KEY` for multi-instance deployments
  - Backward compatible with plain JSON content

  ## Test Plan

  ```bash
  # Start vLLM with encryption enabled
  export VLLM_ENCRYPT_REASONING_CONTENT=1
  vllm serve &lt;model&gt; --port 8000

  # Send request
  curl -X POST http://localhost:8000/v1/responses \
    -H &quot;Co

...</description>
  </item>
  <item>
    <title>[Bugfix] Fix Harmony preamble visibility in Responses API</title>
    <link>https://github.com/vllm-project/vllm/pull/32114</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32114</guid>
    <pubDate>Sun, 11 Jan 2026 12:40:51 +0000</pubDate>
    <description>##  Purpose

Fix incorrect handling of Harmony format preambles in the Responses API.

Per the https://cookbook.openai.com/articles/openai-harmony, preambles (commentary channel messages with recipient=None) are explanatory text intended to be shown to end-users (e.g., &quot;I&#x27;ll search for that information now...&quot;). The current implementation incorrectly treats them as hidden reasoning content.

This PR changes how commentary-channel preambles are parsed:
  - Before: ResponseReasoningItem (hidden in reasoning_content)
  - After: ResponseOutputMessage (visible in content)

Related: This is a follow-up fix to #29972, which added GPT-OSS models with Harmony chat format.

## Migration Note

Breaking Change: Preambles now appear in content instead of reasoning_content. Clients that relied on the pr

...</description>
  </item>
  <item>
    <title>[RFC] Improve environment variable declaration and handling (#31249)</title>
    <link>https://github.com/vllm-project/vllm/pull/32070</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32070</guid>
    <pubDate>Sat, 10 Jan 2026 01:06:48 +0000</pubDate>
    <description>This PR implements the refactoring proposed in issue #31249 to improve environment variable handling in vLLM by consolidating declarations into a single source of truth with automatic type conversion.

## Summary

Replaces the duplicated type definitions and getter dictionary pattern with a unified system that:
- Declares each variable once with type annotations and defaults
- Automatically converts types based on type hints
- Supports custom parsing, lazy defaults, and validated choices
- Provides better IDE support through proper docstrings
- Includes pre-commit validation to prevent errors

## Changes

### New Package Structure
- `vllm/envs/__init__.py`: Module interface with `__getattr__` for dynamic variable access and automatic type conversion
- `vllm/envs/_variables.py`: Single sour

...</description>
  </item>
  <item>
    <title>[Frontend] Add `reasoning_effort` and `parallel_tool_calls` to `extra_args` of `SamplingParams`.</title>
    <link>https://github.com/vllm-project/vllm/pull/31952</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31952</guid>
    <pubDate>Thu, 08 Jan 2026 07:25:45 +0000</pubDate>
    <description>## Purpose

~~I suggest adding two fields, `reasoning_effort` and `parallel_tool_calls`, to `SamplingParams` to enable logits processors to support the OpenAI-compatible parameters by controlling token generation.~~

I suggest adding two fields, `reasoning_effort` and `parallel_tool_calls`, to `extra_args` of `SamplingParams` to enable logits processors to support the OpenAI-compatible parameters by controlling token generation.

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e 

...</description>
  </item>
  <item>
    <title>LoRA Per Request Loading Pipelining Support</title>
    <link>https://github.com/vllm-project/vllm/pull/31941</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31941</guid>
    <pubDate>Thu, 08 Jan 2026 03:27:58 +0000</pubDate>
    <description>## Purpose
Currently LoRA Loading for Per Request needs to first sequentially **load all the LoRA layers** before **beginning Pre-Fill**. This adds significant overhead in TTFT. 
&lt;img width=&quot;1522&quot; height=&quot;545&quot; alt=&quot;Screenshot 2026-01-07 at 6 54 53 PM&quot; src=&quot;https://github.com/user-attachments/assets/75932a7d-c8d2-47b2-a121-ab40d5435665&quot; /&gt;

However, **Pre-Fill** is a layer by layer process and for large enough input tokens(**&gt; 1000 tokens**) it is **longer than LoRA Loading time**. If we run **LoRA loading and Compute in separate streams(CUDA or otherwise)** we can hid significant part of the LoRA loading  time with **Sync points** in each LoRA layer of the forward pass to make sure the LoRA layer is loaded.

&lt;img width=&quot;1313&quot; height=&quot;467&quot; alt=&quot;Screenshot 2026-01-07 at 6 59 12 PM&quot; src=&quot;http

...</description>
  </item>
  <item>
    <title>[Misc] Add VLLM_USE_FLASHINFER_ROPE to control the RoPE kernel for cuda</title>
    <link>https://github.com/vllm-project/vllm/pull/31893</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31893</guid>
    <pubDate>Wed, 07 Jan 2026 10:49:29 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Related PRs:
#21126 integrated the Flashinfer RoPE kernel.
#25299 disabled it by default due to CI failures.
#30729 enabled it by default for Deepseek RoPE.

This PR adds `VLLM_USE_FLASHINFER_ROPE` env so that we can easily enable Flashinfer RoPE kernel.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes up

...</description>
  </item>
  <item>
    <title>[responsesAPI] get reasoning token metrics for simpleContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31839</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31839</guid>
    <pubDate>Tue, 06 Jan 2026 22:18:30 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Core] Add optional flags to check for repetitive token patterns in engine output</title>
    <link>https://github.com/vllm-project/vllm/pull/31690</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31690</guid>
    <pubDate>Sun, 04 Jan 2026 21:13:48 +0000</pubDate>
    <description>## Purpose
This PR allows users to end generation early if they see repetitive token patterns (for example, nonsensically repeated sentences) in their output. This is a real failure case for models prone to hallucination. Testing with Qwen3 VL on multimodal input shows a ~33% increase in throughput and near-total reduction in models hitting the configured max_output_len.

This feature is off by default. We include it in the core so that this can be leveraged for non-streaming generation too.

## Test Plan
New test file, online tests

## Test Result
Huge increase in throughput for error-prone multimodal input without sacrificing quality/latency.
</description>
  </item>
  <item>
    <title>[Bugfix] Sanitize malformed tool call recipients in Harmony parser</title>
    <link>https://github.com/vllm-project/vllm/pull/31677</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31677</guid>
    <pubDate>Sun, 04 Jan 2026 09:30:09 +0000</pubDate>
    <description>Some GPT-OSS base models occasionally generate malformed Harmony format sequences like `to=functions.bash&lt;|channel|&gt;commentary` instead of the correct `to=functions.bash &lt;|constrain|&gt;json`. This causes the function name to be parsed incorrectly as `bash&lt;|channel|&gt;commentary` instead of `bash`.

This fix sanitizes the recipient string by stripping `&lt;|channel|&gt;` and everything after it before extracting the function name. The fix is applied in three locations to cover all API endpoints:

- `harmony_utils.py`: /v1/responses (non-streaming)
- `openai_tool_parser.py`: /v1/chat/completions (non-streaming)
- `serving_chat_stream_harmony.py`: /v1/chat/completions (streaming)

The /v1/responses streaming endpoint already worked correctly because it captures the recipient before malformed tokens can

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31250</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31250</guid>
    <pubDate>Wed, 24 Dec 2025 00:18:46 +0000</pubDate>
    <description>This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and allows us to reuse the weights wh

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31247</guid>
    <pubDate>Tue, 23 Dec 2025 23:27:39 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;
This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and all

...</description>
  </item>
  <item>
    <title>[Feature] add StreamableResponsesParser and token usage counting for ParsableContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31094</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31094</guid>
    <pubDate>Sun, 21 Dec 2025 11:11:46 +0000</pubDate>
    <description># Purpose

#30759 

- Count the token usage for ParsableContext properly, including `reasoning_tokens` and `output_tokens` and `tool_output_tokens`.
- A StreamableResposesParser is created to parse output token one by one, which can also be used to create an StreamParsableContext

# Design

## Overview

This PR introduces a new `StreamableResponsesParser` that processes output tokens one by one, enabling accurate token counting for reasoning tokens in `ParsableContext`.

## Key Changes

### 1. StreamableResponsesParser (`vllm/entrypoints/openai/parser/responses_parser.py`)

A new streaming parser that processes tokens incrementally:

- **`current_channel`** property: Identifies the current token type
  - `&#x27;analysis&#x27;`: reasoning token
  - `&#x27;commentary&#x27;`: tool call token
  - `&#x27;final&#x27;`: regul

...</description>
  </item>
  <item>
    <title>Add positional embedding and kv_cache fusion for llama and gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30978</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30978</guid>
    <pubDate>Thu, 18 Dec 2025 17:57:04 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Perf] Eliminate padding and slicing op for GPT-OSS with Flashinfer MXFP4 MXFP8 MoE</title>
    <link>https://github.com/vllm-project/vllm/pull/30647</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30647</guid>
    <pubDate>Sun, 14 Dec 2025 13:40:05 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

- Depends on Flashinfer update #30993
- Eliminated padding op before the MoE: by setting the alignment in flashinfer mxfp8 quant, the output quantized tensor will be padded.
- Eliminated slicing op after the MoE: by passing the output tensor with unpadded hidden size to MoE kernel, this depends on a Flashinfer PR:
  - https://github.com/flashinfer-ai/flashinfer/pull/2217
  - This will also resolve the previous AR+Norm fusion broken by slice op issue: #28841
- Cleaned up the padding logic: for mxfp4 quant, the padded hidden size is calculated in `create_weights()`, the `maybe_roundup_hidden_size()` in `vllm/model_executor/layers/fused_moe/layer.py` seems like a dup.


## Test Plan &amp;&amp; Test Result(GPT-OSS-120b TP8)

### Accuracy

PR:
```
[{&#x27;eval_name

...</description>
  </item>
  <item>
    <title>[GPT OSS] Fix tool_choice required</title>
    <link>https://github.com/vllm-project/vllm/pull/30557</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30557</guid>
    <pubDate>Fri, 12 Dec 2025 14:30:24 +0000</pubDate>
    <description></description>
  </item>
  <item>
    <title>[Release 2.10] Update to Torch 2.10 - final release</title>
    <link>https://github.com/vllm-project/vllm/pull/30525</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30525</guid>
    <pubDate>Thu, 11 Dec 2025 23:52:48 +0000</pubDate>
    <description>&gt; [!NOTE]
&gt; &lt;sup&gt;[Cursor Bugbot](https://cursor.com/dashboard?tab=bugbot) is generating a summary for commit 37af14c52963e6d528866dded7f51985a1fcef7e. Configure [here](https://cursor.com/dashboard?tab=bugbot).&lt;/sup&gt;

---

&gt; [!NOTE]
&gt; **Upgrade to PyTorch 2.10.0 across the project**
&gt; 
&gt; - Bumps `torch/torchaudio/torchvision` to `2.10.0` (and `torchvision 0.25.0`), updates CMake supported torch versions to `2.10.0`, and refreshes related deps (`triton 3.6.0`, `nvidia-nvshmem-cu12 3.4.5`).
&gt; - Docker: switch PyTorch index to `.../whl/test`, enable `--prerelease=allow` for installs, and plumb test indexes in CUDA and CPU images; add extra-index usage to the python-only compile test.
&gt; - CI/tooling: pre-commit `pip-compile` now uses test cu129 index; Prime-RL script force-reinstalls torch/visi

...</description>
  </item>
  <item>
    <title>[Frontend] Honor chat template for gpt-oss harmony (#23015)</title>
    <link>https://github.com/vllm-project/vllm/pull/30482</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30482</guid>
    <pubDate>Thu, 11 Dec 2025 10:23:37 +0000</pubDate>
    <description>## Issue
- For gpt-oss models, --chat-template (and tokenizer/chat_template_kwargs) are ignored; prompts are rendered via Harmony instead of the normal chat-template pipeline.
## Reason
- When hf_config.model_type == &quot;gpt_oss&quot;, vLLM sets use_harmony and routes requests through _make_request_with_harmony(...), which directly constructs Harmony system/developer/user messages and calls render_for_completion. That path never calls _preprocess_chat / apply_hf_chat_template, so any chat template settings are bypassed.
## Summary
- allow server-level chat_template to be applied even in Harmony (gpt-oss) paths for chat and responses
- pass tokenizer into Harmony preprocessing and render via apply_hf_chat_template when provided, with safe fallback to Harmony default on errors
- tighten typing/forma

...</description>
  </item>
  <item>
    <title>[Optimization]: Add fused router for GPTOSS</title>
    <link>https://github.com/vllm-project/vllm/pull/30471</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30471</guid>
    <pubDate>Thu, 11 Dec 2025 07:55:47 +0000</pubDate>
    <description>&lt;details&gt;
&lt;summary&gt; output collect_env &lt;/summary&gt;

```
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 24.04.3 LTS (x86_64)
GCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version                : Could not collect
CMake version                : version 3.28.3
Libc version                 : glibc-2.39

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version               : 3.12.11

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] GPT OSS, DS FP4 Upstream fp8 with static scales</title>
    <link>https://github.com/vllm-project/vllm/pull/30357</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30357</guid>
    <pubDate>Tue, 09 Dec 2025 19:49:32 +0000</pubDate>
    <description>## gpt-oss120b-w-mxfp4-a-fp8

server:
&gt; VLLM_ROCM_USE_AITER=1 HIP_VISIBLE_DEVICES=1 VLLM_USE_AITER_UNIFIED_ATTENTION=1 VLLM_ROCM_USE_AITER_MHA=0 vllm serve /data/models/gpt-oss120b-w-mxfp4-a-fp8     --port 8000     --swap-space 64     --max-model-len 10368     --tensor-parallel-size 1     --max-num-seqs 1024     --gpu-memory-utilization 0.95     --no-enable-prefix-caching     --enforce-eager

client:
&gt; curl http://localhost:8000/v1/completions -H &quot;Content-Type: application/json&quot; -d &#x27;{
    &quot;model&quot;: &quot;/data/models/gpt-oss120b-w-mxfp4-a-fp8&quot;,
    &quot;prompt&quot;: &quot;San Francisco is a&quot;,
    &quot;max_tokens&quot;: 16,
    &quot;temperature&quot;: 0
}&#x27;

output:
&gt; {&quot;id&quot;:&quot;cmpl-9e80e71642c56933&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1765949222,&quot;model&quot;:&quot;/data/models/gpt-oss120b-w-mxfp4-a-fp8&quot;,&quot;choices&quot;:[{&quot;index&quot;:0,&quot;text&quot;:&quot; city

...</description>
  </item>
  <item>
    <title>[gpt-oss] Add model_identity to system message retrieval for harmony chat template</title>
    <link>https://github.com/vllm-project/vllm/pull/30247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30247</guid>
    <pubDate>Mon, 08 Dec 2025 08:43:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
When using GPT-OSS, `model_identity` passed in from `chat_template_kwargs` is not being used to adjust model&#x27;s identity. This PR is to pass the `model_identity` from chat/completion to the system message formatter.
## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If you

...</description>
  </item>
  <item>
    <title>[Perf] Add benchmark script for triton unified attention kernel performance</title>
    <link>https://github.com/vllm-project/vllm/pull/30191</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30191</guid>
    <pubDate>Sun, 07 Dec 2025 03:13:29 +0000</pubDate>
    <description>
## Purpose
Add a script for benchmarking triton unified attention kernel performance and compare against flash attention. This could be useful for testing new hardware performance etc 

## Test Plan
```

(vllm) ubuntu@209-20-157-255:~/vllm$ python benchmarks/kernels/benchmark_triton_unified_attention.py
flash_attn not available. Only Triton Unified Attention will be benchmarked.
================================================================================
Triton Unified Attention Benchmark
================================================================================
Configuration:
  dtype: torch.bfloat16
  num_query_heads: 32
  num_kv_heads: 8
  head_size: 128
  block_size: 16
  use_alibi: False
  sliding_window: -1
  softcap: 0.0
  FlashAttention available: False
==================

...</description>
  </item>
  <item>
    <title>[Feature] Add track_token_ids for Efficient Selective Token Logprobs Tracking</title>
    <link>https://github.com/vllm-project/vllm/pull/30030</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30030</guid>
    <pubDate>Thu, 04 Dec 2025 07:04:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
This PR adds a new `track_token_ids` parameter to `SamplingParams` that enables efficient tracking of log probabilities (logprobs) for specific tokens without requiring full vocabulary logprobs retrieval. This feature addresses [Issue#29280](https://github.com/vllm-project/vllm/issues/29280)

## Implementation Overview

The feature is implemented across the vLLM V1 engine pipeline, from parameter validation to output processing.

### 1. Parameter Extension (`vllm/sampling_params.py`)

Added `track_token_ids: list[int] | None` parameter with validation ensuring all IDs are non-negative integers:

```python
track_token_ids: list[int] | None = None
&quot;&quot;&quot;List of token IDs to track logprobs for at every generation step.&quot;&quot;&quot;
```

### 2. New Data Structures 

...</description>
  </item>
  <item>
    <title>[Model] Add LoRA support for Whisper models</title>
    <link>https://github.com/vllm-project/vllm/pull/29856</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29856</guid>
    <pubDate>Tue, 02 Dec 2025 08:56:32 +0000</pubDate>
    <description>## Purpose

This PR enables Multi-LoRA support for Whisper speech-to-text models, allowing users to serve multiple fine-tuned Whisper adapters from a single base model.

### Background

Currently, vLLM&#x27;s `WhisperForConditionalGeneration` does not implement the `SupportsLoRA` interface, preventing users from using LoRA adapters with Whisper models. This limitation requires users to deploy
 separate model instances for each fine-tuned variant, which is inefficient in terms of GPU memory usage.

### Changes

**1. `vllm/model_executor/models/whisper.py`**
- Add `SupportsLoRA` interface to `WhisperForConditionalGeneration`
- Add `embedding_modules` and `embedding_padding_modules` attributes required by LoRA
- Update `packed_modules_mapping` to use simplified keys (`qkv_proj`, `kv_proj`) for LoR

...</description>
  </item>
  <item>
    <title>Added regression test for openai/harmony/issues/78</title>
    <link>https://github.com/vllm-project/vllm/pull/29830</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29830</guid>
    <pubDate>Tue, 02 Dec 2025 01:06:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Add regression tests for [openai/harmony#78](https://github.com/openai/harmony/issues/78), particularly the FastAPI `vllm serve` path which wasn&#x27;t fully addressed in #26185.

## Test Plan

Modify the existing `test_output_messages_enabled` test in test_response_api_with_harmony.py to add the following validations:

```python
    for _message in [*response.input_messages, *response.output_messages]:
        for _item in _message.get(&quot;content&quot;):
            assert isinstance(_item, dict), _message
            assert len(_item) &gt; 0, _message
```

&lt;details&gt;&lt;summary&gt;full code details&lt;/summary&gt;

```python
@pytest.mark.asyncio
@pytest.mark.parametrize(&quot;model_name&quot;, [MODEL_NAME])
async def test_output_messages_enabled(client: OpenAI, model_name: str, serv

...</description>
  </item>
  <item>
    <title>[Frontend] Add streaming tool-call support to Responses API (non-Harmony)</title>
    <link>https://github.com/vllm-project/vllm/pull/29726</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29726</guid>
    <pubDate>Sat, 29 Nov 2025 12:18:12 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Fix for #29725, 

### Summary
This pull request fixes an issue where non harmony models using the Responses API with streaming and tools emit only ResponseTextDeltaEvent events, instead of ResponseFunctionCallArgumentsDeltaEvent when a tool call is selected. This prevents clients from reliably detecting and parsing tool call arguments from the stream.

### Fix
This change updates the streaming path for non harmony models so that:

When the model selects a tool call, the arguments are surfaced as ResponseFunctionCallArgumentsDeltaEvent instead of plain text deltas. The event structure is now consistent with harmony models and with the non streaming Responses API behavior. With this, clients can treat harmony and non harmony models uniformly when han

...</description>
  </item>
  <item>
    <title>Flashrl</title>
    <link>https://github.com/vllm-project/vllm/pull/29586</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29586</guid>
    <pubDate>Thu, 27 Nov 2025 05:56:16 +0000</pubDate>
    <description>## Updating vLLM version to 0.11 with flash-rl updates and patches</description>
  </item>
  <item>
    <title>fix: Add validation for tool requests that the tool is available</title>
    <link>https://github.com/vllm-project/vllm/pull/29498</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29498</guid>
    <pubDate>Wed, 26 Nov 2025 14:26:48 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

  - Add request-level validation so Harmony tool calls fail fast when the requested built-in tool is not available on the configured tool server (browser/
    code_interpreter/container). This prevents the silent “best-effort” behavior noted in the PR discussion and surfaces an invalid_request_error instead.

## Test Plan

  - .venv/bin/pytest tests/entrypoints/openai/test_serving_responses.py -q

## Test Result

  - 6 passed, 3 warnings (module-level tests including new coverage)

## Fix PR

https://github.com/vllm-project/vllm/issues/29432

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [x] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The tes

...</description>
  </item>
  <item>
    <title>Support compressed-tensors W4A8 MoE checkpoints in GptOssModel weight loader for CPU</title>
    <link>https://github.com/vllm-project/vllm/pull/29315</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29315</guid>
    <pubDate>Mon, 24 Nov 2025 14:43:11 +0000</pubDate>
    <description>1. Add GptOssModel.load_per_expert_unfused_w4a8 helper to handle per-expert unfused MoE weights (gate_proj, up_proj, down_proj) in W4A8 checkpoints and map them into the fused FusedMoE layout (w13_* and w2_* parameters).
	•	Handles .weight, .weight_scale, .bias, and .input_scale suffixes.
	•	For biases, manually slices and writes into the appropriate columns of w13_bias (gate vs up) and w2_bias, supporting both 1D and 2D parameter layouts and using expert_id to pick the correct expert slice when the source tensor has an extra expert dimension.
	•	For weights/scales, delegates to a custom weight_loader when present, falling back to default_weight_loader otherwise, and surfaces whether the mapping was successfully handled.

2. Extend _load_weights_other to:
	•	Detect W4A8 (int4 weights, int8

...</description>
  </item>
  <item>
    <title>Fix gpt oss tool parser v2</title>
    <link>https://github.com/vllm-project/vllm/pull/29236</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29236</guid>
    <pubDate>Sat, 22 Nov 2025 11:27:13 +0000</pubDate>
    <description>Purpose
This PR fixes critical issues with tool call extraction in the OpenAIToolParser (Harmony/gpt-oss), specifically when using Custom Tools and MCP (Model Context Protocol).

Previously, the parser failed to correctly capture tool calls that resided in the active stream buffer (current_content) but hadn&#x27;t yet been finalized into a message object. This led to two major issues:

Tool Call Failure: valid tool calls were dropped or ignored.

Channel Leakage/Hallucination: The model appeared to hallucinate or leak tool call structures into the final (user-facing) message channel instead of processing them as structured tool calls.

Changes Implemented:

Active Buffer Parsing: Added logic to inspect parser.current_content and parser.current_channel. If the active channel is commentary (used 

...</description>
  </item>
</channel>
</rss>
