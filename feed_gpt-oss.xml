<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: gpt-oss</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'gpt-oss' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Thu, 01 Jan 2026 17:44:41 +0000</lastBuildDate>
  <item>
    <title>Remove unused `use_marlin` variable in `Mxfp4MoEMethod`</title>
    <link>https://github.com/vllm-project/vllm/pull/31549</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31549</guid>
    <pubDate>Tue, 30 Dec 2025 22:27:12 +0000</pubDate>
    <description>
## Purpose
Remove unused  `use_marlin` variable from Mxfp4MoEMethod class. All cases with Marlin backend in this class use `self.mxfp4_backend == Mxfp4Backend.MARLIN`.

## Test Plan

None

## Test Result

None
</description>
  </item>
  <item>
    <title>[LoRA] Load gpt-oss w13_lora_b interleaved</title>
    <link>https://github.com/vllm-project/vllm/pull/31255</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31255</guid>
    <pubDate>Wed, 24 Dec 2025 02:55:40 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

This PR updates gpt-oss `w13_lora_b` weight loading to interleaved.
* `swigluoai_and_mul` kernel expects interleaved gate and up. So need to load `w13_lora_b` weight interleaved.

## Test Plan

```
pytest -s -v tests/lora/test_gptoss_tp.py
```
## Test Result

Tests passed.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optiona

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31250</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31250</guid>
    <pubDate>Wed, 24 Dec 2025 00:18:46 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;
This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and all

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31247</guid>
    <pubDate>Tue, 23 Dec 2025 23:27:39 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;
This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and all

...</description>
  </item>
  <item>
    <title>Cleanup basic and entrypoint test organisation</title>
    <link>https://github.com/vllm-project/vllm/pull/31228</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31228</guid>
    <pubDate>Tue, 23 Dec 2025 17:33:22 +0000</pubDate>
    <description>Supersedes https://github.com/vllm-project/vllm/pull/27747:

- Don&#x27;t call basic test files individually
- Move the unit tests into a `unit` directory so we don&#x27;t have to manually ignore everything else
- Move `openai/tool_parsers` into the `unit` tests because that&#x27;s what they are</description>
  </item>
  <item>
    <title>[BugFix] LoRA: Support loading base_layer of experts</title>
    <link>https://github.com/vllm-project/vllm/pull/31104</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31104</guid>
    <pubDate>Mon, 22 Dec 2025 00:44:31 +0000</pubDate>
    <description>## Purpose

This PR fixes weight loading when LoRA is enabled, i.e., we have `base_layer` added to the:

`model.layers.0.mlp.experts.0.up_proj.weight` -&gt; `model.layers.0.mlp.experts.0.up_proj.base_layer.weight`

Currently before this fix, the patched code will handled this as:
`model.layers.0.mlp.experts.w13_base_layer.weight`, which is wrong and
it should actually be `model.layers.0.mlp.experts.base_layer.w13_weight`

## Test Plan

Test on Qwen3 30B A3B

## Test Result

Looks good.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [X] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [X] The test plan, such as providing test command.
- [X] The test results, such as pasting the results comparison 

...</description>
  </item>
  <item>
    <title>[Feature] add StreamableResponsesParser and token usage counting for ParsableContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31094</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31094</guid>
    <pubDate>Sun, 21 Dec 2025 11:11:46 +0000</pubDate>
    <description># Purpose

#30759 

- Count the token usage for ParsableContext properly, including `reasoning_tokens` and `output_tokens` and `tool_output_tokens`.
- A StreamableResposesParser is created to parse output token one by one, which can also be used to create an StreamParsableContext

# Design

## Overview

This PR introduces a new `StreamableResponsesParser` that processes output tokens one by one, enabling accurate token counting for reasoning tokens in `ParsableContext`.

## Key Changes

### 1. StreamableResponsesParser (`vllm/entrypoints/openai/parser/responses_parser.py`)

A new streaming parser that processes tokens incrementally:

- **`current_channel`** property: Identifies the current token type
  - `&#x27;analysis&#x27;`: reasoning token
  - `&#x27;commentary&#x27;`: tool call token
  - `&#x27;final&#x27;`: regul

...</description>
  </item>
  <item>
    <title>Add positional embedding and kv_cache fusion for llama and gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30978</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30978</guid>
    <pubDate>Thu, 18 Dec 2025 17:57:04 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>Use aiter triton fused_add_rmsnorm_pad for gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30976</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30976</guid>
    <pubDate>Thu, 18 Dec 2025 17:39:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Adds fused padding op before router GEMM on ROCm, eliminating this unfused pad after the GEMM before the fused_moe: https://github.com/ROCm/vllm/blob/main/vllm/model_executor/layers/fused_moe/layer.py#1603

Before:
&lt;img width=&quot;1370&quot; height=&quot;30&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/11db1bf1-e8b0-49c5-b180-3d1c730f07ca&quot; /&gt;
After:
&lt;img width=&quot;1462&quot; height=&quot;34&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/e5979f88-03c0-4173-adbd-8227786e929f&quot; /&gt;

Follow-up is to replace this with a single `F.pad` and use pattern matching to fuse AITER rmsnorm and pad instead of manually calling this kernel.

See also #30357

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Che

...</description>
  </item>
  <item>
    <title>[Refactor][TPU] Remove torch_xla path and use tpu-inference</title>
    <link>https://github.com/vllm-project/vllm/pull/30808</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30808</guid>
    <pubDate>Tue, 16 Dec 2025 18:32:26 +0000</pubDate>
    <description>## Purpose
Removes torch_xla related code paths as this backend is now deprecated. To run vLLM on TPU, users should now install and use tpu-inference.

## Test Plan
- Triggered tpu-inference CI/CD pipeline.
- Verified that removal does not impact non-TPU backends.

## Test Result
- tpu-inference CI/CD: https://buildkite.com/tpu-commons/tpu-inference-ci/builds/71

&lt;details&gt;
&lt;summary&gt; What has been removed &lt;/summary&gt;

- Classes previously migrated to tpu-inference, this is guarded by USE_TPU_INFERENCE before.
- Functions explicitly used torch_xla.
- All TPU-related test files (migrated to tpu-inference repo)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Pending Removal &lt;/summary&gt;

- Update README.md
- Update Dockerfile.tpu
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Perf] Eliminate padding and slicing op for GPT-OSS with Flashinfer MXFP4 MXFP8 MoE</title>
    <link>https://github.com/vllm-project/vllm/pull/30647</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30647</guid>
    <pubDate>Sun, 14 Dec 2025 13:40:05 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

- Depends on Flashinfer update #30993
- Eliminated padding op before the MoE: by setting the alignment in flashinfer mxfp8 quant, the output quantized tensor will be padded.
- Eliminated slicing op after the MoE: by passing the output tensor with unpadded hidden size to MoE kernel, this depends on a Flashinfer PR:
  - https://github.com/flashinfer-ai/flashinfer/pull/2217
  - This will also resolve the previous AR+Norm fusion broken by slice op issue: #28841
- Cleaned up the padding logic: for mxfp4 quant, the padded hidden size is calculated in `create_weights()`, the `maybe_roundup_hidden_size()` in `vllm/model_executor/layers/fused_moe/layer.py` seems like a dup.


## Test Plan &amp;&amp; Test Result(GPT-OSS-120b TP8)

### Accuracy

PR:
```
[{&#x27;eval_name

...</description>
  </item>
  <item>
    <title>[Bugfix] Fix NaN issue for Triton FusedMoE LoRA</title>
    <link>https://github.com/vllm-project/vllm/pull/30585</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30585</guid>
    <pubDate>Sat, 13 Dec 2025 01:35:10 +0000</pubDate>
    <description>## Purpose

This PR is to fix NaN issue with fused_moe_lora. Currently running `test_gptoss_tp.py` will fail. We found it&#x27;s because NaN values in tensor causing with `triton_kernels` having wrong `gather_indx`.
* attention `output` tensor is created as `output = torch.empty(output_shape, dtype=output_dtype, device=query.device)` in [here](https://github.com/vllm-project/vllm/blob/v0.12.0/vllm/attention/layer.py#L346), which was changed from `output = torch.zeros(output_shape, dtype=output_dtype, device=query.device)` by #26680. `torch.empty()` allocates uninitialized memory and may contain NaNs. Then in `torch.ops.vllm.unified_attention_with_output` it writes `output[:num_actual_tokens]` [here](https://github.com/vllm-project/vllm/blob/v0.12.0/vllm/v1/attention/backends/flash_attn.py#L678)

...</description>
  </item>
  <item>
    <title>[GPT OSS] Fix tool_choice required</title>
    <link>https://github.com/vllm-project/vllm/pull/30557</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30557</guid>
    <pubDate>Fri, 12 Dec 2025 14:30:24 +0000</pubDate>
    <description></description>
  </item>
  <item>
    <title>[Frontend] Honor chat template for gpt-oss harmony (#23015)</title>
    <link>https://github.com/vllm-project/vllm/pull/30482</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30482</guid>
    <pubDate>Thu, 11 Dec 2025 10:23:37 +0000</pubDate>
    <description>## Issue
- For gpt-oss models, --chat-template (and tokenizer/chat_template_kwargs) are ignored; prompts are rendered via Harmony instead of the normal chat-template pipeline.
## Reason
- When hf_config.model_type == &quot;gpt_oss&quot;, vLLM sets use_harmony and routes requests through _make_request_with_harmony(...), which directly constructs Harmony system/developer/user messages and calls render_for_completion. That path never calls _preprocess_chat / apply_hf_chat_template, so any chat template settings are bypassed.
## Summary
- allow server-level chat_template to be applied even in Harmony (gpt-oss) paths for chat and responses
- pass tokenizer into Harmony preprocessing and render via apply_hf_chat_template when provided, with safe fallback to Harmony default on errors
- tighten typing/forma

...</description>
  </item>
  <item>
    <title>[Optimization]: Add fused router for GPTOSS</title>
    <link>https://github.com/vllm-project/vllm/pull/30471</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30471</guid>
    <pubDate>Thu, 11 Dec 2025 07:55:47 +0000</pubDate>
    <description>&lt;details&gt;
&lt;summary&gt; output collect_env &lt;/summary&gt;

```
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 24.04.3 LTS (x86_64)
GCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version                : Could not collect
CMake version                : version 3.28.3
Libc version                 : glibc-2.39

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version               : 3.12.11

...</description>
  </item>
  <item>
    <title>[Bugfix] missing tokens occur in harmony streaming</title>
    <link>https://github.com/vllm-project/vllm/pull/30437</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30437</guid>
    <pubDate>Wed, 10 Dec 2025 23:36:17 +0000</pubDate>
    <description>
## Purpose
Fixed an issue where in harmony streaming mode, when the engine yields more than one token at a time, only the last token is used. #28635 #30099
## Test Plan
```text
uv run api_server.py --model openai/gpt-oss-120b --gpu-memory-utilization 0.95 --port 8000 --served-model-name gptoss120b --disable-log-request --tool-call-parser openai --enable-auto-tool-choice
```
```python
from openai import AsyncOpenAI
import asyncio
import json

client = AsyncOpenAI(base_url=&#x27;http://127.0.0.1:8000/v1&#x27;, api_key=&#x27;empty&#x27;)

async def run(semaphore, i):
    async with semaphore:
        for count in range(100):
            print(f&#x27;{i}: {count}&#x27;)
            a = []
            stream = await client.responses.create(model=&#x27;gptoss120b&#x27;, input=&#x27;say something long.&#x27;, stream=True)
            async for 

...</description>
  </item>
  <item>
    <title>Upstream fp8 with static scales gpt oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30357</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30357</guid>
    <pubDate>Tue, 09 Dec 2025 19:49:32 +0000</pubDate>
    <description>##gpt-oss120b-w-mxfp4-a-fp8
------------------------------------------------------
server:
&gt; HIP_VISIBLE_DEVICES=1 VLLM_USE_AITER_UNIFIED_ATTENTION=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_ROCM_USE_AITER_FUSED_MOE_A16W4=0 vllm serve /data/models/gpt-oss120b-w-mxfp4-a-fp8     --port 8000     --swap-space 64     --max-model-len 10368     --tensor-parallel-size 1     --max-num-seqs 1024     --gpu-memory-utilization 0.95     --no-enable-prefix-caching     --enforce-eager

client:
&gt; curl http://localhost:8000/v1/completions -H &quot;Content-Type: application/json&quot; -d &#x27;{
    &quot;model&quot;: &quot;/data/models/gpt-oss120b-w-mxfp4-a-fp8&quot;,
    &quot;prompt&quot;: &quot;San Francisco is a&quot;,
    &quot;max_tokens&quot;: 16,
    &quot;temperature&quot;: 0
}&#x27;

output:
&gt; {&quot;id&quot;:&quot;cmpl-9e80e71642c56933&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1765949222,&quot;model&quot;:&quot;/data/

...</description>
  </item>
  <item>
    <title>[ResponsesAPI] Add GPTOSS MCP tool streaming</title>
    <link>https://github.com/vllm-project/vllm/pull/30301</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30301</guid>
    <pubDate>Tue, 09 Dec 2025 04:36:25 +0000</pubDate>
    <description>## Purpose
This change enables streaming support for MCP tools when using GPT OSS. It extends the harmony utilities and response serving infrastructure to handle tool streaming, allowing tool calls and their results to be incrementally streamed back to clients rather than returned as a single batch.

taken over from #30192, builds on top of https://github.com/vllm-project/vllm/pull/30054

## Test Plan

```
VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS=web_search_preview,container,code_interpreter CUDA_VISIBLE_DEVICES=2,3 with-proxy vllm serve &quot;openai/gpt-oss-120b&quot; -tp 2     --trust-remote-code  --tool-server=localhost:8081/container,localhost:8081/browser,localhost:8081/python
```

```
curl -X POST &quot;http://localhost:8000/v1/responses&quot;   -H &quot;Content-Type: application/json&quot;   -H &quot;Authorization: Bearer

...</description>
  </item>
  <item>
    <title>[gpt-oss] Add model_identity to system message retrieval for harmony chat template</title>
    <link>https://github.com/vllm-project/vllm/pull/30247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30247</guid>
    <pubDate>Mon, 08 Dec 2025 08:43:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
When using GPT-OSS, `model_identity` passed in from `chat_template_kwargs` is not being used to adjust model&#x27;s identity. This PR is to pass the `model_identity` from chat/completion to the system message formatter.
## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If you

...</description>
  </item>
  <item>
    <title>Feat/support nemotron h mtp wip</title>
    <link>https://github.com/vllm-project/vllm/pull/30208</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30208</guid>
    <pubDate>Sun, 07 Dec 2025 13:46:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[DO NOT MERGE] Introduce Renderer for processing chat messages (using `ModelConfig`)</title>
    <link>https://github.com/vllm-project/vllm/pull/30200</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30200</guid>
    <pubDate>Sun, 07 Dec 2025 08:30:34 +0000</pubDate>
    <description>Ready for review.

## Purpose

- Prototype an interface, `vllm.renderers.RendererLike`, to process chat messages into engine inputs.
- Introduce `RENDERER_REGISTRY` which lazily registers renderers to avoid circular import problem.
- Move implementation-specific chat utils to the corresponding renderer in `vllm.renderers`.
- Initialize the renderer in `InputPreprocessor`, replacing the tokenizer initialization inside `LLMEngine` and `AsyncLLM`.
- Replace `EngineClient.get_tokenizer()` with `EngineClient.renderer.get_tokenizer()` to avoid unnecessary async.
- Update tests accordingly, and move some tests into a new directory `tests/renderers` that is run under `Async Engine, Inputs, Utils, Worker, Config Test (CPU)`.

Towards #22880 and #23873

Future work:

- Merge `CompletionRenderer` wit

...</description>
  </item>
  <item>
    <title>[Frontend] Add MCP tool streaming support to Responses API</title>
    <link>https://github.com/vllm-project/vllm/pull/30192</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30192</guid>
    <pubDate>Sun, 07 Dec 2025 04:46:48 +0000</pubDate>
    <description>Purpose
This change enables streaming support for MCP tools when using GPT OSS. It extends the harmony utilities and response serving infrastructure to handle tool streaming, allowing tool calls and their results to be incrementally streamed back to clients rather than returned as a single batch.

Test Plan
curl -X POST &quot;http://localhost:8000/v1/responses&quot;   -H &quot;Content-Type: application/json&quot;   -H &quot;Authorization: Bearer dummy-api-key&quot;   -d &#x27;{
    &quot;model&quot;: &quot;default&quot;,
    &quot;input&quot;: &quot;Multiply 123*456 using the mcp.code_interpreter tool.&quot;,
    &quot;tools&quot;: [{
      &quot;type&quot;: &quot;mcp&quot;,
      &quot;server_label&quot;: &quot;code_interpreter&quot;,
      &quot;headers&quot;: {&quot;test&quot;: &quot;test&quot;},
      &quot;server_url&quot;: &quot;IGNORED&quot;
    }],
    &quot;stream&quot;: true,
    &quot;enable_response_messages&quot;: true
  }&#x27;
Test Result
event: response.created
data: {&quot;

...</description>
  </item>
  <item>
    <title>[Perf] Add benchmark script for triton unified attention kernel performance</title>
    <link>https://github.com/vllm-project/vllm/pull/30191</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30191</guid>
    <pubDate>Sun, 07 Dec 2025 03:13:29 +0000</pubDate>
    <description>
## Purpose
Add a script for benchmarking triton unified attention kernel performance and compare against flash attention. This could be useful for testing new hardware performance etc 

## Test Plan
```

(vllm) ubuntu@209-20-157-255:~/vllm$ python benchmarks/kernels/benchmark_triton_unified_attention.py
flash_attn not available. Only Triton Unified Attention will be benchmarked.
================================================================================
Triton Unified Attention Benchmark
================================================================================
Configuration:
  dtype: torch.bfloat16
  num_query_heads: 32
  num_kv_heads: 8
  head_size: 128
  block_size: 16
  use_alibi: False
  sliding_window: -1
  softcap: 0.0
  FlashAttention available: False
==================

...</description>
  </item>
  <item>
    <title>[Feature] Add track_token_ids for Efficient Selective Token Logprobs Tracking</title>
    <link>https://github.com/vllm-project/vllm/pull/30030</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30030</guid>
    <pubDate>Thu, 04 Dec 2025 07:04:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
This PR adds a new `track_token_ids` parameter to `SamplingParams` that enables efficient tracking of log probabilities (logprobs) for specific tokens without requiring full vocabulary logprobs retrieval. This feature addresses [Issue#29280](https://github.com/vllm-project/vllm/issues/29280)

## Implementation Overview

The feature is implemented across the vLLM V1 engine pipeline, from parameter validation to output processing.

### 1. Parameter Extension (`vllm/sampling_params.py`)

Added `track_token_ids: list[int] | None` parameter with validation ensuring all IDs are non-negative integers:

```python
track_token_ids: list[int] | None = None
&quot;&quot;&quot;List of token IDs to track logprobs for at every generation step.&quot;&quot;&quot;
```

### 2. New Data Structures 

...</description>
  </item>
  <item>
    <title>Added regression test for openai/harmony/issues/78</title>
    <link>https://github.com/vllm-project/vllm/pull/29830</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29830</guid>
    <pubDate>Tue, 02 Dec 2025 01:06:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Add regression tests for [openai/harmony#78](https://github.com/openai/harmony/issues/78), particularly the FastAPI `vllm serve` path which wasn&#x27;t fully addressed in #26185.

## Test Plan

Modify the existing `test_output_messages_enabled` test in test_response_api_with_harmony.py to add the following validations:

```python
    for _message in [*response.input_messages, *response.output_messages]:
        for _item in _message.get(&quot;content&quot;):
            assert isinstance(_item, dict), _message
            assert len(_item) &gt; 0, _message
```

&lt;details&gt;&lt;summary&gt;full code details&lt;/summary&gt;

```python
@pytest.mark.asyncio
@pytest.mark.parametrize(&quot;model_name&quot;, [MODEL_NAME])
async def test_output_messages_enabled(client: OpenAI, model_name: str, serv

...</description>
  </item>
  <item>
    <title>[Frontend] Add streaming tool-call support to Responses API (non-Harmony)</title>
    <link>https://github.com/vllm-project/vllm/pull/29726</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29726</guid>
    <pubDate>Sat, 29 Nov 2025 12:18:12 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Fix for #29725, 

### Summary
This pull request fixes an issue where non harmony models using the Responses API with streaming and tools emit only ResponseTextDeltaEvent events, instead of ResponseFunctionCallArgumentsDeltaEvent when a tool call is selected. This prevents clients from reliably detecting and parsing tool call arguments from the stream.

### Fix
This change updates the streaming path for non harmony models so that:

When the model selects a tool call, the arguments are surfaced as ResponseFunctionCallArgumentsDeltaEvent instead of plain text deltas. The event structure is now consistent with harmony models and with the non streaming Responses API behavior. With this, clients can treat harmony and non harmony models uniformly when han

...</description>
  </item>
  <item>
    <title>Flashrl</title>
    <link>https://github.com/vllm-project/vllm/pull/29586</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29586</guid>
    <pubDate>Thu, 27 Nov 2025 05:56:16 +0000</pubDate>
    <description>## Updating vLLM version to 0.11 with flash-rl updates and patches</description>
  </item>
  <item>
    <title>fix: Add validation for tool requests that the tool is available</title>
    <link>https://github.com/vllm-project/vllm/pull/29498</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29498</guid>
    <pubDate>Wed, 26 Nov 2025 14:26:48 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

  - Add request-level validation so Harmony tool calls fail fast when the requested built-in tool is not available on the configured tool server (browser/
    code_interpreter/container). This prevents the silent “best-effort” behavior noted in the PR discussion and surfaces an invalid_request_error instead.

## Test Plan

  - .venv/bin/pytest tests/entrypoints/openai/test_serving_responses.py -q

## Test Result

  - 6 passed, 3 warnings (module-level tests including new coverage)

## Fix PR

https://github.com/vllm-project/vllm/issues/29432

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [x] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The tes

...</description>
  </item>
  <item>
    <title>Support compressed-tensors W4A8 MoE checkpoints in GptOssModel weight loader for CPU</title>
    <link>https://github.com/vllm-project/vllm/pull/29315</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29315</guid>
    <pubDate>Mon, 24 Nov 2025 14:43:11 +0000</pubDate>
    <description>1. Add GptOssModel.load_per_expert_unfused_w4a8 helper to handle per-expert unfused MoE weights (gate_proj, up_proj, down_proj) in W4A8 checkpoints and map them into the fused FusedMoE layout (w13_* and w2_* parameters).
	•	Handles .weight, .weight_scale, .bias, and .input_scale suffixes.
	•	For biases, manually slices and writes into the appropriate columns of w13_bias (gate vs up) and w2_bias, supporting both 1D and 2D parameter layouts and using expert_id to pick the correct expert slice when the source tensor has an extra expert dimension.
	•	For weights/scales, delegates to a custom weight_loader when present, falling back to default_weight_loader otherwise, and surfaces whether the mapping was successfully handled.

2. Extend _load_weights_other to:
	•	Detect W4A8 (int4 weights, int8

...</description>
  </item>
  <item>
    <title>Fix gpt oss tool parser v2</title>
    <link>https://github.com/vllm-project/vllm/pull/29236</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29236</guid>
    <pubDate>Sat, 22 Nov 2025 11:27:13 +0000</pubDate>
    <description>Purpose
This PR fixes critical issues with tool call extraction in the OpenAIToolParser (Harmony/gpt-oss), specifically when using Custom Tools and MCP (Model Context Protocol).

Previously, the parser failed to correctly capture tool calls that resided in the active stream buffer (current_content) but hadn&#x27;t yet been finalized into a message object. This led to two major issues:

Tool Call Failure: valid tool calls were dropped or ignored.

Channel Leakage/Hallucination: The model appeared to hallucinate or leak tool call structures into the final (user-facing) message channel instead of processing them as structured tool calls.

Changes Implemented:

Active Buffer Parsing: Added logic to inspect parser.current_content and parser.current_channel. If the active channel is commentary (used 

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations </title>
    <link>https://github.com/vllm-project/vllm/pull/29008</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29008</guid>
    <pubDate>Wed, 19 Nov 2025 11:21:15 +0000</pubDate>
    <description>## Purpose

This PR aims for:
- [x] quark model loading, combined with original `mxfp4` loading [function ](https://github.com/vllm-project/vllm/blob/256a33ecb4923a4bfa6e1a9adbc69f5255a5aa6d/vllm/model_executor/models/gpt_oss.py#L309)
- [x] OCPMX_W4A16, OCPMX_W4AFP8 MoE scheme and emulation forward, unified into class `QuarkOCP_MX_MoEMethod`

## Test Plan

- Models:
  - [x] GPT_OSS_20B
  - [x] GPT_OSS_120B
- Quantization schemes:
  - [x] W: MXFP4, A: BF16
  - [x] W: MXFP4, A: FP8
  - [x] W: MXFP4, A: MXFP4
  - [x] W: MXFP4, A: MXFP4, KV: FP8
- TP:
  - [x] TP1
  - [x] TP2
  - [x] TP4
  - [x] TP8

See results below.

## Test Result

&lt;img width=&quot;1293&quot; height=&quot;265&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/a531f52b-3c56-4854-a6b1-04ac337f3649&quot; /&gt;


## TODO

- [x] unify `mxfp4

...</description>
  </item>
  <item>
    <title>[Feature] Generic Model Support via TrainableAttention and ModelRegistry parallelism constructor callback</title>
    <link>https://github.com/vllm-project/vllm/pull/28685</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28685</guid>
    <pubDate>Thu, 13 Nov 2025 21:25:45 +0000</pubDate>
    <description>## Purpose

Implement RFC #28326 to enable users to easily integrate custom models with vLLM for both training and inference, with support for external parallelism libraries (e.g., Megatron-LM, FSDP, DeepSpeed).

Enables use cases like:
  - RL training with vLLM acceleration
  - Integrating third-party models without rewriting parallelism logic
  - Mixing external tensor parallel layers (Megatron) with vLLM attention

## Test Plan

```
# example, should be a test
python examples/offline_inference/custom_model_with_megatron.py
```
  - Full transformer using Megatron-LM&#x27;s ColumnParallelLinear + RowParallelLinear for MLPs
  - TrainableFlashAttention for attention (with KV cache + backward pass)
  - Ground truth validation against independent PyTorch run
  - Tests TP=4, dynamic batching, train

...</description>
  </item>
  <item>
    <title>[Bug]: rocm crash AMD Ryzen AI 9 HX PRO 370 w/ Radeon 890M - docker/podman</title>
    <link>https://github.com/vllm-project/vllm/issues/28460</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28460</guid>
    <pubDate>Tue, 11 Nov 2025 10:16:32 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0
Clang version                : 20.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-7.0.0 25314 f4087f6b428f0e6f575ebac8a8a724dab123d06e)
CMake version                : version 3.31.6
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0a0+git1c57644
Is debug build               : False
CUDA used to build PyTorc

...</description>
  </item>
  <item>
    <title>[Core] Parse vLLM engine required fields from hf_config to model_arch_config</title>
    <link>https://github.com/vllm-project/vllm/pull/28454</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28454</guid>
    <pubDate>Tue, 11 Nov 2025 08:25:52 +0000</pubDate>
    <description>## Purpose
See https://github.com/vllm-project/vllm/issues/24384 for more context.

Use llama3 as prototype 

## Design
- `model_arch_config` explicitly specify all standardized fields required for vllm runtime
- model arch parser read from config.json/params.json/etc. and perform the standardization process. The goal is to eventually remove most of the standardization logic from config/model.py once the migration to the new parser workflow is complete
- For hf-model-arch-parser, if the model is not in `_CONFIG_REGISTRY`, we still call `AutoConfig.from_pretrained`. This allows us to leverage the normalization already implemented in HuggingFace’s `PretrainedConfig`. A more standardized PretrainedConfig will enable a thinner, simpler parser layer.
https://github.com/vllm-project/vllm/blob/f0

...</description>
  </item>
  <item>
    <title>Fix/responses api harmony channel metadata    #28262</title>
    <link>https://github.com/vllm-project/vllm/pull/28355</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28355</guid>
    <pubDate>Sun, 09 Nov 2025 00:48:27 +0000</pubDate>
    <description> 
 
## Purpose
Fix issue #28262: Restore missing channel metadata when converting Responses API output items back to Harmony Messages for multi-turn conversations.
Changes:
Set channel=&#x27;commentary&#x27; for function_call_output type inputs
Set channel=&#x27;analysis&#x27; or &#x27;commentary&#x27; for reasoning type based on the following message (commentary if followed by function_call, analysis otherwise)
Add test to verify channel metadata is correctly preserved across conversation turns
Update parse_response_input() to accept optional next_msg parameter for context-aware channel assignmen
## Test Plan
pytest tests/entrypoints/openai/test_response_api_with_harmony.py::test_function_call_with_previous_input_messages -v
## Test Result
pass 
---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description

...</description>
  </item>
  <item>
    <title>[Bugfix] Parse gpt-oss refusals w/ newer openai-harmony</title>
    <link>https://github.com/vllm-project/vllm/pull/28303</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28303</guid>
    <pubDate>Fri, 07 Nov 2025 15:35:05 +0000</pubDate>
    <description>## Purpose

The output generated by gpt-oss models does not always strictly follow its expected harmony chat template format. This commonly - but not exclusively - happens when gpt-oss-120b generates refusals for content that violates its built-in safety guidelines.

To fix this, a non-strict mode was added to the openai-harmony library to allow attempted recovery of malformed message headers in the model output, such as a missing `&lt;|message|&gt;` special token before the assistant text.

This will resolve some cases where the error
`openai_harmony.HarmonyError: unexpected tokens remaining in message header` was previously thrown. It will not resolve all of those, as not every malformed message output can be recovered. Other ongoing work around using structured output for the Harmony format c

...</description>
  </item>
  <item>
    <title>[Frontend] [gpt-oss] Chat format GD for tool calling with gptoss</title>
    <link>https://github.com/vllm-project/vllm/pull/28148</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28148</guid>
    <pubDate>Wed, 05 Nov 2025 18:17:40 +0000</pubDate>
    <description>## Purpose
Building on top of the initial gpt-oss reasoning parser in #25515, this PR fleshes out the full structural tags schema to guide the chat format for gpt-oss.

It is only added to the Responses API path for now, but it should be able to fix issues like the ones mentioned in #24954 without needing to make adjustments to Harmony.

Guided decoding for the gpt-oss chat format should only be enabled if `structured_outputs_config.enable_in_reasoning` is True, since the chat format is technically in the reasoning section of model output. It also lets us continue to use structured outputs for the content of the final message. `structured_outputs_config.reasoning_parser` is set by default for gpt-oss. 

TODO: test that structured output works alongside this after #28000 is merged, and adju

...</description>
  </item>
  <item>
    <title>[Quantization] quantized kv cache loading for models like qwen</title>
    <link>https://github.com/vllm-project/vllm/pull/27980</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27980</guid>
    <pubDate>Mon, 03 Nov 2025 12:03:40 +0000</pubDate>
    <description>## Purpose
This PR aims to fix quantized kv cache model loading issues using AMD-Quark. 
The fix was initially applied to the `gpt-oss` model and may be extended to other models in `vllm/model_executor/models`, as they share a nearly identical methodology.

Here is the original error traceback w/o this PR fixing.
&lt;img width=&quot;1219&quot; height=&quot;468&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/1ce1b92e-04a9-404a-91da-f06151cd54c9&quot; /&gt;

##
Here is a check list of some models defined in `vllm/model_executor/models/` for the supportness of quantized kv cache loading. Models with `get_cache_scale` called is marked as ✅, while models without `get_cache_scale` called is marked as ⚠️.

The ⚠️ symbol is preferred over ❌ since some models can intrinsically have non-quantized KV cache, for i

...</description>
  </item>
  <item>
    <title>[Bugfix] Handle escaped characters in GLM tool parser to prevent double serialization</title>
    <link>https://github.com/vllm-project/vllm/pull/27970</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27970</guid>
    <pubDate>Mon, 03 Nov 2025 08:37:00 +0000</pubDate>
    <description>## Purpose

This PR fixes a bug where the tool call parser fails when the model&#x27;s output contains literal escaped characters, such as `\n`, `\&quot;`.

**The Problem:**
When GLM-4 outputs tool calls, the XML may contain &#x27;\n&#x27; literal characters and the JSON content within `&lt;arg_value&gt;` tags may contain escaped characters like `\&quot;`, causing regex matching and json parsing both failed. For example:

```
&lt;tool_call&gt;todo_write\n&lt;arg_key&gt;todos&lt;/arg_key&gt;\n&lt;arg_value&gt;[{\&quot;id\&quot;: \&quot;1\&quot;, \&quot;task\&quot;: \&quot;检查后端代码中的硬编码问题\&quot;, \&quot;status\&quot;: \&quot;in_progress\&quot;}, {\&quot;id\&quot;: \&quot;2\&quot;, \&quot;task\&quot;: \&quot;检查前端代码中的硬编码问题\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;3\&quot;, \&quot;task\&quot;: \&quot;检查违反单一职责的代码\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;4\&quot;, \&quot;task\&quot;: \&quot;创建整改建议报告\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}]&lt;/arg_value&gt;
&lt;/tool_call&gt;
```

The current regex fails to ma

...</description>
  </item>
  <item>
    <title>[Metrics] [KVConnector] Add Offloading Connector metrics</title>
    <link>https://github.com/vllm-project/vllm/pull/27942</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27942</guid>
    <pubDate>Sun, 02 Nov 2025 15:36:12 +0000</pubDate>
    <description>Added queries and hits metrics for the Offloading Connector.
Also added timing metrics for store and load operations, which take the average time it takes to load/store, per-token.
The metrics are available from Prometheus and from the StatLogger.
## Purpose
Allows collection of timing metrics for the Offloading Connector, which is essential for future development.
@orozery please review.
## Test Plan

## Test Result
</description>
  </item>
  <item>
    <title>[WIP] [GPT-OSS] customized symm_mem based EP comm kernel integration</title>
    <link>https://github.com/vllm-project/vllm/pull/27495</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27495</guid>
    <pubDate>Sat, 25 Oct 2025 01:04:50 +0000</pubDate>
    <description>Integrates Triton MoE communication kernels

achieved 60% throughput increase compared to #[24588](https://github.com/vllm-project/vllm/pull/24588)

&lt;img width=&quot;3918&quot; height=&quot;1996&quot; alt=&quot;36117&quot; src=&quot;https://github.com/user-attachments/assets/ad93165c-6869-4a01-8c5b-836f4421f656&quot; /&gt;
</description>
  </item>
  <item>
    <title>[CI/Build] Add aime25 eval to gpt-oss CI</title>
    <link>https://github.com/vllm-project/vllm/pull/27398</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27398</guid>
    <pubDate>Thu, 23 Oct 2025 07:38:49 +0000</pubDate>
    <description>## Purpose
This PR added eval config for gpt-oss-120b on H100. 

## Test Plan
pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-gpt-oss.txt --tp-size=1

## Test Result

1 passed, 6 warnings in 455.90s (0:07:35) 

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the releas

...</description>
  </item>
  <item>
    <title>[Bug]: gptoss calls built-in tool when no tools are given</title>
    <link>https://github.com/vllm-project/vllm/issues/27385</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27385</guid>
    <pubDate>Thu, 23 Oct 2025 04:01:36 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
==============================
        System Info
==============================
OS                           : Ubuntu 20.04.6 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-2ubuntu1~20.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 3.29.2
Libc version                 : glibc-2.31

==============================
       PyTorch Info
==============================
PyTorch version              : 2.8.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version

...</description>
  </item>
  <item>
    <title>[Bugfix] Actually enable serialize_messages for harmony Responses (related to #26185)</title>
    <link>https://github.com/vllm-project/vllm/pull/27377</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27377</guid>
    <pubDate>Thu, 23 Oct 2025 01:07:10 +0000</pubDate>
    <description>## Purpose

For the OpenAI-compatible `v1/responses` route, enable raw messages to be sent when `enable_response_messages` is set to True in `extra_body`.

Previously, the responses are empty because of an issue in openai/harmony. (https://github.com/openai/harmony/issues/78)

https://github.com/vllm-project/vllm/pull/26185 implements most of the fix, but these aren&#x27;t actually invoked, at least not when serving the model through the `vllm serve`. The reason is that the said PR specifies `when_used=&quot;json&quot;`. Thus, this serialization method is ignored because of the use of `model_dump()` in [vllm/entrypoints/openai/api_server.py#L527-L529](https://github.com/vllm-project/vllm/blob/a0003b56b0b822c52bb0f3035c164370a802e6f5/vllm/entrypoints/openai/api_server.py#L527-L529).

The fix is to trigger

...</description>
  </item>
  <item>
    <title>[Frontend][gpt-oss] Allow system message to overwrite model identity</title>
    <link>https://github.com/vllm-project/vllm/pull/27310</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27310</guid>
    <pubDate>Wed, 22 Oct 2025 00:57:44 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
The current way of implementation forces the system prompt to go into the developer message and it confuses the usage of different roles. We should have developer into developer message, and allow system message to override the system prompt as supported by signature of harmony_utils.py

## Test Result
Override
            &quot;openai_harmony_message&quot;: {
                &quot;author&quot;: {
                    &quot;role&quot;: &quot;system&quot;
                },
                &quot;content&quot;: [
                    {
                        &quot;system_content&quot;: {
                            &quot;type&quot;: &quot;system_content&quot;,
                            &quot;model_identity&quot;: &quot;You are a helpful agent reponds in Chinese.&quot;,
                            &quot;reasoning_effort&quot;: &quot;high&quot;,
                       

...</description>
  </item>
  <item>
    <title>Support PP on tpu_inference</title>
    <link>https://github.com/vllm-project/vllm/pull/27296</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27296</guid>
    <pubDate>Tue, 21 Oct 2025 21:58:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>Experimental attention backend in helion</title>
    <link>https://github.com/vllm-project/vllm/pull/27293</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27293</guid>
    <pubDate>Tue, 21 Oct 2025 20:10:50 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

very experimental and draft PR so far

## Test Plan

```
VLLM_ATTENTION_BAKCEND=EXPERIMENTAL_HELION_ATTN vllm serve meta-llama/Llama-3.1-8B-Instruct 
```
## Test Result

Performance see upcoming publications. 
Correctness:
```
VLLM_ATTENTION_BAKCEND=EXPERIMENTAL_HELION_ATTN lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks gsm8k --num_fewshot 5 --batch_size auto --limit 500
```
results in 
```
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.798|±  |0.0180|
|     |       |strict-match    |     5|exact_match|↑  |0.780|±  |0.0185|
```

---
&lt;details&gt;
&lt;summary&gt; Ess

...</description>
  </item>
  <item>
    <title>Add process pool support fro tokenizer</title>
    <link>https://github.com/vllm-project/vllm/pull/27180</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27180</guid>
    <pubDate>Mon, 20 Oct 2025 03:02:35 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;


## Purpose
The preprocessing stage experiences a bottleneck under high concurrency, resulting in extremely long preprocessing times. By introducing additional processes to handle tokenization, this bottleneck can be alleviated.

Related PR: #25301


## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Rel

...</description>
  </item>
  <item>
    <title>[GPT-OSS] Persistent Masked Activation Kernel</title>
    <link>https://github.com/vllm-project/vllm/pull/27100</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27100</guid>
    <pubDate>Fri, 17 Oct 2025 13:02:02 +0000</pubDate>
    <description>
## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Bug] Pass correct ep_rank_start and ep_rank_end</title>
    <link>https://github.com/vllm-project/vllm/pull/27025</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27025</guid>
    <pubDate>Thu, 16 Oct 2025 13:30:25 +0000</pubDate>
    <description>## Purpose
- _load_weights_others expect the parameters as ep_rank_start and ep_rank_end but its passed in the reverse order
- Fix to pass the correct parameters

## Test Plan
Tested with gpt-oss and enabling the expert parallelism

## Test Result
Earlier gpt-oss was failing when expert parallelism was enabled, after the fix it started passing
</description>
  </item>
</channel>
</rss>
