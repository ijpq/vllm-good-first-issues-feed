<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: gpt-oss</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'gpt-oss' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Fri, 12 Dec 2025 05:47:36 +0000</lastBuildDate>
  <item>
    <title>[Perf] Set split_k to 1 for triton_kernels</title>
    <link>https://github.com/vllm-project/vllm/pull/30528</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30528</guid>
    <pubDate>Fri, 12 Dec 2025 01:15:38 +0000</pubDate>
    <description>## Purpose

This PR set split_k to 1 for triton_kernels:
* It&#x27;s reported that triton has worse performance than marlin for small batch size, see https://github.com/vllm-project/vllm/issues/28894. This is because the computed split_k become greater than 1 for small batch size, then triton_kernels will use torch.float32 dtype, see https://github.com/triton-lang/triton/blob/v3.5.1/python/triton_kernels/triton_kernels/matmul_ogs.py#L169
  * split_k computation: `split_k = max(n_sms // grid_size, 1)`, see https://github.com/triton-lang/triton/blob/v3.5.1/python/triton_kernels/triton_kernels/matmul_ogs_details/opt_flags_details/opt_flags_nvidia.py#L45-L54
  * split_k become smaller when grid_size become bigger (Since n_sms =132 for Hopper, if grid_size is &gt; 66, split_k start to become 1). grid_s

...</description>
  </item>
  <item>
    <title>[Frontend] Honor chat template for gpt-oss harmony (#23015)</title>
    <link>https://github.com/vllm-project/vllm/pull/30482</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30482</guid>
    <pubDate>Thu, 11 Dec 2025 10:23:37 +0000</pubDate>
    <description>## Issue
- For gpt-oss models, --chat-template (and tokenizer/chat_template_kwargs) are ignored; prompts are rendered via Harmony instead of the normal chat-template pipeline.
## Reason
- When hf_config.model_type == &quot;gpt_oss&quot;, vLLM sets use_harmony and routes requests through _make_request_with_harmony(...), which directly constructs Harmony system/developer/user messages and calls render_for_completion. That path never calls _preprocess_chat / apply_hf_chat_template, so any chat template settings are bypassed.
## Summary
- allow server-level chat_template to be applied even in Harmony (gpt-oss) paths for chat and responses
- pass tokenizer into Harmony preprocessing and render via apply_hf_chat_template when provided, with safe fallback to Harmony default on errors
- tighten typing/forma

...</description>
  </item>
  <item>
    <title>[Bugfix] missing tokens occur in harmony streaming</title>
    <link>https://github.com/vllm-project/vllm/pull/30437</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30437</guid>
    <pubDate>Wed, 10 Dec 2025 23:36:17 +0000</pubDate>
    <description>
## Purpose
Fixed an issue where in harmony streaming mode, when the engine yields more than one token at a time, only the last token is used. #28635 #30099
## Test Plan
```text
uv run api_server.py --model openai/gpt-oss-120b --gpu-memory-utilization 0.95 --port 8000 --served-model-name gptoss120b --disable-log-request --tool-call-parser openai --enable-auto-tool-choice
```
```python
from openai import AsyncOpenAI
import asyncio
import json

client = AsyncOpenAI(base_url=&#x27;http://127.0.0.1:8000/v1&#x27;, api_key=&#x27;empty&#x27;)

async def run(semaphore, i):
    async with semaphore:
        for count in range(100):
            print(f&#x27;{i}: {count}&#x27;)
            a = []
            stream = await client.responses.create(model=&#x27;gptoss120b&#x27;, input=&#x27;say something long.&#x27;, stream=True)
            async for 

...</description>
  </item>
  <item>
    <title>Upstream fp8 with static scales gpt oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30357</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30357</guid>
    <pubDate>Tue, 09 Dec 2025 19:49:32 +0000</pubDate>
    <description></description>
  </item>
  <item>
    <title>[ResponsesAPI] Add GPTOSS MCP tool streaming</title>
    <link>https://github.com/vllm-project/vllm/pull/30301</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30301</guid>
    <pubDate>Tue, 09 Dec 2025 04:36:25 +0000</pubDate>
    <description>## Purpose
This change enables streaming support for MCP tools when using GPT OSS. It extends the harmony utilities and response serving infrastructure to handle tool streaming, allowing tool calls and their results to be incrementally streamed back to clients rather than returned as a single batch.

taken over from #30192, builds on top of https://github.com/vllm-project/vllm/pull/30054

## Test Plan

```
VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS=web_search_preview,container,code_interpreter CUDA_VISIBLE_DEVICES=2,3 with-proxy vllm serve &quot;openai/gpt-oss-120b&quot; -tp 2     --trust-remote-code  --tool-server=localhost:8081/container,localhost:8081/browser,localhost:8081/python
```

```
curl -X POST &quot;http://localhost:8000/v1/responses&quot;   -H &quot;Content-Type: application/json&quot;   -H &quot;Authorization: Bearer

...</description>
  </item>
  <item>
    <title>[gpt-oss] Add model_identity to system message retrieval for harmony chat template</title>
    <link>https://github.com/vllm-project/vllm/pull/30247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30247</guid>
    <pubDate>Mon, 08 Dec 2025 08:43:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
When using GPT-OSS, `model_identity` passed in from `chat_template_kwargs` is not being used to adjust model&#x27;s identity. This PR is to pass the `model_identity` from chat/completion to the system message formatter.
## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If you

...</description>
  </item>
  <item>
    <title>[Bugfix] fix streaming final output for non harmony</title>
    <link>https://github.com/vllm-project/vllm/pull/30237</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30237</guid>
    <pubDate>Mon, 08 Dec 2025 05:58:33 +0000</pubDate>
    <description># Summary

  Fixed a bug where the output field in response.completed event was empty when using streaming mode with non-Harmony
  models (e.g., Qwen), even though the streaming delta events contained correct content.

# Problem

  When using the Responses API with stream=True, the streaming events (response.output_text.delta) correctly returned
  text deltas, but the final response.completed event had an empty output array:
```json
  {
    &quot;type&quot;: &quot;response.completed&quot;,
    &quot;response&quot;: {
      &quot;output&quot;: [],  // Should contain the message with full text
      &quot;status&quot;: &quot;completed&quot;
    }
  }
```

# Root Cause

  In _process_simple_streaming_events, each iteration overwrites context.last_output with only the latest incremental
  output (delta). When responses_full_generator builds the final r

...</description>
  </item>
  <item>
    <title>Feat/support nemotron h mtp wip</title>
    <link>https://github.com/vllm-project/vllm/pull/30208</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30208</guid>
    <pubDate>Sun, 07 Dec 2025 13:46:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[gpt-oss] Fix harmony parser in streaming responses</title>
    <link>https://github.com/vllm-project/vllm/pull/30205</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30205</guid>
    <pubDate>Sun, 07 Dec 2025 12:14:24 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Fix missed final streaming tokens in speculative decoding (https://github.com/vllm-project/vllm/issues/30204)
## Test Plan
Rerun the reproducer after fix is implemented.
## Test Result
Final tokens are indeed returned as expected.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [*] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [*] The test plan, such as providing test command.
- [*] The test results, such as pasting the results comparison before and after, or e2e results
- [*] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [*] (Optional) Release notes update. If your change is u

...</description>
  </item>
  <item>
    <title>[DO NOT MERGE] Introduce Renderer for processing chat messages (using `ModelConfig`)</title>
    <link>https://github.com/vllm-project/vllm/pull/30200</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30200</guid>
    <pubDate>Sun, 07 Dec 2025 08:30:34 +0000</pubDate>
    <description>## Purpose

- Prototype an interface, `vllm.renderers.RendererLike`, to process chat messages into engine inputs.
- Introduce `RENDERER_REGISTRY` which lazily registers renderers to avoid circular import problem.
- Move implementation-specific chat utils to the corresponding renderer in `vllm.renderers`.
- Merge `init_tokenizer_from_config` implementation into `cached_tokenizer_from_config`.
- Update `TokenizerRegistry` to only use lazy imports, even for in-tree tokenizers.
- Initialize the renderer in `InputPreprocessor`, replacing the tokenizer initialization inside `LLMEngine` and `AsyncLLM`.
- Replace `EngineClient.get_tokenizer()` with `EngineClient.renderer.get_tokenizer()` to avoid unnecessary async.
- Replace `RequestPrompt` in online server with the prompt formats in `vllm.inputs`

...</description>
  </item>
  <item>
    <title>[Frontend] Add MCP tool streaming support to Responses API</title>
    <link>https://github.com/vllm-project/vllm/pull/30192</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30192</guid>
    <pubDate>Sun, 07 Dec 2025 04:46:48 +0000</pubDate>
    <description>Purpose
This change enables streaming support for MCP tools when using GPT OSS. It extends the harmony utilities and response serving infrastructure to handle tool streaming, allowing tool calls and their results to be incrementally streamed back to clients rather than returned as a single batch.

Test Plan
curl -X POST &quot;http://localhost:8000/v1/responses&quot;   -H &quot;Content-Type: application/json&quot;   -H &quot;Authorization: Bearer dummy-api-key&quot;   -d &#x27;{
    &quot;model&quot;: &quot;default&quot;,
    &quot;input&quot;: &quot;Multiply 123*456 using the mcp.code_interpreter tool.&quot;,
    &quot;tools&quot;: [{
      &quot;type&quot;: &quot;mcp&quot;,
      &quot;server_label&quot;: &quot;code_interpreter&quot;,
      &quot;headers&quot;: {&quot;test&quot;: &quot;test&quot;},
      &quot;server_url&quot;: &quot;IGNORED&quot;
    }],
    &quot;stream&quot;: true,
    &quot;enable_response_messages&quot;: true
  }&#x27;
Test Result
event: response.created
data: {&quot;

...</description>
  </item>
  <item>
    <title>[Perf] Add benchmark script for triton unified attention kernel performance</title>
    <link>https://github.com/vllm-project/vllm/pull/30191</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30191</guid>
    <pubDate>Sun, 07 Dec 2025 03:13:29 +0000</pubDate>
    <description>
## Purpose
Add a script for benchmarking triton unified attention kernel performance and compare against flash attention. This could be useful for testing new hardware performance etc 

## Test Plan
```

(vllm) ubuntu@209-20-157-255:~/vllm$ python benchmarks/kernels/benchmark_triton_unified_attention.py
flash_attn not available. Only Triton Unified Attention will be benchmarked.
================================================================================
Triton Unified Attention Benchmark
================================================================================
Configuration:
  dtype: torch.bfloat16
  num_query_heads: 32
  num_kv_heads: 8
  head_size: 128
  block_size: 16
  use_alibi: False
  sliding_window: -1
  softcap: 0.0
  FlashAttention available: False
==================

...</description>
  </item>
  <item>
    <title>[responsesAPI][8] input/output messages for ResponsesParser</title>
    <link>https://github.com/vllm-project/vllm/pull/30158</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30158</guid>
    <pubDate>Fri, 05 Dec 2025 22:09:21 +0000</pubDate>
    <description>## Purpose

- add input/output messages to ResponsesParser. If we have a 2 turn conversation (user input 1, output 1, MCP call, input 2, output 2), we&#x27;ll have user input 1 in response.input_messages, and [output 1, input 2, output 2] in response.output_messages).
- refactor make_response_output_items_from_parsable_context to be a class function of ResponsesParser.

NOTE: This PR must be merged in after #30230, this builds on top of https://github.com/vllm-project/vllm/issues/30115

## Test Plan


```
 CUDA_VISIBLE_DEVICES=4,5,6,7 VLLM_USE_EXPERIMENTAL_PARSER_CONTEXT=1 vllm serve MiniMaxAI/MiniMax-M2   --tensor-parallel-size 4   --tool-call-parser minimax_m2   --reasoning-parser minimax_m2    --enable-auto-tool-choice --trust-remote-code  --tool-server=localhost:8081/container,localhost:808

...</description>
  </item>
  <item>
    <title>[Bugfix] Use PIECEWISE cudagraph with gpt-oss on Ampere</title>
    <link>https://github.com/vllm-project/vllm/pull/30096</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30096</guid>
    <pubDate>Fri, 05 Dec 2025 02:01:30 +0000</pubDate>
    <description>## Purpose

Multiple user reports have come in about repeated, infinite generations by gpt-oss models on Ampere hardware. After reproducing this on Ampere hardware, I was able to workaround it by downgrading to PIECEWISE cudagraphs. FULL and FULL_AND_PIECEWISE can reliably reproduce infinite generation on the 2nd and later requests.

The check here to downgrade only triggers for gpt-oss models on Ampere or older CUDA GPUs. This matches where I was able to reproduce the issue and with what appears to be the largest number of user reports.

There could potentially be problems on more MoE models or with a slightly larger range of GPUs, but thought it best to make the most targeted fix for now just to get these models working reliably again on Ampere GPUs. It&#x27;s probably worth auditing the enti

...</description>
  </item>
  <item>
    <title>Added regression test for openai/harmony/issues/78</title>
    <link>https://github.com/vllm-project/vllm/pull/29830</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29830</guid>
    <pubDate>Tue, 02 Dec 2025 01:06:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Add regression tests for [openai/harmony#78](https://github.com/openai/harmony/issues/78), particularly the FastAPI `vllm serve` path which wasn&#x27;t fully addressed in #26185.

## Test Plan

Modify the existing `test_output_messages_enabled` test in test_response_api_with_harmony.py to add the following validations:

```python
    for _message in [*response.input_messages, *response.output_messages]:
        for _item in _message.get(&quot;content&quot;):
            assert isinstance(_item, dict), _message
            assert len(_item) &gt; 0, _message
```

&lt;details&gt;&lt;summary&gt;full code details&lt;/summary&gt;

```python
@pytest.mark.asyncio
@pytest.mark.parametrize(&quot;model_name&quot;, [MODEL_NAME])
async def test_output_messages_enabled(client: OpenAI, model_name: str, serv

...</description>
  </item>
  <item>
    <title>[Frontend] Add streaming tool-call support to Responses API (non-Harmony)</title>
    <link>https://github.com/vllm-project/vllm/pull/29726</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29726</guid>
    <pubDate>Sat, 29 Nov 2025 12:18:12 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Fix for #29725, 

### Summary
This pull request fixes an issue where non harmony models using the Responses API with streaming and tools emit only ResponseTextDeltaEvent events, instead of ResponseFunctionCallArgumentsDeltaEvent when a tool call is selected. This prevents clients from reliably detecting and parsing tool call arguments from the stream.

### Fix
This change updates the streaming path for non harmony models so that:

When the model selects a tool call, the arguments are surfaced as ResponseFunctionCallArgumentsDeltaEvent instead of plain text deltas. The event structure is now consistent with harmony models and with the non streaming Responses API behavior. With this, clients can treat harmony and non harmony models uniformly when han

...</description>
  </item>
  <item>
    <title>Flashrl</title>
    <link>https://github.com/vllm-project/vllm/pull/29586</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29586</guid>
    <pubDate>Thu, 27 Nov 2025 05:56:16 +0000</pubDate>
    <description>## Updating vLLM version to 0.11 with flash-rl updates and patches</description>
  </item>
  <item>
    <title>fix: Add validation for tool requests that the tool is available</title>
    <link>https://github.com/vllm-project/vllm/pull/29498</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29498</guid>
    <pubDate>Wed, 26 Nov 2025 14:26:48 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

  - Add request-level validation so Harmony tool calls fail fast when the requested built-in tool is not available on the configured tool server (browser/
    code_interpreter/container). This prevents the silent ‚Äúbest-effort‚Äù behavior noted in the PR discussion and surfaces an invalid_request_error instead.

## Test Plan

  - .venv/bin/pytest tests/entrypoints/openai/test_serving_responses.py -q

## Test Result

  - 6 passed, 3 warnings (module-level tests including new coverage)

## Fix PR

https://github.com/vllm-project/vllm/issues/29432

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [x] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The tes

...</description>
  </item>
  <item>
    <title>[Bug]: Add validation for tool requests that the tool is available</title>
    <link>https://github.com/vllm-project/vllm/issues/29432</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/29432</guid>
    <pubDate>Tue, 25 Nov 2025 19:04:15 +0000</pubDate>
    <description>### üêõ Describe the bug

Currently there is no check for this, vLLM will just respond sub-optimally in a &quot;silent&quot; way. We should fail the individual request in this case with appropriate error message.

This is a problem in particular for example when using the demo tool server without the `gpt-oss` package installed.

See discussion here: https://github.com/vllm-project/vllm/pull/29336#issuecomment-3573053391
</description>
  </item>
  <item>
    <title>Support compressed-tensors W4A8 MoE checkpoints in GptOssModel weight loader for CPU</title>
    <link>https://github.com/vllm-project/vllm/pull/29315</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29315</guid>
    <pubDate>Mon, 24 Nov 2025 14:43:11 +0000</pubDate>
    <description>1. Add GptOssModel.load_per_expert_unfused_w4a8 helper to handle per-expert unfused MoE weights (gate_proj, up_proj, down_proj) in W4A8 checkpoints and map them into the fused FusedMoE layout (w13_* and w2_* parameters).
	‚Ä¢	Handles .weight, .weight_scale, .bias, and .input_scale suffixes.
	‚Ä¢	For biases, manually slices and writes into the appropriate columns of w13_bias (gate vs up) and w2_bias, supporting both 1D and 2D parameter layouts and using expert_id to pick the correct expert slice when the source tensor has an extra expert dimension.
	‚Ä¢	For weights/scales, delegates to a custom weight_loader when present, falling back to default_weight_loader otherwise, and surfaces whether the mapping was successfully handled.

2. Extend _load_weights_other to:
	‚Ä¢	Detect W4A8 (int4 weights, int8

...</description>
  </item>
  <item>
    <title>Fix gpt oss tool parser v2</title>
    <link>https://github.com/vllm-project/vllm/pull/29236</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29236</guid>
    <pubDate>Sat, 22 Nov 2025 11:27:13 +0000</pubDate>
    <description>Purpose
This PR fixes critical issues with tool call extraction in the OpenAIToolParser (Harmony/gpt-oss), specifically when using Custom Tools and MCP (Model Context Protocol).

Previously, the parser failed to correctly capture tool calls that resided in the active stream buffer (current_content) but hadn&#x27;t yet been finalized into a message object. This led to two major issues:

Tool Call Failure: valid tool calls were dropped or ignored.

Channel Leakage/Hallucination: The model appeared to hallucinate or leak tool call structures into the final (user-facing) message channel instead of processing them as structured tool calls.

Changes Implemented:

Active Buffer Parsing: Added logic to inspect parser.current_content and parser.current_channel. If the active channel is commentary (used 

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] add quark format moe mapping in WeightsMapper for gpt-oss model</title>
    <link>https://github.com/vllm-project/vllm/pull/29008</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29008</guid>
    <pubDate>Wed, 19 Nov 2025 11:21:15 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[ROCm][CI] Fixes tests for pytorch nightly and python only builds</title>
    <link>https://github.com/vllm-project/vllm/pull/28979</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28979</guid>
    <pubDate>Wed, 19 Nov 2025 01:26:03 +0000</pubDate>
    <description>This PR fixes tests for labels:
- `Python-only Installation Test`
- `Entrypoints Integration Test (API Server)`</description>
  </item>
  <item>
    <title>[Feature] Generic Model Support via TrainableAttention and ModelRegistry parallelism constructor callback</title>
    <link>https://github.com/vllm-project/vllm/pull/28685</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28685</guid>
    <pubDate>Thu, 13 Nov 2025 21:25:45 +0000</pubDate>
    <description>## Purpose

Implement RFC #28326 to enable users to easily integrate custom models with vLLM for both training and inference, with support for external parallelism libraries (e.g., Megatron-LM, FSDP, DeepSpeed).

Enables use cases like:
  - RL training with vLLM acceleration
  - Integrating third-party models without rewriting parallelism logic
  - Mixing external tensor parallel layers (Megatron) with vLLM attention

## Test Plan

```
# example, should be a test
python examples/offline_inference/custom_model_with_megatron.py
```
  - Full transformer using Megatron-LM&#x27;s ColumnParallelLinear + RowParallelLinear for MLPs
  - TrainableFlashAttention for attention (with KV cache + backward pass)
  - Ground truth validation against independent PyTorch run
  - Tests TP=4, dynamic batching, train

...</description>
  </item>
  <item>
    <title>[Bug]: rocm crash AMD Ryzen AI 9 HX PRO 370 w/ Radeon 890M - docker/podman</title>
    <link>https://github.com/vllm-project/vllm/issues/28460</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28460</guid>
    <pubDate>Tue, 11 Nov 2025 10:16:32 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0
Clang version                : 20.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-7.0.0 25314 f4087f6b428f0e6f575ebac8a8a724dab123d06e)
CMake version                : version 3.31.6
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0a0+git1c57644
Is debug build               : False
CUDA used to build PyTorc

...</description>
  </item>
  <item>
    <title>[Core] Parse vLLM engine required fields from hf_config to model_arch_config</title>
    <link>https://github.com/vllm-project/vllm/pull/28454</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28454</guid>
    <pubDate>Tue, 11 Nov 2025 08:25:52 +0000</pubDate>
    <description>## Purpose
See https://github.com/vllm-project/vllm/issues/24384 for more context.

Use llama3 as prototype 

## Design
- `model_arch_config` explicitly specify all standardized fields required for vllm runtime
- model arch parser read from config.json/params.json/etc. and perform the standardization process. The goal is to eventually remove most of the standardization logic from config/model.py once the migration to the new parser workflow is complete
- For hf-model-arch-parser, if the model is not in `_CONFIG_REGISTRY`, we still call `AutoConfig.from_pretrained`. This allows us to leverage the normalization already implemented in HuggingFace‚Äôs `PretrainedConfig`. A more standardized PretrainedConfig will enable a thinner, simpler parser layer.
https://github.com/vllm-project/vllm/blob/f0

...</description>
  </item>
  <item>
    <title>Fix/responses api harmony channel metadata    #28262</title>
    <link>https://github.com/vllm-project/vllm/pull/28355</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28355</guid>
    <pubDate>Sun, 09 Nov 2025 00:48:27 +0000</pubDate>
    <description> 
 
## Purpose
Fix issue #28262: Restore missing channel metadata when converting Responses API output items back to Harmony Messages for multi-turn conversations.
Changes:
Set channel=&#x27;commentary&#x27; for function_call_output type inputs
Set channel=&#x27;analysis&#x27; or &#x27;commentary&#x27; for reasoning type based on the following message (commentary if followed by function_call, analysis otherwise)
Add test to verify channel metadata is correctly preserved across conversation turns
Update parse_response_input() to accept optional next_msg parameter for context-aware channel assignmen
## Test Plan
pytest tests/entrypoints/openai/test_response_api_with_harmony.py::test_function_call_with_previous_input_messages -v
## Test Result
pass 
---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description

...</description>
  </item>
  <item>
    <title>[Bugfix] Parse gpt-oss refusals w/ newer openai-harmony</title>
    <link>https://github.com/vllm-project/vllm/pull/28303</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28303</guid>
    <pubDate>Fri, 07 Nov 2025 15:35:05 +0000</pubDate>
    <description>## Purpose

The output generated by gpt-oss models does not always strictly follow its expected harmony chat template format. This commonly - but not exclusively - happens when gpt-oss-120b generates refusals for content that violates its built-in safety guidelines.

To fix this, a non-strict mode was added to the openai-harmony library to allow attempted recovery of malformed message headers in the model output, such as a missing `&lt;|message|&gt;` special token before the assistant text.

This will resolve some cases where the error
`openai_harmony.HarmonyError: unexpected tokens remaining in message header` was previously thrown. It will not resolve all of those, as not every malformed message output can be recovered. Other ongoing work around using structured output for the Harmony format c

...</description>
  </item>
  <item>
    <title>[Frontend] [gpt-oss] Chat format GD for tool calling with gptoss</title>
    <link>https://github.com/vllm-project/vllm/pull/28148</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28148</guid>
    <pubDate>Wed, 05 Nov 2025 18:17:40 +0000</pubDate>
    <description>## Purpose
Building on top of the initial gpt-oss reasoning parser in #25515, this PR fleshes out the full structural tags schema to guide the chat format for gpt-oss.

It is only added to the Responses API path for now, but it should be able to fix issues like the ones mentioned in #24954 without needing to make adjustments to Harmony.

Guided decoding for the gpt-oss chat format should only be enabled if `structured_outputs_config.enable_in_reasoning` is True, since the chat format is technically in the reasoning section of model output. It also lets us continue to use structured outputs for the content of the final message. `structured_outputs_config.reasoning_parser` is set by default for gpt-oss. 

TODO: test that structured output works alongside this after #28000 is merged, and adju

...</description>
  </item>
  <item>
    <title>[Core] Add a random suffix to frontend-provided request IDs</title>
    <link>https://github.com/vllm-project/vllm/pull/27987</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27987</guid>
    <pubDate>Mon, 03 Nov 2025 15:13:12 +0000</pubDate>
    <description>Since #9550 and #10968 we support client&#x27;s supplying a custom request ID. The motivation for this is that it can be very helpful when you need to correlate vLLM logs with logs of a related service.

Since the request ID is used ubiquitously across vLLM as a unique key, it obviously is problematic if we ever have multiple in-flight requests using the same client-provided request ID.

We saw this happening recently when `vllm serve bench` started including a request ID and the request IDs from multiple concurrent instances caused collisions. See #27723

We try to guard against request ID collisions currently in the frontend in OutputProcessor:

```
    def add_request(...):
        if request_id in self.request_states:
            raise ValueError(f&quot;Request id {request_id} already running.&quot;)

...</description>
  </item>
  <item>
    <title>[Quantization] support gpt-oss for quantized kv cache weight loading</title>
    <link>https://github.com/vllm-project/vllm/pull/27980</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27980</guid>
    <pubDate>Mon, 03 Nov 2025 12:03:40 +0000</pubDate>
    <description>## Purpose
This PR aims to fix quantized kv cache model loading issues using AMD-Quark. 
The fix was initially applied to the `gpt-oss` model and may be extended to other models in `vllm/model_executor/models`, as they share a nearly identical methodology.

Here is the original error traceback w/o this PR fixing.
&lt;img width=&quot;1219&quot; height=&quot;468&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/1ce1b92e-04a9-404a-91da-f06151cd54c9&quot; /&gt;

##
Here is a check list of some models defined in `vllm/model_executor/models/` for the supportness of quantized kv cache loading. Models with `get_cache_scale` called is marked as ‚úÖ, while models without `get_cache_scale` called is marked as ‚ö†Ô∏è.

The ‚ö†Ô∏è symbol is preferred over ‚ùå since some models can intrinsically have non-quantized KV cache, for i

...</description>
  </item>
  <item>
    <title>[Bugfix] Handle escaped characters in GLM tool parser to prevent double serialization</title>
    <link>https://github.com/vllm-project/vllm/pull/27970</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27970</guid>
    <pubDate>Mon, 03 Nov 2025 08:37:00 +0000</pubDate>
    <description>## Purpose

This PR fixes a bug where the tool call parser fails when the model&#x27;s output contains literal escaped characters, such as `\n`, `\&quot;`.

**The Problem:**
When GLM-4 outputs tool calls, the XML may contain &#x27;\n&#x27; literal characters and the JSON content within `&lt;arg_value&gt;` tags may contain escaped characters like `\&quot;`, causing regex matching and json parsing both failed. For example:

```
&lt;tool_call&gt;todo_write\n&lt;arg_key&gt;todos&lt;/arg_key&gt;\n&lt;arg_value&gt;[{\&quot;id\&quot;: \&quot;1\&quot;, \&quot;task\&quot;: \&quot;Ê£ÄÊü•ÂêéÁ´Ø‰ª£Á†Å‰∏≠ÁöÑÁ°¨ÁºñÁ†ÅÈóÆÈ¢ò\&quot;, \&quot;status\&quot;: \&quot;in_progress\&quot;}, {\&quot;id\&quot;: \&quot;2\&quot;, \&quot;task\&quot;: \&quot;Ê£ÄÊü•ÂâçÁ´Ø‰ª£Á†Å‰∏≠ÁöÑÁ°¨ÁºñÁ†ÅÈóÆÈ¢ò\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;3\&quot;, \&quot;task\&quot;: \&quot;Ê£ÄÊü•ËøùÂèçÂçï‰∏ÄËÅåË¥£ÁöÑ‰ª£Á†Å\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;4\&quot;, \&quot;task\&quot;: \&quot;ÂàõÂª∫Êï¥ÊîπÂª∫ËÆÆÊä•Âëä\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}]&lt;/arg_value&gt;
&lt;/tool_call&gt;
```

The current regex fails to ma

...</description>
  </item>
  <item>
    <title>[Metrics] [KVConnector] Add Offloading Connector metrics</title>
    <link>https://github.com/vllm-project/vllm/pull/27942</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27942</guid>
    <pubDate>Sun, 02 Nov 2025 15:36:12 +0000</pubDate>
    <description>Added queries and hits metrics for the Offloading Connector.
Also added timing metrics for store and load operations, which take the average time it takes to load/store, per-token.
The metrics are available from Prometheus and from the StatLogger.
## Purpose
Allows collection of timing metrics for the Offloading Connector, which is essential for future development.
@orozery please review.
## Test Plan

## Test Result
</description>
  </item>
  <item>
    <title>[WIP] Enable async scheduling by default</title>
    <link>https://github.com/vllm-project/vllm/pull/27614</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27614</guid>
    <pubDate>Mon, 27 Oct 2025 23:07:26 +0000</pubDate>
    <description>Currently just doing full CI test to flush out any issues.
</description>
  </item>
  <item>
    <title>[Attention] Use sparse prefill kernel for fp8 kv-cache in DeepSeek-v3.2</title>
    <link>https://github.com/vllm-project/vllm/pull/27532</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27532</guid>
    <pubDate>Sun, 26 Oct 2025 13:30:51 +0000</pubDate>
    <description>When doing prefill up-convert the kv-cache from fp8 to bf16 and call the bf16 prefill kernel instead of the decode kernel. This PR introduce global workspace management to have the bf16 workspace overlap with the MoE workspace buffers.

## GSM8K Accuracy (DeepSeek-V3.2, FP8 KV-cache, TP=8)

| Branch | Accuracy |
|--------|----------|
| Main | 95.30% ¬± 0.58% |
| PR (Hybrid) | **95.53% ¬± 0.57%** |

## Benchmark Results (4096 in, 512 out, 100 prompts)

| Config | Branch | Req/s | Tok/s | Delta |
|--------|--------|-------|-------|-------|
| TP=8, FP8 | Main | 1.58 | 806 | ‚Äî |
| TP=8, FP8 | PR | 1.58 | 806 | **0%** |
| DP=8+EP, FP8 | Main | 2.36 | 1207 | ‚Äî |
| DP=8+EP, FP8 | PR | **2.53** | **1293** | **+7%** |

## Hybrid Approach

Uses `MIN_HEADS_FOR_BF16_PREFILL = 32` threshold:
- **TP=8** (

...</description>
  </item>
  <item>
    <title>[WIP] [GPT-OSS] customized symm_mem based EP comm kernel integration</title>
    <link>https://github.com/vllm-project/vllm/pull/27495</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27495</guid>
    <pubDate>Sat, 25 Oct 2025 01:04:50 +0000</pubDate>
    <description>Integrates Triton MoE communication kernels

achieved 60% throughput increase compared to #[24588](https://github.com/vllm-project/vllm/pull/24588)

&lt;img width=&quot;3918&quot; height=&quot;1996&quot; alt=&quot;36117&quot; src=&quot;https://github.com/user-attachments/assets/ad93165c-6869-4a01-8c5b-836f4421f656&quot; /&gt;
</description>
  </item>
  <item>
    <title>[CI/Build] Add aime25 eval to gpt-oss CI</title>
    <link>https://github.com/vllm-project/vllm/pull/27398</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27398</guid>
    <pubDate>Thu, 23 Oct 2025 07:38:49 +0000</pubDate>
    <description>## Purpose
This PR added eval config for gpt-oss-120b on H100. 

## Test Plan
pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-gpt-oss.txt --tp-size=1

## Test Result

1 passed, 6 warnings in 455.90s (0:07:35) 

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the releas

...</description>
  </item>
  <item>
    <title>[Bug]: gptoss calls built-in tool when no tools are given</title>
    <link>https://github.com/vllm-project/vllm/issues/27385</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27385</guid>
    <pubDate>Thu, 23 Oct 2025 04:01:36 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
==============================
        System Info
==============================
OS                           : Ubuntu 20.04.6 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-2ubuntu1~20.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 3.29.2
Libc version                 : glibc-2.31

==============================
       PyTorch Info
==============================
PyTorch version              : 2.8.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version

...</description>
  </item>
  <item>
    <title>[Bugfix] Actually enable serialize_messages for harmony Responses (related to #26185)</title>
    <link>https://github.com/vllm-project/vllm/pull/27377</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27377</guid>
    <pubDate>Thu, 23 Oct 2025 01:07:10 +0000</pubDate>
    <description>## Purpose

For the OpenAI-compatible `v1/responses` route, enable raw messages to be sent when `enable_response_messages` is set to True in `extra_body`.

Previously, the responses are empty because of an issue in openai/harmony. (https://github.com/openai/harmony/issues/78)

https://github.com/vllm-project/vllm/pull/26185 implements most of the fix, but these aren&#x27;t actually invoked, at least not when serving the model through the `vllm serve`. The reason is that the said PR specifies `when_used=&quot;json&quot;`. Thus, this serialization method is ignored because of the use of `model_dump()` in [vllm/entrypoints/openai/api_server.py#L527-L529](https://github.com/vllm-project/vllm/blob/a0003b56b0b822c52bb0f3035c164370a802e6f5/vllm/entrypoints/openai/api_server.py#L527-L529).

The fix is to trigger

...</description>
  </item>
  <item>
    <title>[Frontend][gpt-oss] Allow system message to overwrite model identity</title>
    <link>https://github.com/vllm-project/vllm/pull/27310</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27310</guid>
    <pubDate>Wed, 22 Oct 2025 00:57:44 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
The current way of implementation forces the system prompt to go into the developer message and it confuses the usage of different roles. We should have developer into developer message, and allow system message to override the system prompt as supported by signature of harmony_utils.py

## Test Result
Override
            &quot;openai_harmony_message&quot;: {
                &quot;author&quot;: {
                    &quot;role&quot;: &quot;system&quot;
                },
                &quot;content&quot;: [
                    {
                        &quot;system_content&quot;: {
                            &quot;type&quot;: &quot;system_content&quot;,
                            &quot;model_identity&quot;: &quot;You are a helpful agent reponds in Chinese.&quot;,
                            &quot;reasoning_effort&quot;: &quot;high&quot;,
                       

...</description>
  </item>
  <item>
    <title>Support PP on tpu_inference</title>
    <link>https://github.com/vllm-project/vllm/pull/27296</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27296</guid>
    <pubDate>Tue, 21 Oct 2025 21:58:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>Experimental attention backend in helion</title>
    <link>https://github.com/vllm-project/vllm/pull/27293</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27293</guid>
    <pubDate>Tue, 21 Oct 2025 20:10:50 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

very experimental and draft PR so far

## Test Plan

```
VLLM_ATTENTION_BAKCEND=EXPERIMENTAL_HELION_ATTN vllm serve meta-llama/Llama-3.1-8B-Instruct 
```
## Test Result

t.b.a.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in 

...</description>
  </item>
  <item>
    <title>Add process pool support fro tokenizer</title>
    <link>https://github.com/vllm-project/vllm/pull/27180</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27180</guid>
    <pubDate>Mon, 20 Oct 2025 03:02:35 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;


## Purpose
The preprocessing stage experiences a bottleneck under high concurrency, resulting in extremely long preprocessing times. By introducing additional processes to handle tokenization, this bottleneck can be alleviated.

Related PR: #25301


## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Rel

...</description>
  </item>
  <item>
    <title>[GPT-OSS] Persistent Masked Activation Kernel</title>
    <link>https://github.com/vllm-project/vllm/pull/27100</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27100</guid>
    <pubDate>Fri, 17 Oct 2025 13:02:02 +0000</pubDate>
    <description>
## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Bug] Pass correct ep_rank_start and ep_rank_end</title>
    <link>https://github.com/vllm-project/vllm/pull/27025</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27025</guid>
    <pubDate>Thu, 16 Oct 2025 13:30:25 +0000</pubDate>
    <description>## Purpose
- _load_weights_others expect the parameters as ep_rank_start and ep_rank_end but its passed in the reverse order
- Fix to pass the correct parameters

## Test Plan
Tested with gpt-oss and enabling the expert parallelism

## Test Result
Earlier gpt-oss was failing when expert parallelism was enabled, after the fix it started passing
</description>
  </item>
  <item>
    <title>[Frontend][gpt-oss] Support all MCP servers for gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/26704</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/26704</guid>
    <pubDate>Mon, 13 Oct 2025 12:51:59 +0000</pubDate>
    <description>## Purpose
This PR is an implementation of #26703 

Note: This PR is really big, but there was a lot that had to be cleaned up at the same time. A large amount of the code is from adding tests to existing code and a MCP server to test with and new MCP tests.

There are three main motivations:
- vLLM should support integrating with all MCP servers
  - TODO: Test with enterprise ones like Github
- Clean up gpt-oss tool specific code paths in vLLM
  - Tools and models change, but MCP is a protocol meant to support all models
  - Sets the stage for full MCP integration for all tool calling models
- Avoid losing information due to lossy OpenAI types for output
  - An example below

```
# What you can return to the user on the ResponsesAPI for this tool call with the OpenAI types.
class ActionOp

...</description>
  </item>
  <item>
    <title>fix json schema alias serializing when streaming</title>
    <link>https://github.com/vllm-project/vllm/pull/26356</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/26356</guid>
    <pubDate>Tue, 07 Oct 2025 14:12:21 +0000</pubDate>
    <description>## Purpose

Fix a serialization bug in streaming responses where Pydantic field aliases (e.g. `schema` ‚Üí `schema_`) were not preserved during `.model_dump()` calls.

This caused the `&quot;schema_&quot;` key to appear instead of `&quot;schema&quot;` in streamed response events for JSON schema output formats, breaking compatibility with the OpenAI SDK‚Äôs `ResponseFormatTextJSONSchemaConfig` parsing.

**Related issue:** [vllm-project/vllm#26288](https://github.com/vllm-project/vllm/issues/26288)

### Root Cause
- `ResponsesResponse.from_request(...).model_dump()` was called without `by_alias=True` at:
  - `vllm/entrypoints/openai/serving_responses.py:1830`
  - `vllm/entrypoints/openai/serving_responses.py:1879`
- Without `by_alias=True`, Pydantic outputs internal field names (e.g. `schema_`) instead of their ali

...</description>
  </item>
  <item>
    <title>[MODEL] Fix handling of multiple channels for gpt-oss with speculative decoding </title>
    <link>https://github.com/vllm-project/vllm/pull/26291</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/26291</guid>
    <pubDate>Mon, 06 Oct 2025 11:26:54 +0000</pubDate>
    <description>Hi team!

## Purpose

We&#x27;ve noticed that that the¬†[recent PR](https://github.com/vllm-project/vllm/pull/26027)¬†doesn&#x27;t fully fix gpt-oss + streaming + speculative-decoding issue, for example generated messages end abruptly. This happens because multiple tokens can relate to different channels (e.g.¬†`&lt;final&gt;`,¬†`&lt;analysis&gt;`,¬†`None`) in one decoding stage. This PR handles it.

## Test Plan

Server command:

```
vllm serve openai/gpt_oss_20b--speculative-config &#x27;{&quot;method&quot;: &quot;eagle3&quot;, &quot;model&quot;: &lt;name-of-your-draft-model&gt;}&#x27;
```

Test script `streaming_client.py`:

```
#!/usr/bin/env python3

import asyncio
import sys
from typing import List, Dict

import httpx
from openai import AsyncOpenAI


class StreamingClient:
    def __init__(self, api_url: str = &quot;http://127.0.0.1:8000/v1&quot;, api_key: str = &quot;E

...</description>
  </item>
  <item>
    <title>[EPLB] Offline eplb support</title>
    <link>https://github.com/vllm-project/vllm/pull/26176</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/26176</guid>
    <pubDate>Fri, 03 Oct 2025 15:30:00 +0000</pubDate>
    <description>## Purpose
Offline eplb- make it possible to rearrange experts once at the start time.
This allows rebalancing experts when engine starts. With `static` experts are rebalanced only once when engine starts,
this allows better experts balancedness without metrics tracking/rearrangement overhead.

EPLB has more options now:
- `load_initial_load_window` and `load_path`:
Whether to use initial load window and corresponding path.
- `save_load_window` and `load_path`:
Whether to save load window and corresponding path.
- `static`:
Whether to actively rebalance experts during runtime.

## Test Plan
Integration test test_eplb_offline.py added.
## Test Result
Tests pass</description>
  </item>
  <item>
    <title>[Bugfix] fixing streaming issues and tool call output for gpt-oss (#22704)</title>
    <link>https://github.com/vllm-project/vllm/pull/25728</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/25728</guid>
    <pubDate>Fri, 26 Sep 2025 00:32:21 +0000</pubDate>
    <description>## Purpose
Closely related to bugs associated with streaming for gpt-oss response API (#22704):

Harmony‚Äôs streaming didn‚Äôt support `functions.*` tool calls at all. We handled reasoning on `analysis`, final text on `final`, and built-ins like code/web‚Äîbut `&quot;commentary&quot;` messages with a `recipient=&quot;functions.NAME&quot;` had a no-op branch.

## Test Plan

First, tested GPT-OSS with a basic set of instructions, few-shot examples, a function definition, and a prompt to find the weather in San Francisco, using the get_weather function call as input.

Next, tested with the following relevant tests:
```
- pytest -s -v vllm/tests/v1/entrypoints/openai/responses/test_basic.py::test_simple_input
- pytest -s -v vllm/tests/v1/entrypoints/openai/responses/test_basic.py::test_streaming

```

## Test Result



...</description>
  </item>
</channel>
</rss>
