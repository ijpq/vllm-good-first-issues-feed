<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: gpt-oss</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'gpt-oss' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Sun, 11 Jan 2026 13:34:57 +0000</lastBuildDate>
  <item>
    <title>[Bugfix] Fix Harmony preamble visibility in Responses API</title>
    <link>https://github.com/vllm-project/vllm/pull/32114</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/32114</guid>
    <pubDate>Sun, 11 Jan 2026 12:40:51 +0000</pubDate>
    <description>##  Purpose

Fix incorrect handling of Harmony format preambles in the Responses API.

Per the https://cookbook.openai.com/articles/openai-harmony, preambles (commentary channel messages with recipient=None) are explanatory text intended to be shown to end-users (e.g., &quot;I&#x27;ll search for that information now...&quot;). The current implementation incorrectly treats them as hidden reasoning content.

This PR changes how commentary-channel preambles are parsed:
  - Before: ResponseReasoningItem (hidden in reasoning_content)
  - After: ResponseOutputMessage (visible in content)

Related: This is a follow-up fix to #29972, which added GPT-OSS models with Harmony chat format.

## Migration Note

Breaking Change: Preambles now appear in content instead of reasoning_content. Clients that relied on the pr

...</description>
  </item>
  <item>
    <title>[Frontend] Add `reasoning_effort` and `parallel_tool_calls` to `extra_args` of `SamplingParams`.</title>
    <link>https://github.com/vllm-project/vllm/pull/31952</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31952</guid>
    <pubDate>Thu, 08 Jan 2026 07:25:45 +0000</pubDate>
    <description>## Purpose

~~I suggest adding two fields, `reasoning_effort` and `parallel_tool_calls`, to `SamplingParams` to enable logits processors to support the OpenAI-compatible parameters by controlling token generation.~~

I suggest adding two fields, `reasoning_effort` and `parallel_tool_calls`, to `extra_args` of `SamplingParams` to enable logits processors to support the OpenAI-compatible parameters by controlling token generation.

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e 

...</description>
  </item>
  <item>
    <title>[Misc] Add VLLM_USE_FLASHINFER_ROPE to control the RoPE kernel for cuda</title>
    <link>https://github.com/vllm-project/vllm/pull/31893</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31893</guid>
    <pubDate>Wed, 07 Jan 2026 10:49:29 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Related PRs:
#21126 integrated the Flashinfer RoPE kernel.
#25299 disabled it by default due to CI failures.
#30729 enabled it by default for Deepseek RoPE.

This PR adds `VLLM_USE_FLASHINFER_ROPE` env so that we can easily enable Flashinfer RoPE kernel.

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes up

...</description>
  </item>
  <item>
    <title>[responsesAPI] get reasoning token metrics for simpleContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31839</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31839</guid>
    <pubDate>Tue, 06 Jan 2026 22:18:30 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Bugfix] Sanitize malformed tool call recipients in Harmony parser</title>
    <link>https://github.com/vllm-project/vllm/pull/31677</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31677</guid>
    <pubDate>Sun, 04 Jan 2026 09:30:09 +0000</pubDate>
    <description>Some GPT-OSS base models occasionally generate malformed Harmony format sequences like `to=functions.bash&lt;|channel|&gt;commentary` instead of the correct `to=functions.bash &lt;|constrain|&gt;json`. This causes the function name to be parsed incorrectly as `bash&lt;|channel|&gt;commentary` instead of `bash`.

This fix sanitizes the recipient string by stripping `&lt;|channel|&gt;` and everything after it before extracting the function name. The fix is applied in three locations to cover all API endpoints:

- `harmony_utils.py`: /v1/responses (non-streaming)
- `openai_tool_parser.py`: /v1/chat/completions (non-streaming)
- `serving_chat_stream_harmony.py`: /v1/chat/completions (streaming)

The /v1/responses streaming endpoint already worked correctly because it captures the recipient before malformed tokens can

...</description>
  </item>
  <item>
    <title>[FIX] Add NO_MUL activation support for modular kernel path</title>
    <link>https://github.com/vllm-project/vllm/pull/31528</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31528</guid>
    <pubDate>Tue, 30 Dec 2025 10:38:02 +0000</pubDate>
    <description>This PR adds support for `*_no_mul` activations (e.g., `relu2_no_mul`) in the modular 
kernel MoE path (`TritonExperts`).

### Problem
The modular kernel path assumed all activations use gate/up multiplication (like SiLU, GELU),
where output size is `N/2`. For `*_no_mul` activations, which apply activation directly 
without gating, output size should equal input size (`N`). This caused assertion failures 
and buffer size mismatches.



Attional Change:
`modular_kernel.py`: Updated abstract `workspace_shapes()` signature and `_allocate_buffers()` to accept and pass activation parameter
All `FusedMoEPermuteExpertsUnpermute` implementations: Updated to accept activation parameter and use activation-aware workspace sizing where applicable

---

&gt; [!NOTE]
&gt; Introduces activation-aware workspace

...</description>
  </item>
  <item>
    <title>[LoRA] Simplify gpt-oss w13_lora_b weight loading</title>
    <link>https://github.com/vllm-project/vllm/pull/31255</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31255</guid>
    <pubDate>Wed, 24 Dec 2025 02:55:40 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

This PR simplifies gpt-oss `w13_lora_b` weight loading code.

gpt-oss `lora_b` weights is in interleaved layout `[w1_0, w3_0, w1_1, w3_1, ...]`. The current `_slice_w13_b` code do de-interleaving, slicing, and re-interleaving. But the code can be simplified by directly returning a contiguous slice of the original interleaved dimension.

Below is a short example of the original code:

```
&gt;&gt;&gt; start_idx = 0
&gt;&gt;&gt; end_idx = 2
&gt;&gt;&gt; w13_lora_b = torch.tensor([[[11], [21], [12], [22], [13], [23], [14], [24]]]) # shape [num_experts, output_size, rank]
&gt;&gt;&gt; print(w13_lora_b)
tensor([[[11],
         [21],
         [12],
         [22],
         [13],
         [23],
         [14],
         [24]]])
&gt;&gt;&gt; 
&gt;&gt;&gt; w1_lora_b = w13_lora_b[:, ::2, :] # even positions
&gt;&gt;&gt; p

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31250</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31250</guid>
    <pubDate>Wed, 24 Dec 2025 00:18:46 +0000</pubDate>
    <description>This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and allows us to reuse the weights wh

...</description>
  </item>
  <item>
    <title>LoRA Slab Optimization</title>
    <link>https://github.com/vllm-project/vllm/pull/31247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31247</guid>
    <pubDate>Tue, 23 Dec 2025 23:27:39 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;
This draft PR adds an experimental LoRA slab optimization that builds one contiguous CPU slab for all LoRA adapters and performs a single host-to-device copy, reducing fragmented cudaMemcpy operations in multi-LoRA inference.

## Purpose
This PR aims to optimize the host-to-device transfer of LoRA weights by using a slab-based method. Instead of sending fragmented chunks, we perform a single consolidated host-to-device transfer. This approach significantly reduces overhead. For example, with max-lora=128, the host-to-device transfer and activation now takes about **17ms** in total, compared to around **200ms** on the main branch.
Additionally, this PR implements caching of the LoRA weights after the initial build. This avoids repeated CPU pinning costs and all

...</description>
  </item>
  <item>
    <title>Cleanup basic and entrypoint test organisation</title>
    <link>https://github.com/vllm-project/vllm/pull/31228</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31228</guid>
    <pubDate>Tue, 23 Dec 2025 17:33:22 +0000</pubDate>
    <description>Supersedes https://github.com/vllm-project/vllm/pull/27747:

- Don&#x27;t call basic test files individually
- Move the unit tests into a `unit` directory so we don&#x27;t have to manually ignore everything else
- Move `openai/tool_parsers` into the `unit` tests because that&#x27;s what they are</description>
  </item>
  <item>
    <title>[Feature] add StreamableResponsesParser and token usage counting for ParsableContext</title>
    <link>https://github.com/vllm-project/vllm/pull/31094</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/31094</guid>
    <pubDate>Sun, 21 Dec 2025 11:11:46 +0000</pubDate>
    <description># Purpose

#30759 

- Count the token usage for ParsableContext properly, including `reasoning_tokens` and `output_tokens` and `tool_output_tokens`.
- A StreamableResposesParser is created to parse output token one by one, which can also be used to create an StreamParsableContext

# Design

## Overview

This PR introduces a new `StreamableResponsesParser` that processes output tokens one by one, enabling accurate token counting for reasoning tokens in `ParsableContext`.

## Key Changes

### 1. StreamableResponsesParser (`vllm/entrypoints/openai/parser/responses_parser.py`)

A new streaming parser that processes tokens incrementally:

- **`current_channel`** property: Identifies the current token type
  - `&#x27;analysis&#x27;`: reasoning token
  - `&#x27;commentary&#x27;`: tool call token
  - `&#x27;final&#x27;`: regul

...</description>
  </item>
  <item>
    <title>Add positional embedding and kv_cache fusion for llama and gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30978</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30978</guid>
    <pubDate>Thu, 18 Dec 2025 17:57:04 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>Use aiter triton fused_add_rmsnorm_pad for gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/30976</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30976</guid>
    <pubDate>Thu, 18 Dec 2025 17:39:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Adds fused padding op before router GEMM on ROCm, eliminating this unfused pad after the GEMM before the fused_moe: https://github.com/ROCm/vllm/blob/main/vllm/model_executor/layers/fused_moe/layer.py#1603

Before:
&lt;img width=&quot;1370&quot; height=&quot;30&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/11db1bf1-e8b0-49c5-b180-3d1c730f07ca&quot; /&gt;
After:
&lt;img width=&quot;1462&quot; height=&quot;34&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/e5979f88-03c0-4173-adbd-8227786e929f&quot; /&gt;

Follow-up is to replace this with a single `F.pad` and try fusing CK rmsnorm and pad instead of manually calling this kernel.

See also #30357

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- 

...</description>
  </item>
  <item>
    <title>[Perf] Eliminate padding and slicing op for GPT-OSS with Flashinfer MXFP4 MXFP8 MoE</title>
    <link>https://github.com/vllm-project/vllm/pull/30647</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30647</guid>
    <pubDate>Sun, 14 Dec 2025 13:40:05 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

- Depends on Flashinfer update #30993
- Eliminated padding op before the MoE: by setting the alignment in flashinfer mxfp8 quant, the output quantized tensor will be padded.
- Eliminated slicing op after the MoE: by passing the output tensor with unpadded hidden size to MoE kernel, this depends on a Flashinfer PR:
  - https://github.com/flashinfer-ai/flashinfer/pull/2217
  - This will also resolve the previous AR+Norm fusion broken by slice op issue: #28841
- Cleaned up the padding logic: for mxfp4 quant, the padded hidden size is calculated in `create_weights()`, the `maybe_roundup_hidden_size()` in `vllm/model_executor/layers/fused_moe/layer.py` seems like a dup.


## Test Plan &amp;&amp; Test Result(GPT-OSS-120b TP8)

### Accuracy

PR:
```
[{&#x27;eval_name

...</description>
  </item>
  <item>
    <title>[GPT OSS] Fix tool_choice required</title>
    <link>https://github.com/vllm-project/vllm/pull/30557</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30557</guid>
    <pubDate>Fri, 12 Dec 2025 14:30:24 +0000</pubDate>
    <description></description>
  </item>
  <item>
    <title>[Frontend] Honor chat template for gpt-oss harmony (#23015)</title>
    <link>https://github.com/vllm-project/vllm/pull/30482</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30482</guid>
    <pubDate>Thu, 11 Dec 2025 10:23:37 +0000</pubDate>
    <description>## Issue
- For gpt-oss models, --chat-template (and tokenizer/chat_template_kwargs) are ignored; prompts are rendered via Harmony instead of the normal chat-template pipeline.
## Reason
- When hf_config.model_type == &quot;gpt_oss&quot;, vLLM sets use_harmony and routes requests through _make_request_with_harmony(...), which directly constructs Harmony system/developer/user messages and calls render_for_completion. That path never calls _preprocess_chat / apply_hf_chat_template, so any chat template settings are bypassed.
## Summary
- allow server-level chat_template to be applied even in Harmony (gpt-oss) paths for chat and responses
- pass tokenizer into Harmony preprocessing and render via apply_hf_chat_template when provided, with safe fallback to Harmony default on errors
- tighten typing/forma

...</description>
  </item>
  <item>
    <title>[Optimization]: Add fused router for GPTOSS</title>
    <link>https://github.com/vllm-project/vllm/pull/30471</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30471</guid>
    <pubDate>Thu, 11 Dec 2025 07:55:47 +0000</pubDate>
    <description>&lt;details&gt;
&lt;summary&gt; output collect_env &lt;/summary&gt;

```
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 24.04.3 LTS (x86_64)
GCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version                : Could not collect
CMake version                : version 3.28.3
Libc version                 : glibc-2.39

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version               : 3.12.11

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] GPT OSS, DS FP4 Upstream fp8 with static scales</title>
    <link>https://github.com/vllm-project/vllm/pull/30357</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30357</guid>
    <pubDate>Tue, 09 Dec 2025 19:49:32 +0000</pubDate>
    <description>## gpt-oss120b-w-mxfp4-a-fp8

server:
&gt; VLLM_ROCM_USE_AITER=1 HIP_VISIBLE_DEVICES=1 VLLM_USE_AITER_UNIFIED_ATTENTION=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_ROCM_USE_AITER_FUSED_MOE_A16W4=0 vllm serve /data/models/gpt-oss120b-w-mxfp4-a-fp8     --port 8000     --swap-space 64     --max-model-len 10368     --tensor-parallel-size 1     --max-num-seqs 1024     --gpu-memory-utilization 0.95     --no-enable-prefix-caching     --enforce-eager

client:
&gt; curl http://localhost:8000/v1/completions -H &quot;Content-Type: application/json&quot; -d &#x27;{
    &quot;model&quot;: &quot;/data/models/gpt-oss120b-w-mxfp4-a-fp8&quot;,
    &quot;prompt&quot;: &quot;San Francisco is a&quot;,
    &quot;max_tokens&quot;: 16,
    &quot;temperature&quot;: 0
}&#x27;

output:
&gt; {&quot;id&quot;:&quot;cmpl-9e80e71642c56933&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1765949222,&quot;model&quot;:&quot;/data/models/gpt-oss120b-w-mxfp4-a-fp

...</description>
  </item>
  <item>
    <title>[gpt-oss] Add model_identity to system message retrieval for harmony chat template</title>
    <link>https://github.com/vllm-project/vllm/pull/30247</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30247</guid>
    <pubDate>Mon, 08 Dec 2025 08:43:55 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
When using GPT-OSS, `model_identity` passed in from `chat_template_kwargs` is not being used to adjust model&#x27;s identity. This PR is to pass the `model_identity` from chat/completion to the system message formatter.
## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If you

...</description>
  </item>
  <item>
    <title>Feat/support nemotron h mtp wip</title>
    <link>https://github.com/vllm-project/vllm/pull/30208</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30208</guid>
    <pubDate>Sun, 07 Dec 2025 13:46:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[DO NOT MERGE] Introduce Renderer for processing chat messages (using `ModelConfig`)</title>
    <link>https://github.com/vllm-project/vllm/pull/30200</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30200</guid>
    <pubDate>Sun, 07 Dec 2025 08:30:34 +0000</pubDate>
    <description>Ready for review.

## Purpose

- Prototype an interface, `vllm.renderers.RendererLike`, to process chat messages into engine inputs.
- Introduce `RENDERER_REGISTRY` which lazily registers renderers to avoid circular import problem.
- Move implementation-specific chat utils to the corresponding renderer in `vllm.renderers`.
- Initialize the renderer in `InputPreprocessor`, replacing the tokenizer initialization inside `LLMEngine` and `AsyncLLM`.
- Replace `EngineClient.get_tokenizer()` with `EngineClient.renderer.get_tokenizer()` to avoid unnecessary async.
- Update tests accordingly, and move some tests into a new directory `tests/renderers` that is run under `Async Engine, Inputs, Utils, Worker, Config Test (CPU)`.

Towards #22880 and #23873

Future work:

- Merge `CompletionRenderer` wit

...</description>
  </item>
  <item>
    <title>[Perf] Add benchmark script for triton unified attention kernel performance</title>
    <link>https://github.com/vllm-project/vllm/pull/30191</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30191</guid>
    <pubDate>Sun, 07 Dec 2025 03:13:29 +0000</pubDate>
    <description>
## Purpose
Add a script for benchmarking triton unified attention kernel performance and compare against flash attention. This could be useful for testing new hardware performance etc 

## Test Plan
```

(vllm) ubuntu@209-20-157-255:~/vllm$ python benchmarks/kernels/benchmark_triton_unified_attention.py
flash_attn not available. Only Triton Unified Attention will be benchmarked.
================================================================================
Triton Unified Attention Benchmark
================================================================================
Configuration:
  dtype: torch.bfloat16
  num_query_heads: 32
  num_kv_heads: 8
  head_size: 128
  block_size: 16
  use_alibi: False
  sliding_window: -1
  softcap: 0.0
  FlashAttention available: False
==================

...</description>
  </item>
  <item>
    <title>[Feature] Add track_token_ids for Efficient Selective Token Logprobs Tracking</title>
    <link>https://github.com/vllm-project/vllm/pull/30030</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30030</guid>
    <pubDate>Thu, 04 Dec 2025 07:04:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
This PR adds a new `track_token_ids` parameter to `SamplingParams` that enables efficient tracking of log probabilities (logprobs) for specific tokens without requiring full vocabulary logprobs retrieval. This feature addresses [Issue#29280](https://github.com/vllm-project/vllm/issues/29280)

## Implementation Overview

The feature is implemented across the vLLM V1 engine pipeline, from parameter validation to output processing.

### 1. Parameter Extension (`vllm/sampling_params.py`)

Added `track_token_ids: list[int] | None` parameter with validation ensuring all IDs are non-negative integers:

```python
track_token_ids: list[int] | None = None
&quot;&quot;&quot;List of token IDs to track logprobs for at every generation step.&quot;&quot;&quot;
```

### 2. New Data Structures 

...</description>
  </item>
  <item>
    <title>Added regression test for openai/harmony/issues/78</title>
    <link>https://github.com/vllm-project/vllm/pull/29830</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29830</guid>
    <pubDate>Tue, 02 Dec 2025 01:06:49 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Add regression tests for [openai/harmony#78](https://github.com/openai/harmony/issues/78), particularly the FastAPI `vllm serve` path which wasn&#x27;t fully addressed in #26185.

## Test Plan

Modify the existing `test_output_messages_enabled` test in test_response_api_with_harmony.py to add the following validations:

```python
    for _message in [*response.input_messages, *response.output_messages]:
        for _item in _message.get(&quot;content&quot;):
            assert isinstance(_item, dict), _message
            assert len(_item) &gt; 0, _message
```

&lt;details&gt;&lt;summary&gt;full code details&lt;/summary&gt;

```python
@pytest.mark.asyncio
@pytest.mark.parametrize(&quot;model_name&quot;, [MODEL_NAME])
async def test_output_messages_enabled(client: OpenAI, model_name: str, serv

...</description>
  </item>
  <item>
    <title>[Frontend] Add streaming tool-call support to Responses API (non-Harmony)</title>
    <link>https://github.com/vllm-project/vllm/pull/29726</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29726</guid>
    <pubDate>Sat, 29 Nov 2025 12:18:12 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
Fix for #29725, 

### Summary
This pull request fixes an issue where non harmony models using the Responses API with streaming and tools emit only ResponseTextDeltaEvent events, instead of ResponseFunctionCallArgumentsDeltaEvent when a tool call is selected. This prevents clients from reliably detecting and parsing tool call arguments from the stream.

### Fix
This change updates the streaming path for non harmony models so that:

When the model selects a tool call, the arguments are surfaced as ResponseFunctionCallArgumentsDeltaEvent instead of plain text deltas. The event structure is now consistent with harmony models and with the non streaming Responses API behavior. With this, clients can treat harmony and non harmony models uniformly when han

...</description>
  </item>
  <item>
    <title>Flashrl</title>
    <link>https://github.com/vllm-project/vllm/pull/29586</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29586</guid>
    <pubDate>Thu, 27 Nov 2025 05:56:16 +0000</pubDate>
    <description>## Updating vLLM version to 0.11 with flash-rl updates and patches</description>
  </item>
  <item>
    <title>fix: Add validation for tool requests that the tool is available</title>
    <link>https://github.com/vllm-project/vllm/pull/29498</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29498</guid>
    <pubDate>Wed, 26 Nov 2025 14:26:48 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

  - Add request-level validation so Harmony tool calls fail fast when the requested built-in tool is not available on the configured tool server (browser/
    code_interpreter/container). This prevents the silent “best-effort” behavior noted in the PR discussion and surfaces an invalid_request_error instead.

## Test Plan

  - .venv/bin/pytest tests/entrypoints/openai/test_serving_responses.py -q

## Test Result

  - 6 passed, 3 warnings (module-level tests including new coverage)

## Fix PR

https://github.com/vllm-project/vllm/issues/29432

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [x] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The tes

...</description>
  </item>
  <item>
    <title>Support compressed-tensors W4A8 MoE checkpoints in GptOssModel weight loader for CPU</title>
    <link>https://github.com/vllm-project/vllm/pull/29315</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29315</guid>
    <pubDate>Mon, 24 Nov 2025 14:43:11 +0000</pubDate>
    <description>1. Add GptOssModel.load_per_expert_unfused_w4a8 helper to handle per-expert unfused MoE weights (gate_proj, up_proj, down_proj) in W4A8 checkpoints and map them into the fused FusedMoE layout (w13_* and w2_* parameters).
	•	Handles .weight, .weight_scale, .bias, and .input_scale suffixes.
	•	For biases, manually slices and writes into the appropriate columns of w13_bias (gate vs up) and w2_bias, supporting both 1D and 2D parameter layouts and using expert_id to pick the correct expert slice when the source tensor has an extra expert dimension.
	•	For weights/scales, delegates to a custom weight_loader when present, falling back to default_weight_loader otherwise, and surfaces whether the mapping was successfully handled.

2. Extend _load_weights_other to:
	•	Detect W4A8 (int4 weights, int8

...</description>
  </item>
  <item>
    <title>Fix gpt oss tool parser v2</title>
    <link>https://github.com/vllm-project/vllm/pull/29236</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29236</guid>
    <pubDate>Sat, 22 Nov 2025 11:27:13 +0000</pubDate>
    <description>Purpose
This PR fixes critical issues with tool call extraction in the OpenAIToolParser (Harmony/gpt-oss), specifically when using Custom Tools and MCP (Model Context Protocol).

Previously, the parser failed to correctly capture tool calls that resided in the active stream buffer (current_content) but hadn&#x27;t yet been finalized into a message object. This led to two major issues:

Tool Call Failure: valid tool calls were dropped or ignored.

Channel Leakage/Hallucination: The model appeared to hallucinate or leak tool call structures into the final (user-facing) message channel instead of processing them as structured tool calls.

Changes Implemented:

Active Buffer Parsing: Added logic to inspect parser.current_content and parser.current_channel. If the active channel is commentary (used 

...</description>
  </item>
  <item>
    <title>[ROCm][Quantization] GPT_OSS in amd-quark format model loading and emulations </title>
    <link>https://github.com/vllm-project/vllm/pull/29008</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29008</guid>
    <pubDate>Wed, 19 Nov 2025 11:21:15 +0000</pubDate>
    <description>## Purpose

This PR aims for:
- [x] quark model loading, combined with `mxfp4` loading [function ](https://github.com/vllm-project/vllm/blob/256a33ecb4923a4bfa6e1a9adbc69f5255a5aa6d/vllm/model_executor/models/gpt_oss.py#L309) for original openai/gpt-oss-20b &amp; openai/gpt-oss-120b
- [x] OCPMX_W4A16, OCPMX_W4AFP8 MoE scheme and emulation forward, unified into class `QuarkOCP_MX_MoEMethod`

## Test Plan

- Models:
  - [x] GPT_OSS_20B
  - [x] GPT_OSS_120B
- Quantization schemes:
  - [x] W: MXFP4, A: BF16, (optional) KV: FP8
  - [x] W: MXFP4, A: FP8, (optional) KV: FP8
  - [x] W: MXFP4, A: MXFP4, (optional) KV: FP8
- TP:
  - [x] TP1
  - [x] TP2
  - [x] TP4
  - [x] TP8

See results below.

## Test Result
&lt;img width=&quot;1418&quot; height=&quot;304&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/86

...</description>
  </item>
  <item>
    <title>[Feature] Generic Model Support via TrainableAttention and ModelRegistry parallelism constructor callback</title>
    <link>https://github.com/vllm-project/vllm/pull/28685</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28685</guid>
    <pubDate>Thu, 13 Nov 2025 21:25:45 +0000</pubDate>
    <description>## Purpose

Implement RFC #28326 to enable users to easily integrate custom models with vLLM for both training and inference, with support for external parallelism libraries (e.g., Megatron-LM, FSDP, DeepSpeed).

Enables use cases like:
  - RL training with vLLM acceleration
  - Integrating third-party models without rewriting parallelism logic
  - Mixing external tensor parallel layers (Megatron) with vLLM attention

## Test Plan

```
# example, should be a test
python examples/offline_inference/custom_model_with_megatron.py
```
  - Full transformer using Megatron-LM&#x27;s ColumnParallelLinear + RowParallelLinear for MLPs
  - TrainableFlashAttention for attention (with KV cache + backward pass)
  - Ground truth validation against independent PyTorch run
  - Tests TP=4, dynamic batching, train

...</description>
  </item>
  <item>
    <title>[Bug]: rocm crash AMD Ryzen AI 9 HX PRO 370 w/ Radeon 890M - docker/podman</title>
    <link>https://github.com/vllm-project/vllm/issues/28460</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28460</guid>
    <pubDate>Tue, 11 Nov 2025 10:16:32 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0
Clang version                : 20.0.0git (https://github.com/RadeonOpenCompute/llvm-project roc-7.0.0 25314 f4087f6b428f0e6f575ebac8a8a724dab123d06e)
CMake version                : version 3.31.6
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0a0+git1c57644
Is debug build               : False
CUDA used to build PyTorc

...</description>
  </item>
  <item>
    <title>Fix/responses api harmony channel metadata    #28262</title>
    <link>https://github.com/vllm-project/vllm/pull/28355</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28355</guid>
    <pubDate>Sun, 09 Nov 2025 00:48:27 +0000</pubDate>
    <description> 
 
## Purpose
Fix issue #28262: Restore missing channel metadata when converting Responses API output items back to Harmony Messages for multi-turn conversations.
Changes:
Set channel=&#x27;commentary&#x27; for function_call_output type inputs
Set channel=&#x27;analysis&#x27; or &#x27;commentary&#x27; for reasoning type based on the following message (commentary if followed by function_call, analysis otherwise)
Add test to verify channel metadata is correctly preserved across conversation turns
Update parse_response_input() to accept optional next_msg parameter for context-aware channel assignmen
## Test Plan
pytest tests/entrypoints/openai/test_response_api_with_harmony.py::test_function_call_with_previous_input_messages -v
## Test Result
pass 
---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description

...</description>
  </item>
  <item>
    <title>[Bugfix] Parse gpt-oss refusals w/ newer openai-harmony</title>
    <link>https://github.com/vllm-project/vllm/pull/28303</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28303</guid>
    <pubDate>Fri, 07 Nov 2025 15:35:05 +0000</pubDate>
    <description>## Purpose

The output generated by gpt-oss models does not always strictly follow its expected harmony chat template format. This commonly - but not exclusively - happens when gpt-oss-120b generates refusals for content that violates its built-in safety guidelines.

To fix this, a non-strict mode was added to the openai-harmony library to allow attempted recovery of malformed message headers in the model output, such as a missing `&lt;|message|&gt;` special token before the assistant text.

This will resolve some cases where the error
`openai_harmony.HarmonyError: unexpected tokens remaining in message header` was previously thrown. It will not resolve all of those, as not every malformed message output can be recovered. Other ongoing work around using structured output for the Harmony format c

...</description>
  </item>
  <item>
    <title>[Frontend] [gpt-oss] Chat format GD for tool calling with gptoss</title>
    <link>https://github.com/vllm-project/vllm/pull/28148</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/28148</guid>
    <pubDate>Wed, 05 Nov 2025 18:17:40 +0000</pubDate>
    <description>## Purpose
Building on top of the initial gpt-oss reasoning parser in #25515, this PR fleshes out the full structural tags schema to guide the chat format for gpt-oss.

It is only added to the Responses API path for now, but it should be able to fix issues like the ones mentioned in #24954 without needing to make adjustments to Harmony.

Guided decoding for the gpt-oss chat format should only be enabled if `structured_outputs_config.enable_in_reasoning` is True, since the chat format is technically in the reasoning section of model output. It also lets us continue to use structured outputs for the content of the final message. `structured_outputs_config.reasoning_parser` is set by default for gpt-oss. 

TODO: test that structured output works alongside this after #28000 is merged, and adju

...</description>
  </item>
  <item>
    <title>[Quantization] quantized kv cache loading for models like qwen</title>
    <link>https://github.com/vllm-project/vllm/pull/27980</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27980</guid>
    <pubDate>Mon, 03 Nov 2025 12:03:40 +0000</pubDate>
    <description>## Purpose
This PR aims to fix quantized kv cache model loading issues using AMD-Quark. 
The fix was initially applied to the `gpt-oss` model and may be extended to other models in `vllm/model_executor/models`, as they share a nearly identical methodology.

Here is the original error traceback w/o this PR fixing.
&lt;img width=&quot;1219&quot; height=&quot;468&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/1ce1b92e-04a9-404a-91da-f06151cd54c9&quot; /&gt;

##
Here is a check list of some models defined in `vllm/model_executor/models/` for the supportness of quantized kv cache loading. Models with `get_cache_scale` called is marked as ✅, while models without `get_cache_scale` called is marked as ⚠️.

The ⚠️ symbol is preferred over ❌ since some models can intrinsically have non-quantized KV cache, for i

...</description>
  </item>
  <item>
    <title>[Bugfix] Handle escaped characters in GLM tool parser to prevent double serialization</title>
    <link>https://github.com/vllm-project/vllm/pull/27970</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27970</guid>
    <pubDate>Mon, 03 Nov 2025 08:37:00 +0000</pubDate>
    <description>## Purpose

This PR fixes a bug where the tool call parser fails when the model&#x27;s output contains literal escaped characters, such as `\n`, `\&quot;`.

**The Problem:**
When GLM-4 outputs tool calls, the XML may contain &#x27;\n&#x27; literal characters and the JSON content within `&lt;arg_value&gt;` tags may contain escaped characters like `\&quot;`, causing regex matching and json parsing both failed. For example:

```
&lt;tool_call&gt;todo_write\n&lt;arg_key&gt;todos&lt;/arg_key&gt;\n&lt;arg_value&gt;[{\&quot;id\&quot;: \&quot;1\&quot;, \&quot;task\&quot;: \&quot;检查后端代码中的硬编码问题\&quot;, \&quot;status\&quot;: \&quot;in_progress\&quot;}, {\&quot;id\&quot;: \&quot;2\&quot;, \&quot;task\&quot;: \&quot;检查前端代码中的硬编码问题\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;3\&quot;, \&quot;task\&quot;: \&quot;检查违反单一职责的代码\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}, {\&quot;id\&quot;: \&quot;4\&quot;, \&quot;task\&quot;: \&quot;创建整改建议报告\&quot;, \&quot;status\&quot;: \&quot;pending\&quot;}]&lt;/arg_value&gt;
&lt;/tool_call&gt;
```

The current regex fails to ma

...</description>
  </item>
  <item>
    <title>[Metrics] [KVConnector] Add Offloading Connector metrics</title>
    <link>https://github.com/vllm-project/vllm/pull/27942</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27942</guid>
    <pubDate>Sun, 02 Nov 2025 15:36:12 +0000</pubDate>
    <description>Added queries and hits metrics for the Offloading Connector.
Also added timing metrics for store and load operations, which take the average time it takes to load/store, per-token.
The metrics are available from Prometheus and from the StatLogger.
## Purpose
Allows collection of timing metrics for the Offloading Connector, which is essential for future development.
@orozery please review.
## Test Plan

## Test Result
</description>
  </item>
  <item>
    <title>[WIP] [GPT-OSS] customized symm_mem based EP comm kernel integration</title>
    <link>https://github.com/vllm-project/vllm/pull/27495</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27495</guid>
    <pubDate>Sat, 25 Oct 2025 01:04:50 +0000</pubDate>
    <description>Integrates Triton MoE communication kernels

achieved 60% throughput increase compared to #[24588](https://github.com/vllm-project/vllm/pull/24588)

&lt;img width=&quot;3918&quot; height=&quot;1996&quot; alt=&quot;36117&quot; src=&quot;https://github.com/user-attachments/assets/ad93165c-6869-4a01-8c5b-836f4421f656&quot; /&gt;
</description>
  </item>
  <item>
    <title>[CI/Build] Add aime25 eval to gpt-oss CI</title>
    <link>https://github.com/vllm-project/vllm/pull/27398</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27398</guid>
    <pubDate>Thu, 23 Oct 2025 07:38:49 +0000</pubDate>
    <description>## Purpose
This PR added eval config for gpt-oss-120b on H100. 

## Test Plan
pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-gpt-oss.txt --tp-size=1

## Test Result

1 passed, 6 warnings in 455.90s (0:07:35) 

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the releas

...</description>
  </item>
  <item>
    <title>[Bug]: gptoss calls built-in tool when no tools are given</title>
    <link>https://github.com/vllm-project/vllm/issues/27385</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27385</guid>
    <pubDate>Thu, 23 Oct 2025 04:01:36 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
==============================
        System Info
==============================
OS                           : Ubuntu 20.04.6 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-2ubuntu1~20.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 3.29.2
Libc version                 : glibc-2.31

==============================
       PyTorch Info
==============================
PyTorch version              : 2.8.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
==============================
Python version

...</description>
  </item>
  <item>
    <title>[Bugfix] Actually enable serialize_messages for harmony Responses (related to #26185)</title>
    <link>https://github.com/vllm-project/vllm/pull/27377</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27377</guid>
    <pubDate>Thu, 23 Oct 2025 01:07:10 +0000</pubDate>
    <description>## Purpose

For the OpenAI-compatible `v1/responses` route, enable raw messages to be sent when `enable_response_messages` is set to True in `extra_body`.

Previously, the responses are empty because of an issue in openai/harmony. (https://github.com/openai/harmony/issues/78)

https://github.com/vllm-project/vllm/pull/26185 implements most of the fix, but these aren&#x27;t actually invoked, at least not when serving the model through the `vllm serve`. The reason is that the said PR specifies `when_used=&quot;json&quot;`. Thus, this serialization method is ignored because of the use of `model_dump()` in [vllm/entrypoints/openai/api_server.py#L527-L529](https://github.com/vllm-project/vllm/blob/a0003b56b0b822c52bb0f3035c164370a802e6f5/vllm/entrypoints/openai/api_server.py#L527-L529).

The fix is to trigger

...</description>
  </item>
  <item>
    <title>[Frontend][gpt-oss] Allow system message to overwrite model identity</title>
    <link>https://github.com/vllm-project/vllm/pull/27310</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27310</guid>
    <pubDate>Wed, 22 Oct 2025 00:57:44 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose
The current way of implementation forces the system prompt to go into the developer message and it confuses the usage of different roles. We should have developer into developer message, and allow system message to override the system prompt as supported by signature of harmony_utils.py

## Test Result
Override
            &quot;openai_harmony_message&quot;: {
                &quot;author&quot;: {
                    &quot;role&quot;: &quot;system&quot;
                },
                &quot;content&quot;: [
                    {
                        &quot;system_content&quot;: {
                            &quot;type&quot;: &quot;system_content&quot;,
                            &quot;model_identity&quot;: &quot;You are a helpful agent reponds in Chinese.&quot;,
                            &quot;reasoning_effort&quot;: &quot;high&quot;,
                       

...</description>
  </item>
  <item>
    <title>Support PP on tpu_inference</title>
    <link>https://github.com/vllm-project/vllm/pull/27296</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27296</guid>
    <pubDate>Tue, 21 Oct 2025 21:58:37 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>Experimental attention backend in helion</title>
    <link>https://github.com/vllm-project/vllm/pull/27293</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27293</guid>
    <pubDate>Tue, 21 Oct 2025 20:10:50 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

very experimental and draft PR so far

## Test Plan

```
VLLM_ATTENTION_BAKCEND=EXPERIMENTAL_HELION_ATTN vllm serve meta-llama/Llama-3.1-8B-Instruct 
```
## Test Result

Performance see upcoming publications. 
Correctness:
```
VLLM_ATTENTION_BAKCEND=EXPERIMENTAL_HELION_ATTN lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks gsm8k --num_fewshot 5 --batch_size auto --limit 500
```
results in 
```
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.798|±  |0.0180|
|     |       |strict-match    |     5|exact_match|↑  |0.780|±  |0.0185|
```

---
&lt;details&gt;
&lt;summary&gt; Ess

...</description>
  </item>
  <item>
    <title>Add process pool support fro tokenizer</title>
    <link>https://github.com/vllm-project/vllm/pull/27180</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27180</guid>
    <pubDate>Mon, 20 Oct 2025 03:02:35 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;


## Purpose
The preprocessing stage experiences a bottleneck under high concurrency, resulting in extremely long preprocessing times. By introducing additional processes to handle tokenization, this bottleneck can be alleviated.

Related PR: #25301


## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Rel

...</description>
  </item>
  <item>
    <title>[GPT-OSS] Persistent Masked Activation Kernel</title>
    <link>https://github.com/vllm-project/vllm/pull/27100</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27100</guid>
    <pubDate>Fri, 17 Oct 2025 13:02:02 +0000</pubDate>
    <description>
## Purpose

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary documentation update, such as updating `supported_models.md` and `examples` for a new model.
- [ ] (Optional) Release notes update. If your change is user facing, please update the release notes draft in the [Google Doc](https://docs.google.com/document/d/1YyVqrgX4gHTtrstbq8oWUImOyPCKSGnJ7xtTpmXzlRs/edit?tab=t.0).
&lt;/details&gt;

</description>
  </item>
  <item>
    <title>[Bug] Pass correct ep_rank_start and ep_rank_end</title>
    <link>https://github.com/vllm-project/vllm/pull/27025</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/27025</guid>
    <pubDate>Thu, 16 Oct 2025 13:30:25 +0000</pubDate>
    <description>## Purpose
- _load_weights_others expect the parameters as ep_rank_start and ep_rank_end but its passed in the reverse order
- Fix to pass the correct parameters

## Test Plan
Tested with gpt-oss and enabling the expert parallelism

## Test Result
Earlier gpt-oss was failing when expert parallelism was enabled, after the fix it started passing
</description>
  </item>
  <item>
    <title>[Frontend][gpt-oss] Support all MCP servers for gpt-oss</title>
    <link>https://github.com/vllm-project/vllm/pull/26704</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/26704</guid>
    <pubDate>Mon, 13 Oct 2025 12:51:59 +0000</pubDate>
    <description>## Purpose
This PR is an implementation of #26703 

Note: This PR is really big, but there was a lot that had to be cleaned up at the same time. A large amount of the code is from adding tests to existing code and a MCP server to test with and new MCP tests.

There are three main motivations:
- vLLM should support integrating with all MCP servers
  - TODO: Test with enterprise ones like Github
- Clean up gpt-oss tool specific code paths in vLLM
  - Tools and models change, but MCP is a protocol meant to support all models
  - Sets the stage for full MCP integration for all tool calling models
- Avoid losing information due to lossy OpenAI types for output
  - An example below

```
# What you can return to the user on the ResponsesAPI for this tool call with the OpenAI types.
class ActionOp

...</description>
  </item>
  <item>
    <title>fix json schema alias serializing when streaming</title>
    <link>https://github.com/vllm-project/vllm/pull/26356</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/26356</guid>
    <pubDate>Tue, 07 Oct 2025 14:12:21 +0000</pubDate>
    <description>## Purpose

Fix a serialization bug in streaming responses where Pydantic field aliases (e.g. `schema` → `schema_`) were not preserved during `.model_dump()` calls.

This caused the `&quot;schema_&quot;` key to appear instead of `&quot;schema&quot;` in streamed response events for JSON schema output formats, breaking compatibility with the OpenAI SDK’s `ResponseFormatTextJSONSchemaConfig` parsing.

**Related issue:** [vllm-project/vllm#26288](https://github.com/vllm-project/vllm/issues/26288)

### Root Cause
- `ResponsesResponse.from_request(...).model_dump()` was called without `by_alias=True` at:
  - `vllm/entrypoints/openai/serving_responses.py:1830`
  - `vllm/entrypoints/openai/serving_responses.py:1879`
- Without `by_alias=True`, Pydantic outputs internal field names (e.g. `schema_`) instead of their ali

...</description>
  </item>
</channel>
</rss>
