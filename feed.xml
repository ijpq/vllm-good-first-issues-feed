<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: good first issue</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'good first issue' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Sun, 11 Jan 2026 19:51:18 +0000</lastBuildDate>
  <item>
    <title>[Bug]: ModelOpt Llama-4 Checkpoints Take 5+ minutes to load</title>
    <link>https://github.com/vllm-project/vllm/issues/31624</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/31624</guid>
    <pubDate>Fri, 02 Jan 2026 15:18:14 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

In working on some MoE refactors, I discovered that L4 for ModelOpt takes 5+minutes to load weights even from CPU page cache. 
- https://huggingface.co/nvidia/Llama-4-Scout-17B-16E-Instruct-FP8

The root cause is basically this hack logic to load the state dict that ModelOpt uses
- https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llama4.py#L439-L523 [modelopt is the fused case] 

What happens is that the CPU tensor (loaded weight) that we are going to load into the GPU tensor (param) becomes non-contiguous due to this logic. As a result, when we eventually call `_copy()` from CPU-&gt;GPU we are calling this on a non-contiguous cpu tensor which takes 3-4s per weight.

To hack around this for local R&amp;D, I simply immediately move 

...</description>
  </item>
  <item>
    <title>[Feature][Cleanup]: Unify `vllm.utils.flashinfer` and `vllm.model_executor.layers.quantization.utils.flashinfer_utils`</title>
    <link>https://github.com/vllm-project/vllm/issues/31414</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/31414</guid>
    <pubDate>Sat, 27 Dec 2025 18:27:00 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

its confusing to have both

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[RFC]: Improve environment variable declaration and handling</title>
    <link>https://github.com/vllm-project/vllm/issues/31249</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/31249</guid>
    <pubDate>Wed, 24 Dec 2025 00:05:13 +0000</pubDate>
    <description>### Motivation.

Currently, all environment variables are declared twice: the type definition and the custom getter lambda function (shown below). Due to the sheer amount of env vars, it is difficult to keep these in sync, often leading to incorrect type declarations and defaults. Documentation is usually in the form of comments near the getter instead of a docstring on the type declaration, making it unavailable to the IDE. Finally, apart from strings, most variables are ints, bools, or paths, reimplementing custom parser logic. While #25700 argues for limiting the use of envvars (which I agree with), we should still handle the env vars we do have in a robust way.

&lt;details&gt;&lt;summary&gt; Current structure of &lt;code&gt;envs.py&lt;/code&gt; &lt;/summary&gt;

```
if TYPE_CHECKING:
    VLLM_HOST_IP: str = &quot;&quot;
   

...</description>
  </item>
  <item>
    <title>[Feature]: Integrate Sonic MoE</title>
    <link>https://github.com/vllm-project/vllm/issues/31039</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/31039</guid>
    <pubDate>Fri, 19 Dec 2025 17:29:59 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

https://x.com/wentaoguo7/status/2001773245318541324?s=46&amp;t=jLcDgQXDbYe6HgFmTNYgpg
https://github.com/Dao-AILab/sonic-moe

Curious to see benchmarks!

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Bug]: Qwen3-VL 2:4 sparsity llm-compressor RuntimeError: shape mismatch (0.12, 0.13rc2)</title>
    <link>https://github.com/vllm-project/vllm/issues/31019</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/31019</guid>
    <pubDate>Fri, 19 Dec 2025 09:18:00 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 24.04.3 LTS (x86_64)
GCC version                  : (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0
Clang version                : Could not collect
CMake version                : Could not collect
Libc version                 : glibc-2.39

==============================
       PyTorch Info
==============================
PyTorch version              : 2.9.0+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
====

...</description>
  </item>
  <item>
    <title>[Feature][Attention][UX]: Incorporate Features into Attention Selection</title>
    <link>https://github.com/vllm-project/vllm/issues/30654</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/30654</guid>
    <pubDate>Sun, 14 Dec 2025 18:04:14 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

SUMMARY:
* we have default attention backends by priority and a notion of which backend supports what hw
* however, certain features are not considered in this (e.g. fp8 kv cache, e.g. attention sinks)

Recent example, we had test failures because we updated the logic to load kv cache quantization from the model config. But since CUTLASS_MLA is the default backend on B200, we started seeing test failures (since CUTLASS MLA does not support fp8 kv cache) because we were not automatically falling back to FLASHINFER_MLA (which does)


So the proposal is to:
- make sure all attention backends report what features are supported
- update the attention selector to consider these features in the selection

### Alternatives

_No response_

### Additional con

...</description>
  </item>
  <item>
    <title>[Feature]: Remove MXFP4 Logic From `fused_experts`</title>
    <link>https://github.com/vllm-project/vllm/issues/30621</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/30621</guid>
    <pubDate>Sat, 13 Dec 2025 18:30:30 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

SUMMARY:
* as part of effort to refactor MoE, trying to reduce cruft
* we currently only have MX emulation in vLLM
* the logic for this emulation should be moved into quark

https://github.com/vllm-project/vllm/blame/main/vllm/model_executor/layers/fused_moe/fused_moe.py#L1866-L1899

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Feature]: Remove Chunking From FusedMoE</title>
    <link>https://github.com/vllm-project/vllm/issues/30620</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/30620</guid>
    <pubDate>Sat, 13 Dec 2025 18:22:30 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

* we have some chunking logic in the triton kernels to avoid IMA: https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py#L1807
* we chunk in ~65k tokens
* this case does not happen anymore because of chunked prefill

We should remove this

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Feature]: Default eplb num_redundant_experts to the lowest valid value if unspecified</title>
    <link>https://github.com/vllm-project/vllm/issues/30075</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/30075</guid>
    <pubDate>Thu, 04 Dec 2025 18:19:03 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

EPLB requires the number of experts to be chosen up front and there is a known minimum valid value that can be derived from the vllm startup configuration.  Since extra EPLB experts trades kv cache memory for potential performance improvements, but that is not guaranteed to pay off, having the EPLB value default to the minimum valid value would reduce friction on enabling EPLB the first time until users are ready to tune.

As a consequence, it would also streamline templating the same config to work across multiple EP sizes for the default case.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bo

...</description>
  </item>
  <item>
    <title>[Feature]: Qwen3 Omni Transcriptions</title>
    <link>https://github.com/vllm-project/vllm/issues/29405</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/29405</guid>
    <pubDate>Tue, 25 Nov 2025 12:35:16 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Would love to see this model included in the supported as a Transcription model. According to the docs it looks like only 4 different models are supported as of now. 

https://docs.vllm.ai/en/latest/models/supported_models/#transcription

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Feature]: Optimize collectives in TP MoE case using torch.compile pass</title>
    <link>https://github.com/vllm-project/vllm/issues/29139</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/29139</guid>
    <pubDate>Fri, 21 Nov 2025 01:36:06 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

To avoid redundant work in MoE models in the TP case, sequence parallelism was added to the Deepseek model definition in #24134 and expanded to other models in #24982. However, to avoid performing surgery on the linear layer, the current approach performs more communication than necessary. With a torch.compile custom pass, we can rewrite the graph to remove the redundant computation.

### More details

Before the SP optimization, the ops in the model were:
```
- o_proj:[num_tokens, ...] -&gt; [num_tokens, ...] (incomplete results)
- all_reduce:[num_tokens, ...] -&gt; [num_tokens, ...]
- router:[num_tokens, ...] -&gt; [num_tokens, ...]
- experts:[num_tokens, ...] -&gt; [num_tokens, ...]
- ...
```

With sequence parallel enabled, this becomes:
```
- o_proj: [num_

...</description>
  </item>
  <item>
    <title>[Feature]: Disable logging `/metrics`</title>
    <link>https://github.com/vllm-project/vllm/issues/29023</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/29023</guid>
    <pubDate>Wed, 19 Nov 2025 18:25:48 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

- IGW hits `/metrics` continuously to understand the current load on the system
- This leads to an overload of logs
- We can disable this with `--disable-uvicorn-access-log`, but lose access to all access logs

We should have `--disable-uvicorn-metrics-access-log` to avoid logging * just * metrics. Per Gemini, we can do this with something like:

```python
# Define the routes for which access logs should be disabled
EXCLUDE_PATHS = [&quot;/health&quot;, &quot;/metrics&quot;]

class EndpointFilter(logging.Filter):
    def filter(self, record: logging.LogRecord) -&gt; bool:
        # Check if the log record contains arguments and if the path matches an excluded path
        if record.args and len(record.args) &gt;= 3:
            path = record.args[2]  # The path is typically 

...</description>
  </item>
  <item>
    <title>[Feature]: Fused Kernel for GPT-OSS Router</title>
    <link>https://github.com/vllm-project/vllm/issues/28986</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28986</guid>
    <pubDate>Wed, 19 Nov 2025 03:18:25 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

&lt;img width=&quot;1257&quot; height=&quot;250&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/31eba061-522c-4521-b0a9-9f25bb36c3df&quot; /&gt;

- Right now, we spend ~3.5% of the layer in the expert selection
- The operation is unfused

Write a fused kernel like we have for deepseek grouped_topk

### Alternatives

- torch compile
- triton
- cuda

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Bug][RL]: Port Conflict</title>
    <link>https://github.com/vllm-project/vllm/issues/28498</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28498</guid>
    <pubDate>Tue, 11 Nov 2025 22:51:35 +0000</pubDate>
    <description>### Your current environment

- bug report:

```
Hello vLLM team, I&#x27;m running into a suspicious ZMQ socket bug with my 2P 4D configuration for DeepSeek-V3 (see below). I thought it is caused by reusing same nodes for many vLLM launches, but now it happened also at a clean node. Seems like a DP bug of sorts. Please find logs attached. vllm==0.11.0.
```

```bash
[1;36m(APIServer pid=670293)[0;0m   File &quot;XXX/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py&quot;, line 134, in __init__
[1;36m(APIServer pid=670293)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[1;36m(APIServer pid=670293)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(APIServer pid=670293)[0;0m   File &quot;XXX/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py&quot;,

...</description>
  </item>
  <item>
    <title>[Feature]: Improve DCP error messages</title>
    <link>https://github.com/vllm-project/vllm/issues/28407</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28407</guid>
    <pubDate>Mon, 10 Nov 2025 17:46:39 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Currently if a backend doesn&#x27;t support DCP we get the following error message 

```
AssertionError: DCP requires attention impls to return the softmax lse for decode, but the impl FlashInferImpl does not return the softmax lse for decode.
```

It would good to suggest to the user to try an alternative backend using `VLLM_ATTENTION_BACKEND`

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Feature]: Implement naive prepare/finalize class to replace naive dispatching in fused_moe/layer.py</title>
    <link>https://github.com/vllm-project/vllm/issues/28236</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28236</guid>
    <pubDate>Thu, 06 Nov 2025 18:38:38 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

The `FusedMoE` layer has a special case dispatch/combine for EP+DP when there is no specific all2all backend specified.  This makes the code in `layer.py` a bit confusing and hard to follow.  One way to simplify this is to implement a proper `FusedMoEPrepareAndFinalize` subclass for naive dispatch/combine.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Feature]: Support norm+quant &amp; silu+quant fusion for block (group) quantization</title>
    <link>https://github.com/vllm-project/vllm/issues/27847</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27847</guid>
    <pubDate>Thu, 30 Oct 2025 23:21:59 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Currently, we use the CUDA quant kernel for group quant due to performance issues of the H100 kernel. However, that means that it cannot be automatically fused with preceeding RMSNorm and SiluMul.

We should add support for group quant to RMSNorm+quant and SiluMul+quant fusion passes, as well as custom fused CUDA kernels to work around the issue for now and have an alternative when Inductor-generated code is slower.

- [x] RMS + block quant fusion
- [ ] SiluMul + block quant fusion

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.a

...</description>
  </item>
  <item>
    <title>[Feature]: Batch Invariant Feature and Performance Optimization</title>
    <link>https://github.com/vllm-project/vllm/issues/27433</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27433</guid>
    <pubDate>Thu, 23 Oct 2025 19:38:03 +0000</pubDate>
    <description>## ðŸš€ The feature, motivation and pitch

We have basically support Batch Invariant based on https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/

https://github.com/orgs/vllm-project/projects/29/views/1

But there are still some work to be done, so here is the issue to track the work

## TODOs:

- [x] Basic framework https://github.com/vllm-project/vllm/pull/25603 @bwasti 
- [x] Flashinfer support https://github.com/vllm-project/vllm/pull/26373 @bwasti 
- [x] Deepseek-v3 https://github.com/vllm-project/vllm/pull/26609 @bwasti 
- [x] DeepGEMM on Blackwell https://github.com/vllm-project/vllm/pull/27127  @yewentao256 
- [x] Batch Invariant for R1 TP 8 on Blackwell https://github.com/vllm-project/vllm/pull/27229 @yewentao256 
- [x] Torch compile &amp; Cuda Graph support  htt

...</description>
  </item>
  <item>
    <title>[Usage]: how to request a qwen2.5-VL-7B classify model served by vllm using openai SDK?</title>
    <link>https://github.com/vllm-project/vllm/issues/27413</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/27413</guid>
    <pubDate>Thu, 23 Oct 2025 12:32:25 +0000</pubDate>
    <description>### Your current environment

```text
The output of `python collect_env.py`
```


### How would you like to use vllm

I launch a server with the following command to serving a Qwen2.5-VL-7B model finetued for seqence classification. (this model replaced the lm_head with a 2 classes score_head)

The launch command is :
```
vllm serve --model=//video_classification/qwenvl_7b_video_cls/v5-20251011-121851/2340_vllm_format --served_model_name Qwen2.5-7B-shenhe --task=classify --port=8080 --tensor-parallel-size=2
```

I don&#x27;t know how to request the server with the openAI sdk.
I use the code snnipet showed below which works well with pure text, but it got 400 bad request when I put the video url into the prompt

this works well:
```
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText:

...</description>
  </item>
  <item>
    <title>[Feature]: Better base64 to torch tenser</title>
    <link>https://github.com/vllm-project/vllm/issues/26781</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/26781</guid>
    <pubDate>Tue, 14 Oct 2025 08:00:48 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Support float32, float16, bfloat16, fp8_e4m3, fp8_e5m2 embed dtype in #26414


The following line of code will raise annoying UserWarning
```
torch.frombuffer(
    base64.b64decode(data[&quot;embedding&quot;]), dtype=torch_dtype
).to(torch.float32)
```

UserWarning:
```
examples/online_serving/pooling/embedding_embed_dtype_client.py:49: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.

...</description>
  </item>
  <item>
    <title>[Feature]: Improve config validation using Pydantic</title>
    <link>https://github.com/vllm-project/vllm/issues/26366</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/26366</guid>
    <pubDate>Tue, 07 Oct 2025 16:55:37 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

All the configs in https://github.com/vllm-project/vllm/tree/main/vllm/config are decorated with `pydantic.dataclass`. However, many of them include manual validation in `__post_init__`.

This issue is a call for help to improve the configs in the following ways:

- Use [`pydantic.Field`](https://docs.pydantic.dev/latest/api/fields/#pydantic.fields.Field) to apply simple constraints, such as `gt=0`, directly to the default values of the dataclass fields ([usage docs](https://docs.pydantic.dev/latest/concepts/fields/))
- Use [`pydantic.field_validator`](https://docs.pydantic.dev/latest/api/functional_validators/#pydantic.functional_validators.field_validator) to apply more complex field validation to individual fields. ([usage docs](https://docs.pyda

...</description>
  </item>
  <item>
    <title>[RFC]: Limit the use of envvars in vLLM</title>
    <link>https://github.com/vllm-project/vllm/issues/25700</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/25700</guid>
    <pubDate>Thu, 25 Sep 2025 18:13:19 +0000</pubDate>
    <description>### Motivation.

vLLM&#x27;s envvars are on the edge of getting out of control. There are many envvars should instead be configs. Like the attention backend, all2all kernel backend, and even there is a flag to control KV cache layout. envvars are evil because:

1. It is equivalent of using global variables everywhere in the code, which is a bad programming practice.
2. envvars have no advanced structure like, hierarchy, typechecks, etc. 

In summary, I think envvars are the kind of thing that is very easy to add so people tend to add envvars as the shortest path to implement their feature, but this can quickly becomes unmanageable and makes the project very hard to use. 

### Proposed Change.

I think we should:

1. Spend some effort on reviewing the current envvars and move many of them to con

...</description>
  </item>
  <item>
    <title>[Feature]: automatically use pre-compiled wheels when installed from github</title>
    <link>https://github.com/vllm-project/vllm/issues/24706</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/24706</guid>
    <pubDate>Fri, 12 Sep 2025 01:06:34 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

When people use `pip install git+https://github.com/vllm-project/vllm.git` , it&#x27;s safe to turn on pre-compiled wheels.

Many people still use this way, and right now it goes through full compilation.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Bug]: `vllm bench serve` isn&#x27;t an exact replacement of benchmark_serving.py</title>
    <link>https://github.com/vllm-project/vllm/issues/24684</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/24684</guid>
    <pubDate>Thu, 11 Sep 2025 18:21:00 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 4.1.0
Libc version                 : glibc-2.35

==============================
       PyTorch Info
==============================
PyTorch version              : 2.7.1+cu128
Is debug build               : False
CUDA used to build PyTorch   : 12.8
ROCM used to build PyTorch   : N/A

==============================
      Python Environment
========

...</description>
  </item>
  <item>
    <title>[Feature]: Assign worker process titles and logging prefix earlier</title>
    <link>https://github.com/vllm-project/vllm/issues/24476</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/24476</guid>
    <pubDate>Tue, 09 Sep 2025 04:10:23 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

In #22205, we enhanced worker process title by fetching parallelism information from `vllm.distributed.parallel_state`. Worker process title and logging prefix are critical for debugging purpose. Assigning them earlier may be helpful for triaging issues.
Explore whether this is reasonably achievable and implement it if yes.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Bug]: Hangs on mulit-node setup when len(prompt) &gt; max_model_len</title>
    <link>https://github.com/vllm-project/vllm/issues/24454</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/24454</guid>
    <pubDate>Mon, 08 Sep 2025 17:50:25 +0000</pubDate>
    <description>### Your current environment

&lt;details&gt;
&lt;summary&gt;The output of &lt;code&gt;python collect_env.py&lt;/code&gt;&lt;/summary&gt;

```text
Collecting environment information...
/traindata/eugen/lotus/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can&#x27;t initialize NVML
  warnings.warn(&quot;Can&#x27;t initialize NVML&quot;)
/traindata/eugen/lotus/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can&#x27;t initialize NVML
  warnings.warn(&quot;Can&#x27;t initialize NVML&quot;)
==============================
        System Info
==============================
OS                           : Ubuntu 22.04.5 LTS (x86_64)
GCC version                  : (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version                : Could not collect
CMake version                : version 4.0.2
Libc version     

...</description>
  </item>
  <item>
    <title>[Feature]: Support `Phi4Flash` model in V1</title>
    <link>https://github.com/vllm-project/vllm/issues/23957</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/23957</guid>
    <pubDate>Fri, 29 Aug 2025 18:31:01 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

We will drop support for V0 in the very near future, and if we want to continue supporting this model in vLLM, it needs to be ported to V1.

The main work here is in:
1. Adapting the custom Mamba layer to match how V1 manages the mamba state. The MambaMixer, MambaMixer2, MinimaxLinearAttention and ShortConv can all be used as a reference for how to do this.
2. Porting the differential attention backend to V1. 

### Alternatives

Drop model support in vLLM

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked que

...</description>
  </item>
  <item>
    <title>[Feature]: GPT-OSS harmony format support</title>
    <link>https://github.com/vllm-project/vllm/issues/23217</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/23217</guid>
    <pubDate>Wed, 20 Aug 2025 00:12:15 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

From the view of API server, GPT-OSS introduces the following features:
* Builtin tool call: tool calls that happens inside chain of thought. It is different from most existing models where tool call only exists in the output to users.
* Harmony: a new text format to represent the chain of thought, tool calls, etc. vLLM needs to implement the parsing between `OpenAI API &lt;-&gt; harmony &lt;-&gt; model input/output tokens`

vLLM has basic support of the above features on response API now. But as shown in https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#harmony-format-support , the response API with streaming, and chat completion is still in an early stage. And help wanted on completing these features!

### Alternatives

_No response_

### Ad

...</description>
  </item>
  <item>
    <title>[Feature]: consider all env vars in compilation hash with some opt-out</title>
    <link>https://github.com/vllm-project/vllm/issues/23107</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/23107</guid>
    <pubDate>Mon, 18 Aug 2025 14:19:50 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Follow-up of https://github.com/vllm-project/vllm/pull/22449 and https://github.com/vllm-project/vllm/issues/16501 , we can take all env vars into consideration, and use an opt-out list that we don&#x27;t consider in the hash computation.

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[RFC]: Refactor CI/CD</title>
    <link>https://github.com/vllm-project/vllm/issues/22992</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/22992</guid>
    <pubDate>Fri, 15 Aug 2025 16:26:44 +0000</pubDate>
    <description>### Motivation.

vLLM&#x27;s CI/CD has grown in a less than ideal way as it has built up over the years.

We have the following problems:
- CI takes very long, especially on a per commit cycle
- CI has failures that cannot be reproduced on every machine due to numerics
- CI has failures on models that are not the 80-20 of our usage --- which runs per commit
- CI failures in early tests often lead to vLLM not cleaning up properly --- which creates failures across many tests that makes it hard to identify what is wrong
- CI is NOT covering the models that actually matter (since not enough GPU memory) or hardware that actually matters to the majority of our users
- CI is NOT covering performance!

These issues are creating a bad developer experience for vLLM and causes issues like the CI &quot;death sp

...</description>
  </item>
  <item>
    <title>[Feature]: Tune Triton Configs for Qwen3-30B-A3-Fp8 and Bf16</title>
    <link>https://github.com/vllm-project/vllm/issues/22294</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/22294</guid>
    <pubDate>Wed, 06 Aug 2025 02:20:33 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Hardware Cases:
- H100, H200, B200

Configurations:
- TP=1
- TP=2
- TP=4
- TP=8
- EP=1
- EP=2
- EP=4
- EP=8

Precisions
- Fp8
- Bf16

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Bug]: Add tests for all parallel sampling parameter combinations</title>
    <link>https://github.com/vllm-project/vllm/issues/21948</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/21948</guid>
    <pubDate>Wed, 30 Jul 2025 16:20:13 +0000</pubDate>
    <description>Verify that we have tests that explicitly exercise parallel sampling (`n&gt;1` request sampling parameter) for all of the following combinations:
- Via `AsyncLLM.generate`, via `LLMEngine add_request() / step()`
- For `output_kind` equal to each of `CUMULATIVE`, `DELTA`, `FINAL_ONLY`

Ideally a test for each of `AsyncLLM` and `LLMEngine` in the appropriate files, with `output_kind` parameterized.

Additionally we can still test `LLM.generate()` but this enforces `FINAL_ONLY`. There is already a test for this one [here](https://github.com/vllm-project/vllm/blob/004203e95330ac9a878df8192619570b0770667e/tests/v1/engine/test_llm_engine.py#L120), but I&#x27;m not sure about the other cases.

@sethkimmel3 has reported that the `LLMEngine` + `CUMULATIVE` combination is not working properly, this should b

...</description>
  </item>
  <item>
    <title>[Feature] Skip modules for disabled modalities</title>
    <link>https://github.com/vllm-project/vllm/issues/21943</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/21943</guid>
    <pubDate>Wed, 30 Jul 2025 15:50:17 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Provide a way to skip loading modules by modality.

We can enable this automatically via `--limit-mm-per-prompt`, as discussed in https://github.com/vllm-project/vllm/pull/20868#issuecomment-3071205115. For example, we can use text-only mode for an image-text model by setting `--limit-mm-per-prompt &#x27;{&quot;image&quot;: 0}&#x27;`

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Feature]: Add Triton implementation of NVFP4 GEMM</title>
    <link>https://github.com/vllm-project/vllm/issues/21014</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/21014</guid>
    <pubDate>Tue, 15 Jul 2025 21:33:26 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Currently we only have NVFP4 GEMMs written in CUTLASS for SM100, which means we have no support for SM120. While we still expect tuned CUTLASS kernels to provide the best performance, it would be nice to have a reference Triton implementation available as a fallback if no other kernels are available.

It seems Triton has supported FP4 formats for several months so we should have a new enough version of Triton https://github.com/triton-lang/triton/blob/620237edd282d3fa275e7f931af2018423108c4a/python/test/unit/language/test_matmul.py#L652

A good starting point would be to add the triton kernel directly to https://github.com/vllm-project/vllm/blob/main/benchmarks/kernels/bench_nvfp4_gemm.py and compare the performance to the CUTLASS kernel, before int

...</description>
  </item>
  <item>
    <title>[Feature]: Use `QuantFp8` `CustomOp`-abstraction for MoE layers</title>
    <link>https://github.com/vllm-project/vllm/issues/20711</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/20711</guid>
    <pubDate>Wed, 09 Jul 2025 21:41:36 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

#19830 added `QuantFp8`, which uses the `CustomOp` abstraction to implement fp8 quantization in both CUDA and torch, allowing Inductor to achieve superior performance over the CUDA ops (which are unoptimized and also do not fuse by default). However, the class has to be instantiated during init, and MoE uses are currently in util free functions many levels deep. Those need to be mildly rearchitected to take advantage of the new abstraction.

The use to be rearchitected is here: https://github.com/vllm-project/vllm/blob/c7a00e6e6716f45db09e39cb21a8f91f741f10b9/vllm/model_executor/layers/fused_moe/utils.py#L37-L40

The free functions should be converted to class instances with separate init and forward steps.

### Alternatives

_No response_

### Addi

...</description>
  </item>
  <item>
    <title>[Feature]: Support EPLB for More MoE Models, e.g. Qwen 3, Llama 4</title>
    <link>https://github.com/vllm-project/vllm/issues/20468</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/20468</guid>
    <pubDate>Fri, 04 Jul 2025 05:06:31 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

ðŸŽ‰ **#18343 introduces dynamic Expert Parallelism Load Balancing (EPLB)** for DeepSeek-V2/V3/R1 models.

As MoE (Mixture-of-Experts) models become more common, weâ€™d love help extending EPLB support to other MoE modelsâ€”such as Qwen3, Llama 4, and more.

This is a great **first good issue** for anyone interested in model internals or systems work. #18343 was built with generality in mind, so extending it to other models or quantization methods should be relatively straightforward.

---

### âœ… How to add support for a new model

Implement the `MixtureOfExperts` protocol. Specifically, youâ€™ll need to:

- Expose relevant MoE configuration flags.
- Provide access to expert weights for EPLB to rearrange.
- Forward EPLB-related arguments into the `FusedMoE` 

...</description>
  </item>
  <item>
    <title>[Feature]: `CustomOp` cleanup</title>
    <link>https://github.com/vllm-project/vllm/issues/19817</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/19817</guid>
    <pubDate>Wed, 18 Jun 2025 20:10:27 +0000</pubDate>
    <description>### ðŸš€ Motivation

Currently, we do not have a consistent plan for &quot;light&quot; custom ops (subclasses of `CustomOp` with both torch-native and GPU implementations). As we work on improved performance on NVIDIA Blackwell and AMD, we should be more intentional with CompilationConfig defaults that control custom op dispatching. This is a parent issue that tracks smaller PRs addressing `CustomOp`s.

In vLLM, there are two kinds of custom kernels/ops:
1. &quot;heavy&quot; ops like GEMMs, MoE, and attention, which will mostly use tuned custom kernels for maximum performance.
2. &quot;light&quot; ops like `RMSNorm`, `SiluAndMul`, and `RoPE`, which have both torch-native and custom GPU implementations.

This issue only refers to &quot;light&quot; ops, which are (or should be) all subclasses of `CustomOp`.

When we enabled `torch.co

...</description>
  </item>
  <item>
    <title>[Feature]: Composite model loading using `AutoWeightsLoader` for all models</title>
    <link>https://github.com/vllm-project/vllm/issues/15697</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/15697</guid>
    <pubDate>Fri, 28 Mar 2025 10:39:19 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

#9160 first introduced `AutoWeightsLoader` to recursively call `load_weights` on sub-modules. This lets composite models (most notably multi-modal models) use language backbones (`*Model` classes such as `LlamaModel`) without having to repeat their weight loading logic.

Currently, `load_weights` is only implemented in a few language backbones. It would be great to standardize this approach and apply it to all language backbones in vLLM. The steps to do this are pretty straightforward:

1. Move the existing `load_weights` function from `*ForCausalLM` to `*Model`.
2. Create a new `load_weights` function in `*ForCausalLM` that loads the weights using `AutoWeightsLoader`.
3. Move any logic in `*Model.load_weights` that only applies to `*ForCausalLM` ba

...</description>
  </item>
  <item>
    <title>[Performance]: Update Cascade Attention Heuristics for FA3</title>
    <link>https://github.com/vllm-project/vllm/issues/15647</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/15647</guid>
    <pubDate>Thu, 27 Mar 2025 22:02:43 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Currently, we use a heuristic (https://github.com/vllm-project/vllm/blob/4098b72210dc10761bb348b373bbd0fc9b23b0e4/vllm/v1/attention/backends/flash_attn.py#L331)
to determine whether using cascade attention would improve performance. However, this heuristic was developed prior to the FA3 integration and is therefore optimized only for FA2. The calculation of SM occupancy it uses is no longer accurate for FA3 and needs updating.

Specifically,
1. We need to split the case for FA2 and FA3, since FA2 is still used for certain GPUs.
2. Afaik, FA3 uses different heuristics for GQA packing and split kv than FA2. The heuristics in use_cascade should reflect this difference (although it doesn&#x27;t need to be super accurate).
3. It&#x27;d be nice if we can cite speci

...</description>
  </item>
  <item>
    <title>[Feature]: Audit and Update Examples To Use `VLLM_USE_V1=1`</title>
    <link>https://github.com/vllm-project/vllm/issues/14530</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/14530</guid>
    <pubDate>Mon, 10 Mar 2025 02:36:23 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

Many of the examples leverage V0 internals.

We should:
- raise `NotImplementedError` if `envs.VLLM_USE_V1` with these
- convert them to use `V1` if we can

### Alternatives

_No response_

### Additional context

_No response_

### Before submitting a new issue...

- [x] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.</description>
  </item>
  <item>
    <title>[Misc]: CMake Clean-up / Refactor Tasks</title>
    <link>https://github.com/vllm-project/vllm/issues/9129</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/9129</guid>
    <pubDate>Mon, 07 Oct 2024 16:07:04 +0000</pubDate>
    <description>In an effort to make the CMake more readable, stable and easy to use we have a few tasks we&#x27;d like to work on, creating a GitHub issue here to track that progress, some planned changes/investigations:

- [ ] Rename `define_gpu_extension_target`, currently this is used for CPU extensions too so the name is now misleading
- [ ] Add a CI test of local builds, i.e. `pip install -e .`
- [ ]  Warn that PTX builds are not currently supported (post https://github.com/vllm-project/vllm/pull/8845), currently if there is a `+PTX` in `TORCH_CUDA_ARCH_LIST` this will be ignored. We should warn when this is the case. Alternatively we can add support for PTX builds although this is generally not desirable since PTX increases the wheel size by quite a bit (PTX is larger than SASS), and we already build fo

...</description>
  </item>
</channel>
</rss>
