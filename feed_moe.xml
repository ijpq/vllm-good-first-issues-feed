<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>vLLM issues - label: moe</title>
  <link>https://github.com/vllm-project/vllm/issues</link>
  <description>Open issues with label 'moe' in vllm-project/vllm</description>
  <language>en</language>
  <lastBuildDate>Sun, 14 Dec 2025 14:24:56 +0000</lastBuildDate>
  <item>
    <title>[Perf] Do FP4 quant before All gather on flashinfer trtllmgen MOE </title>
    <link>https://github.com/vllm-project/vllm/pull/30014</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/30014</guid>
    <pubDate>Thu, 04 Dec 2025 01:45:51 +0000</pubDate>
    <description>## Purpose
Move the FP4 quant before All Gather when Flashinfer TRTLLMGEN MOE is used on DP + EP enabled. This reduce the message size in All gather thus speed up All gather kernel time.
Blocked by https://github.com/vllm-project/vllm/pull/29804, will add same opt to flashinfer_trtllm_fp4_routed_moe after merged.
Original size
```
hidden_states (num_tokens, hidden_size), dtype bf16
routing_logits (num_tokens, num_experts), dtype fp32
```
After change
```
hidden_state (num_tokens, hidden_size/2), dtype unit8
hidden_state_sf (num_tokens , hidden_size/16), dtype fp8
routing_logits (num_tokens, num_experts), dtype fp32
```
On DeepSeek-R1 case, it is a **2.4x** less message size

## Test Plan
```
python3 -m vllm.entrypoints.openai.api_server --model nvidia/DeepSeek-R1-0528-FP4-v2 --tokenizer nv

...</description>
  </item>
  <item>
    <title>[Perf][Kernels] Vectorize `csrc/activations_kernels.cu`</title>
    <link>https://github.com/vllm-project/vllm/pull/29512</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29512</guid>
    <pubDate>Wed, 26 Nov 2025 17:38:56 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

We have been using `silu_and_mul`, `swigluoai_and_mul`, etc as unvectorized kernels for essentially all of the fused moe methods, so there are free gains to be had there, especially for large batch sizes. This PR expands all of them to 128 bit vectorized loads.

## Traces

```
vllm serve RedHatAI/Qwen3-30B-A3B-FP8-dynamic
vllm bench serve --model RedHatAI/Qwen3-30B-A3B-FP8-dynamic --num-prompts 10 --random-input-len 512 --random-output-len 10 --profile
```

Profiles below are for the prefill

Before:
&lt;img width=&quot;1353&quot; height=&quot;689&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/d642e8bc-aeba-451e-9f3a-d8f0ae4ea8d2&quot; /&gt;

After:
&lt;img width=&quot;1355&quot; height=&quot;666&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/91d5f750-e28d-4dcd-b

...</description>
  </item>
  <item>
    <title>[Refactor] Split up compressed_tensors_moe.py into separate files per method</title>
    <link>https://github.com/vllm-project/vllm/pull/29427</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/29427</guid>
    <pubDate>Tue, 25 Nov 2025 18:23:03 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

Similar to other quant methods like gptq_marlin.py or fp8.py, I&#x27;d like to colocate the MoE methods for a given quantization config next to its same Linear method. So this PR splits the monolithic compressed_tensors_moe.py such that each class is now in its own file i.e. CompressedTensorsW4A4Nvfp4MoeMethod is now in compressed_tensors_w4a4_nvfp4.py

## Test Plan

## Test Result

---
&lt;details&gt;
&lt;summary&gt; Essential Elements of an Effective PR Description Checklist &lt;/summary&gt;

- [ ] The purpose of the PR, such as &quot;Fix some issue (link existing issues this PR will resolve)&quot;.
- [ ] The test plan, such as providing test command.
- [ ] The test results, such as pasting the results comparison before and after, or e2e results
- [ ] (Optional) The necessary d

...</description>
  </item>
  <item>
    <title>[Feature]: Generalize RoutingMethodType for broader MoE routing control</title>
    <link>https://github.com/vllm-project/vllm/issues/28408</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/issues/28408</guid>
    <pubDate>Mon, 10 Nov 2025 17:52:48 +0000</pubDate>
    <description>### ðŸš€ The feature, motivation and pitch

PR #27492 introduced `RoutingMethodType` to support different routing methods for FP8 flashinfer TRTLLM MOE (DeepSeekV3, Llama4, Renormalize, etc.).
While this was implemented to support Qwen3 and Qwen3-next models, the review discussion revealed opportunities to use this more broadly across the
codebase to simplify MoE routing configuration.

### Motivation:

Currently, MoE routing behavior is controlled through multiple fragmented parameters (scoring_func, renormalize, use_grouped_topk, custom routing
functions, etc.). This creates several issues:

1. Lack of clarity: The routing method isn&#x27;t explicitly defined in one place
2. Code duplication: Each model must explicitly specify routing parameters
3. Maintenance burden: Adding new routing methods 

...</description>
  </item>
  <item>
    <title>[Perf] Improve default fused moe configs by analyzing tuned configs</title>
    <link>https://github.com/vllm-project/vllm/pull/24700</link>
    <guid isPermaLink="false">https://github.com/vllm-project/vllm/pull/24700</guid>
    <pubDate>Thu, 11 Sep 2025 22:24:47 +0000</pubDate>
    <description>&lt;!-- markdownlint-disable --&gt;

## Purpose

With the help of Claude, I took a look at the statistics of the 200+ configs we have for fused moe and came away with some recommendations. Haven&#x27;t validated them e2e yet, but from smoke tests it seems faster.

The main improvement seems to be from just including defaults for `&#x27;num_warps&#x27;: 4, &#x27;num_stages&#x27;: 3`, which were omitted before except for block fp8.

### Benchmarks on H100

Mixtral BF16
```
python benchmarks/kernels/benchmark_moe.py --model=mistralai/Mixtral-8x7B-Instruct-v0.1 -tp=2

# Tuned Config
(BenchmarkWorker pid=3339388) INFO 09-12 01:05:21 [fused_moe.py:720] Using configuration from /home/mgoin/code/vllm/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.
Batch size: 1, Kern

...</description>
  </item>
</channel>
</rss>
